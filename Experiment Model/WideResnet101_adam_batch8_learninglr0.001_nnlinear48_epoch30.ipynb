{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368ff613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a75db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='emotion5/train/', transform=transforms)\n",
    "test_dataset = datasets.ImageFolder(root='emotion5/test/', transform=transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7086989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \n",
    "    inp = inp.cpu() if device else inp\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d6856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images-size: torch.Size([8, 3, 224, 224])\n",
      "out-size: torch.Size([3, 228, 1810])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(train_dataloader)) \n",
    "print(\"images-size:\", images.shape)\n",
    "\n",
    "out = torchvision.utils.make_grid(images)\n",
    "print(\"out-size:\", out.shape)\n",
    "\n",
    "imshow(out, title=[train_dataset.classes[x] for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff1dc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Otakuking/.cache\\torch\\hub\\pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torch.hub.load('pytorch/vision:v0.9.0', 'wide_resnet101_2', pretrained=True)\n",
    "net = net.cuda() if device else net\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9927969",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, \n",
    "                 betas=(0.9, 0.999), \n",
    "                 eps=1e-08, \n",
    "                 weight_decay=0, \n",
    "                 amsgrad=True)\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    _,pred = torch.max(out, dim=1)\n",
    "    return torch.sum(pred==labels).item()\n",
    "\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 48)\n",
    "net.fc = net.fc.cuda() if device else net.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d42f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "Epoch [1/30], Step [0/3535], Loss: 3.8566\n",
      "Epoch [1/30], Step [20/3535], Loss: 3.2311\n",
      "Epoch [1/30], Step [40/3535], Loss: 3.4107\n",
      "Epoch [1/30], Step [60/3535], Loss: 2.9420\n",
      "Epoch [1/30], Step [80/3535], Loss: 2.7493\n",
      "Epoch [1/30], Step [100/3535], Loss: 2.7560\n",
      "Epoch [1/30], Step [120/3535], Loss: 2.1847\n",
      "Epoch [1/30], Step [140/3535], Loss: 2.4083\n",
      "Epoch [1/30], Step [160/3535], Loss: 2.4361\n",
      "Epoch [1/30], Step [180/3535], Loss: 2.2394\n",
      "Epoch [1/30], Step [200/3535], Loss: 2.5733\n",
      "Epoch [1/30], Step [220/3535], Loss: 2.4062\n",
      "Epoch [1/30], Step [240/3535], Loss: 2.2114\n",
      "Epoch [1/30], Step [260/3535], Loss: 2.5688\n",
      "Epoch [1/30], Step [280/3535], Loss: 2.1994\n",
      "Epoch [1/30], Step [300/3535], Loss: 2.0434\n",
      "Epoch [1/30], Step [320/3535], Loss: 2.2990\n",
      "Epoch [1/30], Step [340/3535], Loss: 2.0599\n",
      "Epoch [1/30], Step [360/3535], Loss: 1.9394\n",
      "Epoch [1/30], Step [380/3535], Loss: 1.9100\n",
      "Epoch [1/30], Step [400/3535], Loss: 2.1608\n",
      "Epoch [1/30], Step [420/3535], Loss: 2.0439\n",
      "Epoch [1/30], Step [440/3535], Loss: 2.0423\n",
      "Epoch [1/30], Step [460/3535], Loss: 1.9366\n",
      "Epoch [1/30], Step [480/3535], Loss: 1.8295\n",
      "Epoch [1/30], Step [500/3535], Loss: 2.0418\n",
      "Epoch [1/30], Step [520/3535], Loss: 1.9486\n",
      "Epoch [1/30], Step [540/3535], Loss: 1.8851\n",
      "Epoch [1/30], Step [560/3535], Loss: 1.8145\n",
      "Epoch [1/30], Step [580/3535], Loss: 1.8085\n",
      "Epoch [1/30], Step [600/3535], Loss: 1.7699\n",
      "Epoch [1/30], Step [620/3535], Loss: 1.6171\n",
      "Epoch [1/30], Step [640/3535], Loss: 1.8033\n",
      "Epoch [1/30], Step [660/3535], Loss: 1.8635\n",
      "Epoch [1/30], Step [680/3535], Loss: 2.1126\n",
      "Epoch [1/30], Step [700/3535], Loss: 1.9150\n",
      "Epoch [1/30], Step [720/3535], Loss: 2.0235\n",
      "Epoch [1/30], Step [740/3535], Loss: 1.7613\n",
      "Epoch [1/30], Step [760/3535], Loss: 1.9432\n",
      "Epoch [1/30], Step [780/3535], Loss: 1.7686\n",
      "Epoch [1/30], Step [800/3535], Loss: 1.7407\n",
      "Epoch [1/30], Step [820/3535], Loss: 1.9223\n",
      "Epoch [1/30], Step [840/3535], Loss: 1.7590\n",
      "Epoch [1/30], Step [860/3535], Loss: 1.8365\n",
      "Epoch [1/30], Step [880/3535], Loss: 1.8502\n",
      "Epoch [1/30], Step [900/3535], Loss: 1.7797\n",
      "Epoch [1/30], Step [920/3535], Loss: 1.9001\n",
      "Epoch [1/30], Step [940/3535], Loss: 1.8760\n",
      "Epoch [1/30], Step [960/3535], Loss: 1.8337\n",
      "Epoch [1/30], Step [980/3535], Loss: 1.7002\n",
      "Epoch [1/30], Step [1000/3535], Loss: 1.8633\n",
      "Epoch [1/30], Step [1020/3535], Loss: 1.9624\n",
      "Epoch [1/30], Step [1040/3535], Loss: 1.9866\n",
      "Epoch [1/30], Step [1060/3535], Loss: 1.9311\n",
      "Epoch [1/30], Step [1080/3535], Loss: 1.8225\n",
      "Epoch [1/30], Step [1100/3535], Loss: 1.8397\n",
      "Epoch [1/30], Step [1120/3535], Loss: 1.8032\n",
      "Epoch [1/30], Step [1140/3535], Loss: 1.9031\n",
      "Epoch [1/30], Step [1160/3535], Loss: 1.8914\n",
      "Epoch [1/30], Step [1180/3535], Loss: 1.6647\n",
      "Epoch [1/30], Step [1200/3535], Loss: 1.8640\n",
      "Epoch [1/30], Step [1220/3535], Loss: 1.7978\n",
      "Epoch [1/30], Step [1240/3535], Loss: 1.7806\n",
      "Epoch [1/30], Step [1260/3535], Loss: 1.8004\n",
      "Epoch [1/30], Step [1280/3535], Loss: 1.8052\n",
      "Epoch [1/30], Step [1300/3535], Loss: 1.9675\n",
      "Epoch [1/30], Step [1320/3535], Loss: 1.7907\n",
      "Epoch [1/30], Step [1340/3535], Loss: 1.6974\n",
      "Epoch [1/30], Step [1360/3535], Loss: 1.8563\n",
      "Epoch [1/30], Step [1380/3535], Loss: 1.7261\n",
      "Epoch [1/30], Step [1400/3535], Loss: 1.6995\n",
      "Epoch [1/30], Step [1420/3535], Loss: 1.8175\n",
      "Epoch [1/30], Step [1440/3535], Loss: 1.6500\n",
      "Epoch [1/30], Step [1460/3535], Loss: 1.7352\n",
      "Epoch [1/30], Step [1480/3535], Loss: 1.7793\n",
      "Epoch [1/30], Step [1500/3535], Loss: 1.9710\n",
      "Epoch [1/30], Step [1520/3535], Loss: 1.6590\n",
      "Epoch [1/30], Step [1540/3535], Loss: 1.7101\n",
      "Epoch [1/30], Step [1560/3535], Loss: 1.7286\n",
      "Epoch [1/30], Step [1580/3535], Loss: 1.8533\n",
      "Epoch [1/30], Step [1600/3535], Loss: 1.7724\n",
      "Epoch [1/30], Step [1620/3535], Loss: 1.6678\n",
      "Epoch [1/30], Step [1640/3535], Loss: 1.7694\n",
      "Epoch [1/30], Step [1660/3535], Loss: 1.5913\n",
      "Epoch [1/30], Step [1680/3535], Loss: 1.7784\n",
      "Epoch [1/30], Step [1700/3535], Loss: 2.0394\n",
      "Epoch [1/30], Step [1720/3535], Loss: 1.5143\n",
      "Epoch [1/30], Step [1740/3535], Loss: 1.7479\n",
      "Epoch [1/30], Step [1760/3535], Loss: 1.6648\n",
      "Epoch [1/30], Step [1780/3535], Loss: 1.6177\n",
      "Epoch [1/30], Step [1800/3535], Loss: 1.5528\n",
      "Epoch [1/30], Step [1820/3535], Loss: 1.6840\n",
      "Epoch [1/30], Step [1840/3535], Loss: 1.5845\n",
      "Epoch [1/30], Step [1860/3535], Loss: 1.7556\n",
      "Epoch [1/30], Step [1880/3535], Loss: 1.8067\n",
      "Epoch [1/30], Step [1900/3535], Loss: 1.7006\n",
      "Epoch [1/30], Step [1920/3535], Loss: 1.8529\n",
      "Epoch [1/30], Step [1940/3535], Loss: 1.9963\n",
      "Epoch [1/30], Step [1960/3535], Loss: 2.0754\n",
      "Epoch [1/30], Step [1980/3535], Loss: 1.9276\n",
      "Epoch [1/30], Step [2000/3535], Loss: 1.6397\n",
      "Epoch [1/30], Step [2020/3535], Loss: 1.9404\n",
      "Epoch [1/30], Step [2040/3535], Loss: 1.7797\n",
      "Epoch [1/30], Step [2060/3535], Loss: 1.7306\n",
      "Epoch [1/30], Step [2080/3535], Loss: 1.6223\n",
      "Epoch [1/30], Step [2100/3535], Loss: 1.8069\n",
      "Epoch [1/30], Step [2120/3535], Loss: 1.8598\n",
      "Epoch [1/30], Step [2140/3535], Loss: 1.5565\n",
      "Epoch [1/30], Step [2160/3535], Loss: 2.1815\n",
      "Epoch [1/30], Step [2180/3535], Loss: 1.5429\n",
      "Epoch [1/30], Step [2200/3535], Loss: 1.8566\n",
      "Epoch [1/30], Step [2220/3535], Loss: 1.7493\n",
      "Epoch [1/30], Step [2240/3535], Loss: 1.5845\n",
      "Epoch [1/30], Step [2260/3535], Loss: 1.8696\n",
      "Epoch [1/30], Step [2280/3535], Loss: 1.6830\n",
      "Epoch [1/30], Step [2300/3535], Loss: 1.6081\n",
      "Epoch [1/30], Step [2320/3535], Loss: 1.6724\n",
      "Epoch [1/30], Step [2340/3535], Loss: 1.8375\n",
      "Epoch [1/30], Step [2360/3535], Loss: 1.9379\n",
      "Epoch [1/30], Step [2380/3535], Loss: 1.7810\n",
      "Epoch [1/30], Step [2400/3535], Loss: 1.7808\n",
      "Epoch [1/30], Step [2420/3535], Loss: 1.6236\n",
      "Epoch [1/30], Step [2440/3535], Loss: 1.7692\n",
      "Epoch [1/30], Step [2460/3535], Loss: 1.7276\n",
      "Epoch [1/30], Step [2480/3535], Loss: 1.6804\n",
      "Epoch [1/30], Step [2500/3535], Loss: 1.6912\n",
      "Epoch [1/30], Step [2520/3535], Loss: 1.8732\n",
      "Epoch [1/30], Step [2540/3535], Loss: 1.7500\n",
      "Epoch [1/30], Step [2560/3535], Loss: 1.6110\n",
      "Epoch [1/30], Step [2580/3535], Loss: 1.8667\n",
      "Epoch [1/30], Step [2600/3535], Loss: 1.7412\n",
      "Epoch [1/30], Step [2620/3535], Loss: 1.5968\n",
      "Epoch [1/30], Step [2640/3535], Loss: 1.7601\n",
      "Epoch [1/30], Step [2660/3535], Loss: 1.5398\n",
      "Epoch [1/30], Step [2680/3535], Loss: 1.7106\n",
      "Epoch [1/30], Step [2700/3535], Loss: 1.8671\n",
      "Epoch [1/30], Step [2720/3535], Loss: 1.7819\n",
      "Epoch [1/30], Step [2740/3535], Loss: 1.6726\n",
      "Epoch [1/30], Step [2760/3535], Loss: 2.0668\n",
      "Epoch [1/30], Step [2780/3535], Loss: 1.5196\n",
      "Epoch [1/30], Step [2800/3535], Loss: 1.7778\n",
      "Epoch [1/30], Step [2820/3535], Loss: 2.2938\n",
      "Epoch [1/30], Step [2840/3535], Loss: 1.8832\n",
      "Epoch [1/30], Step [2860/3535], Loss: 1.9695\n",
      "Epoch [1/30], Step [2880/3535], Loss: 2.1518\n",
      "Epoch [1/30], Step [2900/3535], Loss: 1.7111\n",
      "Epoch [1/30], Step [2920/3535], Loss: 1.7046\n",
      "Epoch [1/30], Step [2940/3535], Loss: 1.8005\n",
      "Epoch [1/30], Step [2960/3535], Loss: 1.5472\n",
      "Epoch [1/30], Step [2980/3535], Loss: 1.6076\n",
      "Epoch [1/30], Step [3000/3535], Loss: 1.9876\n",
      "Epoch [1/30], Step [3020/3535], Loss: 2.0527\n",
      "Epoch [1/30], Step [3040/3535], Loss: 1.9503\n",
      "Epoch [1/30], Step [3060/3535], Loss: 1.6093\n",
      "Epoch [1/30], Step [3080/3535], Loss: 1.7297\n",
      "Epoch [1/30], Step [3100/3535], Loss: 1.5885\n",
      "Epoch [1/30], Step [3120/3535], Loss: 1.8549\n",
      "Epoch [1/30], Step [3140/3535], Loss: 1.6084\n",
      "Epoch [1/30], Step [3160/3535], Loss: 1.6507\n",
      "Epoch [1/30], Step [3180/3535], Loss: 1.8181\n",
      "Epoch [1/30], Step [3200/3535], Loss: 1.5444\n",
      "Epoch [1/30], Step [3220/3535], Loss: 1.8786\n",
      "Epoch [1/30], Step [3240/3535], Loss: 1.6289\n",
      "Epoch [1/30], Step [3260/3535], Loss: 1.5179\n",
      "Epoch [1/30], Step [3280/3535], Loss: 1.4251\n",
      "Epoch [1/30], Step [3300/3535], Loss: 1.5630\n",
      "Epoch [1/30], Step [3320/3535], Loss: 1.7507\n",
      "Epoch [1/30], Step [3340/3535], Loss: 2.0069\n",
      "Epoch [1/30], Step [3360/3535], Loss: 1.6519\n",
      "Epoch [1/30], Step [3380/3535], Loss: 1.7781\n",
      "Epoch [1/30], Step [3400/3535], Loss: 1.6736\n",
      "Epoch [1/30], Step [3420/3535], Loss: 1.6967\n",
      "Epoch [1/30], Step [3440/3535], Loss: 1.5648\n",
      "Epoch [1/30], Step [3460/3535], Loss: 1.6756\n",
      "Epoch [1/30], Step [3480/3535], Loss: 1.8469\n",
      "Epoch [1/30], Step [3500/3535], Loss: 1.7494\n",
      "Epoch [1/30], Step [3520/3535], Loss: 2.0026\n",
      "\n",
      "train-loss: 1.8613, train-acc: 25.9046\n",
      "validation loss: 1.7435, validation acc: 32.0362\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 2\n",
      "\n",
      "Epoch [2/30], Step [0/3535], Loss: 1.6754\n",
      "Epoch [2/30], Step [20/3535], Loss: 1.6508\n",
      "Epoch [2/30], Step [40/3535], Loss: 1.7750\n",
      "Epoch [2/30], Step [60/3535], Loss: 1.9679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], Step [80/3535], Loss: 1.4734\n",
      "Epoch [2/30], Step [100/3535], Loss: 1.7926\n",
      "Epoch [2/30], Step [120/3535], Loss: 1.3935\n",
      "Epoch [2/30], Step [140/3535], Loss: 1.6058\n",
      "Epoch [2/30], Step [160/3535], Loss: 2.0115\n",
      "Epoch [2/30], Step [180/3535], Loss: 1.6030\n",
      "Epoch [2/30], Step [200/3535], Loss: 1.3199\n",
      "Epoch [2/30], Step [220/3535], Loss: 2.0166\n",
      "Epoch [2/30], Step [240/3535], Loss: 1.3355\n",
      "Epoch [2/30], Step [260/3535], Loss: 1.7097\n",
      "Epoch [2/30], Step [280/3535], Loss: 1.6031\n",
      "Epoch [2/30], Step [300/3535], Loss: 1.7300\n",
      "Epoch [2/30], Step [320/3535], Loss: 1.6540\n",
      "Epoch [2/30], Step [340/3535], Loss: 1.2006\n",
      "Epoch [2/30], Step [360/3535], Loss: 1.7626\n",
      "Epoch [2/30], Step [380/3535], Loss: 2.0476\n",
      "Epoch [2/30], Step [400/3535], Loss: 1.7030\n",
      "Epoch [2/30], Step [420/3535], Loss: 1.5740\n",
      "Epoch [2/30], Step [440/3535], Loss: 1.6121\n",
      "Epoch [2/30], Step [460/3535], Loss: 1.6286\n",
      "Epoch [2/30], Step [480/3535], Loss: 1.7591\n",
      "Epoch [2/30], Step [500/3535], Loss: 1.9895\n",
      "Epoch [2/30], Step [520/3535], Loss: 1.5195\n",
      "Epoch [2/30], Step [540/3535], Loss: 1.8450\n",
      "Epoch [2/30], Step [560/3535], Loss: 1.5047\n",
      "Epoch [2/30], Step [580/3535], Loss: 1.3030\n",
      "Epoch [2/30], Step [600/3535], Loss: 1.8287\n",
      "Epoch [2/30], Step [620/3535], Loss: 1.5738\n",
      "Epoch [2/30], Step [640/3535], Loss: 1.5901\n",
      "Epoch [2/30], Step [660/3535], Loss: 1.8887\n",
      "Epoch [2/30], Step [680/3535], Loss: 1.6393\n",
      "Epoch [2/30], Step [700/3535], Loss: 1.7272\n",
      "Epoch [2/30], Step [720/3535], Loss: 1.4063\n",
      "Epoch [2/30], Step [740/3535], Loss: 1.6142\n",
      "Epoch [2/30], Step [760/3535], Loss: 1.5483\n",
      "Epoch [2/30], Step [780/3535], Loss: 1.4751\n",
      "Epoch [2/30], Step [800/3535], Loss: 1.5578\n",
      "Epoch [2/30], Step [820/3535], Loss: 1.7638\n",
      "Epoch [2/30], Step [840/3535], Loss: 1.5499\n",
      "Epoch [2/30], Step [860/3535], Loss: 1.6018\n",
      "Epoch [2/30], Step [880/3535], Loss: 1.3862\n",
      "Epoch [2/30], Step [900/3535], Loss: 1.6330\n",
      "Epoch [2/30], Step [920/3535], Loss: 1.5305\n",
      "Epoch [2/30], Step [940/3535], Loss: 1.5165\n",
      "Epoch [2/30], Step [960/3535], Loss: 1.5754\n",
      "Epoch [2/30], Step [980/3535], Loss: 1.5542\n",
      "Epoch [2/30], Step [1000/3535], Loss: 1.6890\n",
      "Epoch [2/30], Step [1020/3535], Loss: 1.6763\n",
      "Epoch [2/30], Step [1040/3535], Loss: 1.0676\n",
      "Epoch [2/30], Step [1060/3535], Loss: 1.6972\n",
      "Epoch [2/30], Step [1080/3535], Loss: 1.6648\n",
      "Epoch [2/30], Step [1100/3535], Loss: 1.5759\n",
      "Epoch [2/30], Step [1120/3535], Loss: 1.3858\n",
      "Epoch [2/30], Step [1140/3535], Loss: 1.6926\n",
      "Epoch [2/30], Step [1160/3535], Loss: 1.7996\n",
      "Epoch [2/30], Step [1180/3535], Loss: 1.6517\n",
      "Epoch [2/30], Step [1200/3535], Loss: 1.5176\n",
      "Epoch [2/30], Step [1220/3535], Loss: 1.5661\n",
      "Epoch [2/30], Step [1240/3535], Loss: 1.4417\n",
      "Epoch [2/30], Step [1260/3535], Loss: 1.5177\n",
      "Epoch [2/30], Step [1280/3535], Loss: 2.0891\n",
      "Epoch [2/30], Step [1300/3535], Loss: 1.4601\n",
      "Epoch [2/30], Step [1320/3535], Loss: 1.5832\n",
      "Epoch [2/30], Step [1340/3535], Loss: 1.6009\n",
      "Epoch [2/30], Step [1360/3535], Loss: 1.3316\n",
      "Epoch [2/30], Step [1380/3535], Loss: 1.2129\n",
      "Epoch [2/30], Step [1400/3535], Loss: 1.7205\n",
      "Epoch [2/30], Step [1420/3535], Loss: 1.4886\n",
      "Epoch [2/30], Step [1440/3535], Loss: 1.3603\n",
      "Epoch [2/30], Step [1460/3535], Loss: 1.4996\n",
      "Epoch [2/30], Step [1480/3535], Loss: 1.4163\n",
      "Epoch [2/30], Step [1500/3535], Loss: 1.4744\n",
      "Epoch [2/30], Step [1520/3535], Loss: 1.8187\n",
      "Epoch [2/30], Step [1540/3535], Loss: 1.7167\n",
      "Epoch [2/30], Step [1560/3535], Loss: 1.5338\n",
      "Epoch [2/30], Step [1580/3535], Loss: 1.6441\n",
      "Epoch [2/30], Step [1600/3535], Loss: 1.5487\n",
      "Epoch [2/30], Step [1620/3535], Loss: 1.4236\n",
      "Epoch [2/30], Step [1640/3535], Loss: 1.3989\n",
      "Epoch [2/30], Step [1660/3535], Loss: 1.1978\n",
      "Epoch [2/30], Step [1680/3535], Loss: 1.7816\n",
      "Epoch [2/30], Step [1700/3535], Loss: 1.9722\n",
      "Epoch [2/30], Step [1720/3535], Loss: 1.8754\n",
      "Epoch [2/30], Step [1740/3535], Loss: 1.3095\n",
      "Epoch [2/30], Step [1760/3535], Loss: 1.1773\n",
      "Epoch [2/30], Step [1780/3535], Loss: 1.8061\n",
      "Epoch [2/30], Step [1800/3535], Loss: 1.0796\n",
      "Epoch [2/30], Step [1820/3535], Loss: 1.4989\n",
      "Epoch [2/30], Step [1840/3535], Loss: 2.0309\n",
      "Epoch [2/30], Step [1860/3535], Loss: 1.6546\n",
      "Epoch [2/30], Step [1880/3535], Loss: 1.7895\n",
      "Epoch [2/30], Step [1900/3535], Loss: 1.1073\n",
      "Epoch [2/30], Step [1920/3535], Loss: 1.8396\n",
      "Epoch [2/30], Step [1940/3535], Loss: 1.7643\n",
      "Epoch [2/30], Step [1960/3535], Loss: 1.3900\n",
      "Epoch [2/30], Step [1980/3535], Loss: 1.3608\n",
      "Epoch [2/30], Step [2000/3535], Loss: 1.2546\n",
      "Epoch [2/30], Step [2020/3535], Loss: 1.8089\n",
      "Epoch [2/30], Step [2040/3535], Loss: 1.9147\n",
      "Epoch [2/30], Step [2060/3535], Loss: 1.5279\n",
      "Epoch [2/30], Step [2080/3535], Loss: 1.5511\n",
      "Epoch [2/30], Step [2100/3535], Loss: 1.4123\n",
      "Epoch [2/30], Step [2120/3535], Loss: 1.9776\n",
      "Epoch [2/30], Step [2140/3535], Loss: 1.2396\n",
      "Epoch [2/30], Step [2160/3535], Loss: 1.8563\n",
      "Epoch [2/30], Step [2180/3535], Loss: 1.2912\n",
      "Epoch [2/30], Step [2200/3535], Loss: 1.7240\n",
      "Epoch [2/30], Step [2220/3535], Loss: 1.4858\n",
      "Epoch [2/30], Step [2240/3535], Loss: 1.4429\n",
      "Epoch [2/30], Step [2260/3535], Loss: 2.0878\n",
      "Epoch [2/30], Step [2280/3535], Loss: 0.9413\n",
      "Epoch [2/30], Step [2300/3535], Loss: 1.2508\n",
      "Epoch [2/30], Step [2320/3535], Loss: 1.3177\n",
      "Epoch [2/30], Step [2340/3535], Loss: 1.8701\n",
      "Epoch [2/30], Step [2360/3535], Loss: 1.5646\n",
      "Epoch [2/30], Step [2380/3535], Loss: 1.5597\n",
      "Epoch [2/30], Step [2400/3535], Loss: 1.4335\n",
      "Epoch [2/30], Step [2420/3535], Loss: 1.3547\n",
      "Epoch [2/30], Step [2440/3535], Loss: 1.5722\n",
      "Epoch [2/30], Step [2460/3535], Loss: 1.8681\n",
      "Epoch [2/30], Step [2480/3535], Loss: 1.2383\n",
      "Epoch [2/30], Step [2500/3535], Loss: 1.0828\n",
      "Epoch [2/30], Step [2520/3535], Loss: 1.7304\n",
      "Epoch [2/30], Step [2540/3535], Loss: 1.4899\n",
      "Epoch [2/30], Step [2560/3535], Loss: 0.9967\n",
      "Epoch [2/30], Step [2580/3535], Loss: 1.6383\n",
      "Epoch [2/30], Step [2600/3535], Loss: 2.0589\n",
      "Epoch [2/30], Step [2620/3535], Loss: 1.6953\n",
      "Epoch [2/30], Step [2640/3535], Loss: 1.4336\n",
      "Epoch [2/30], Step [2660/3535], Loss: 1.1260\n",
      "Epoch [2/30], Step [2680/3535], Loss: 1.7108\n",
      "Epoch [2/30], Step [2700/3535], Loss: 1.1706\n",
      "Epoch [2/30], Step [2720/3535], Loss: 1.6288\n",
      "Epoch [2/30], Step [2740/3535], Loss: 1.2212\n",
      "Epoch [2/30], Step [2760/3535], Loss: 1.4633\n",
      "Epoch [2/30], Step [2780/3535], Loss: 1.4741\n",
      "Epoch [2/30], Step [2800/3535], Loss: 1.1455\n",
      "Epoch [2/30], Step [2820/3535], Loss: 1.3345\n",
      "Epoch [2/30], Step [2840/3535], Loss: 1.6149\n",
      "Epoch [2/30], Step [2860/3535], Loss: 1.0012\n",
      "Epoch [2/30], Step [2880/3535], Loss: 1.4362\n",
      "Epoch [2/30], Step [2900/3535], Loss: 1.7088\n",
      "Epoch [2/30], Step [2920/3535], Loss: 1.7497\n",
      "Epoch [2/30], Step [2940/3535], Loss: 1.6256\n",
      "Epoch [2/30], Step [2960/3535], Loss: 1.7972\n",
      "Epoch [2/30], Step [2980/3535], Loss: 1.1370\n",
      "Epoch [2/30], Step [3000/3535], Loss: 1.0251\n",
      "Epoch [2/30], Step [3020/3535], Loss: 1.1779\n",
      "Epoch [2/30], Step [3040/3535], Loss: 1.5202\n",
      "Epoch [2/30], Step [3060/3535], Loss: 0.9908\n",
      "Epoch [2/30], Step [3080/3535], Loss: 1.0570\n",
      "Epoch [2/30], Step [3100/3535], Loss: 1.6715\n",
      "Epoch [2/30], Step [3120/3535], Loss: 1.3278\n",
      "Epoch [2/30], Step [3140/3535], Loss: 1.4557\n",
      "Epoch [2/30], Step [3160/3535], Loss: 1.3890\n",
      "Epoch [2/30], Step [3180/3535], Loss: 1.2761\n",
      "Epoch [2/30], Step [3200/3535], Loss: 1.3632\n",
      "Epoch [2/30], Step [3220/3535], Loss: 1.3236\n",
      "Epoch [2/30], Step [3240/3535], Loss: 1.8445\n",
      "Epoch [2/30], Step [3260/3535], Loss: 1.3830\n",
      "Epoch [2/30], Step [3280/3535], Loss: 1.4551\n",
      "Epoch [2/30], Step [3300/3535], Loss: 1.5496\n",
      "Epoch [2/30], Step [3320/3535], Loss: 1.4135\n",
      "Epoch [2/30], Step [3340/3535], Loss: 1.3671\n",
      "Epoch [2/30], Step [3360/3535], Loss: 1.2340\n",
      "Epoch [2/30], Step [3380/3535], Loss: 1.5464\n",
      "Epoch [2/30], Step [3400/3535], Loss: 1.3705\n",
      "Epoch [2/30], Step [3420/3535], Loss: 0.9929\n",
      "Epoch [2/30], Step [3440/3535], Loss: 1.5586\n",
      "Epoch [2/30], Step [3460/3535], Loss: 1.7262\n",
      "Epoch [2/30], Step [3480/3535], Loss: 1.5085\n",
      "Epoch [2/30], Step [3500/3535], Loss: 0.8228\n",
      "Epoch [2/30], Step [3520/3535], Loss: 1.9692\n",
      "\n",
      "train-loss: 1.7026, train-acc: 38.2096\n",
      "validation loss: 1.5458, validation acc: 47.0921\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 3\n",
      "\n",
      "Epoch [3/30], Step [0/3535], Loss: 1.6698\n",
      "Epoch [3/30], Step [20/3535], Loss: 1.2594\n",
      "Epoch [3/30], Step [40/3535], Loss: 1.2406\n",
      "Epoch [3/30], Step [60/3535], Loss: 1.5731\n",
      "Epoch [3/30], Step [80/3535], Loss: 1.7452\n",
      "Epoch [3/30], Step [100/3535], Loss: 1.3425\n",
      "Epoch [3/30], Step [120/3535], Loss: 1.4016\n",
      "Epoch [3/30], Step [140/3535], Loss: 1.6872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], Step [160/3535], Loss: 1.3929\n",
      "Epoch [3/30], Step [180/3535], Loss: 1.3544\n",
      "Epoch [3/30], Step [200/3535], Loss: 1.7213\n",
      "Epoch [3/30], Step [220/3535], Loss: 1.0380\n",
      "Epoch [3/30], Step [240/3535], Loss: 1.5780\n",
      "Epoch [3/30], Step [260/3535], Loss: 1.4394\n",
      "Epoch [3/30], Step [280/3535], Loss: 1.0300\n",
      "Epoch [3/30], Step [300/3535], Loss: 1.1169\n",
      "Epoch [3/30], Step [320/3535], Loss: 1.6245\n",
      "Epoch [3/30], Step [340/3535], Loss: 1.0957\n",
      "Epoch [3/30], Step [360/3535], Loss: 1.3968\n",
      "Epoch [3/30], Step [380/3535], Loss: 1.9945\n",
      "Epoch [3/30], Step [400/3535], Loss: 1.8662\n",
      "Epoch [3/30], Step [420/3535], Loss: 1.6498\n",
      "Epoch [3/30], Step [440/3535], Loss: 1.1110\n",
      "Epoch [3/30], Step [460/3535], Loss: 1.4647\n",
      "Epoch [3/30], Step [480/3535], Loss: 1.0523\n",
      "Epoch [3/30], Step [500/3535], Loss: 1.1925\n",
      "Epoch [3/30], Step [520/3535], Loss: 1.6198\n",
      "Epoch [3/30], Step [540/3535], Loss: 1.1940\n",
      "Epoch [3/30], Step [560/3535], Loss: 1.1655\n",
      "Epoch [3/30], Step [580/3535], Loss: 1.3705\n",
      "Epoch [3/30], Step [600/3535], Loss: 1.7459\n",
      "Epoch [3/30], Step [620/3535], Loss: 1.6823\n",
      "Epoch [3/30], Step [640/3535], Loss: 1.0546\n",
      "Epoch [3/30], Step [660/3535], Loss: 1.3448\n",
      "Epoch [3/30], Step [680/3535], Loss: 1.0392\n",
      "Epoch [3/30], Step [700/3535], Loss: 0.6103\n",
      "Epoch [3/30], Step [720/3535], Loss: 1.1093\n",
      "Epoch [3/30], Step [740/3535], Loss: 1.6403\n",
      "Epoch [3/30], Step [760/3535], Loss: 1.4016\n",
      "Epoch [3/30], Step [780/3535], Loss: 1.4923\n",
      "Epoch [3/30], Step [800/3535], Loss: 1.3421\n",
      "Epoch [3/30], Step [820/3535], Loss: 1.4435\n",
      "Epoch [3/30], Step [840/3535], Loss: 1.6394\n",
      "Epoch [3/30], Step [860/3535], Loss: 1.5708\n",
      "Epoch [3/30], Step [880/3535], Loss: 1.4912\n",
      "Epoch [3/30], Step [900/3535], Loss: 0.8080\n",
      "Epoch [3/30], Step [920/3535], Loss: 1.2876\n",
      "Epoch [3/30], Step [940/3535], Loss: 1.4427\n",
      "Epoch [3/30], Step [960/3535], Loss: 1.1989\n",
      "Epoch [3/30], Step [980/3535], Loss: 1.1121\n",
      "Epoch [3/30], Step [1000/3535], Loss: 1.4482\n",
      "Epoch [3/30], Step [1020/3535], Loss: 1.5205\n",
      "Epoch [3/30], Step [1040/3535], Loss: 1.6480\n",
      "Epoch [3/30], Step [1060/3535], Loss: 1.2085\n",
      "Epoch [3/30], Step [1080/3535], Loss: 1.7122\n",
      "Epoch [3/30], Step [1100/3535], Loss: 1.6820\n",
      "Epoch [3/30], Step [1120/3535], Loss: 1.5179\n",
      "Epoch [3/30], Step [1140/3535], Loss: 1.0979\n",
      "Epoch [3/30], Step [1160/3535], Loss: 0.9090\n",
      "Epoch [3/30], Step [1180/3535], Loss: 1.2637\n",
      "Epoch [3/30], Step [1200/3535], Loss: 1.2308\n",
      "Epoch [3/30], Step [1220/3535], Loss: 1.2832\n",
      "Epoch [3/30], Step [1240/3535], Loss: 1.0441\n",
      "Epoch [3/30], Step [1260/3535], Loss: 1.4033\n",
      "Epoch [3/30], Step [1280/3535], Loss: 0.9876\n",
      "Epoch [3/30], Step [1300/3535], Loss: 1.0061\n",
      "Epoch [3/30], Step [1320/3535], Loss: 1.0148\n",
      "Epoch [3/30], Step [1340/3535], Loss: 1.1210\n",
      "Epoch [3/30], Step [1360/3535], Loss: 1.1881\n",
      "Epoch [3/30], Step [1380/3535], Loss: 1.0255\n",
      "Epoch [3/30], Step [1400/3535], Loss: 1.8518\n",
      "Epoch [3/30], Step [1420/3535], Loss: 1.2076\n",
      "Epoch [3/30], Step [1440/3535], Loss: 1.6302\n",
      "Epoch [3/30], Step [1460/3535], Loss: 1.4475\n",
      "Epoch [3/30], Step [1480/3535], Loss: 1.8381\n",
      "Epoch [3/30], Step [1500/3535], Loss: 1.0662\n",
      "Epoch [3/30], Step [1520/3535], Loss: 1.1535\n",
      "Epoch [3/30], Step [1540/3535], Loss: 1.2144\n",
      "Epoch [3/30], Step [1560/3535], Loss: 0.7539\n",
      "Epoch [3/30], Step [1580/3535], Loss: 1.3808\n",
      "Epoch [3/30], Step [1600/3535], Loss: 1.1437\n",
      "Epoch [3/30], Step [1620/3535], Loss: 1.3228\n",
      "Epoch [3/30], Step [1640/3535], Loss: 0.8049\n",
      "Epoch [3/30], Step [1660/3535], Loss: 1.2569\n",
      "Epoch [3/30], Step [1680/3535], Loss: 0.9691\n",
      "Epoch [3/30], Step [1700/3535], Loss: 1.5778\n",
      "Epoch [3/30], Step [1720/3535], Loss: 0.9564\n",
      "Epoch [3/30], Step [1740/3535], Loss: 1.7576\n",
      "Epoch [3/30], Step [1760/3535], Loss: 0.8625\n",
      "Epoch [3/30], Step [1780/3535], Loss: 1.4265\n",
      "Epoch [3/30], Step [1800/3535], Loss: 1.0725\n",
      "Epoch [3/30], Step [1820/3535], Loss: 1.2423\n",
      "Epoch [3/30], Step [1840/3535], Loss: 1.7413\n",
      "Epoch [3/30], Step [1860/3535], Loss: 0.9093\n",
      "Epoch [3/30], Step [1880/3535], Loss: 1.3333\n",
      "Epoch [3/30], Step [1900/3535], Loss: 0.9673\n",
      "Epoch [3/30], Step [1920/3535], Loss: 1.1464\n",
      "Epoch [3/30], Step [1940/3535], Loss: 1.2469\n",
      "Epoch [3/30], Step [1960/3535], Loss: 1.5462\n",
      "Epoch [3/30], Step [1980/3535], Loss: 1.2482\n",
      "Epoch [3/30], Step [2000/3535], Loss: 1.4743\n",
      "Epoch [3/30], Step [2020/3535], Loss: 0.9368\n",
      "Epoch [3/30], Step [2040/3535], Loss: 1.1237\n",
      "Epoch [3/30], Step [2060/3535], Loss: 1.3544\n",
      "Epoch [3/30], Step [2080/3535], Loss: 1.4613\n",
      "Epoch [3/30], Step [2100/3535], Loss: 0.7551\n",
      "Epoch [3/30], Step [2120/3535], Loss: 1.1799\n",
      "Epoch [3/30], Step [2140/3535], Loss: 1.3549\n",
      "Epoch [3/30], Step [2160/3535], Loss: 1.6261\n",
      "Epoch [3/30], Step [2180/3535], Loss: 1.4101\n",
      "Epoch [3/30], Step [2200/3535], Loss: 1.3560\n",
      "Epoch [3/30], Step [2220/3535], Loss: 1.2199\n",
      "Epoch [3/30], Step [2240/3535], Loss: 1.3800\n",
      "Epoch [3/30], Step [2260/3535], Loss: 1.2278\n",
      "Epoch [3/30], Step [2280/3535], Loss: 1.5727\n",
      "Epoch [3/30], Step [2300/3535], Loss: 1.7608\n",
      "Epoch [3/30], Step [2320/3535], Loss: 1.0437\n",
      "Epoch [3/30], Step [2340/3535], Loss: 1.4545\n",
      "Epoch [3/30], Step [2360/3535], Loss: 1.3695\n",
      "Epoch [3/30], Step [2380/3535], Loss: 1.5750\n",
      "Epoch [3/30], Step [2400/3535], Loss: 1.2200\n",
      "Epoch [3/30], Step [2420/3535], Loss: 1.2639\n",
      "Epoch [3/30], Step [2440/3535], Loss: 1.4046\n",
      "Epoch [3/30], Step [2460/3535], Loss: 1.1291\n",
      "Epoch [3/30], Step [2480/3535], Loss: 1.9355\n",
      "Epoch [3/30], Step [2500/3535], Loss: 1.7143\n",
      "Epoch [3/30], Step [2520/3535], Loss: 1.1867\n",
      "Epoch [3/30], Step [2540/3535], Loss: 0.8585\n",
      "Epoch [3/30], Step [2560/3535], Loss: 1.6176\n",
      "Epoch [3/30], Step [2580/3535], Loss: 1.1409\n",
      "Epoch [3/30], Step [2600/3535], Loss: 1.6600\n",
      "Epoch [3/30], Step [2620/3535], Loss: 1.0789\n",
      "Epoch [3/30], Step [2640/3535], Loss: 1.2071\n",
      "Epoch [3/30], Step [2660/3535], Loss: 1.0292\n",
      "Epoch [3/30], Step [2680/3535], Loss: 1.2832\n",
      "Epoch [3/30], Step [2700/3535], Loss: 1.4428\n",
      "Epoch [3/30], Step [2720/3535], Loss: 1.4518\n",
      "Epoch [3/30], Step [2740/3535], Loss: 1.1559\n",
      "Epoch [3/30], Step [2760/3535], Loss: 0.9375\n",
      "Epoch [3/30], Step [2780/3535], Loss: 1.2496\n",
      "Epoch [3/30], Step [2800/3535], Loss: 1.2544\n",
      "Epoch [3/30], Step [2820/3535], Loss: 1.4017\n",
      "Epoch [3/30], Step [2840/3535], Loss: 1.2418\n",
      "Epoch [3/30], Step [2860/3535], Loss: 1.0712\n",
      "Epoch [3/30], Step [2880/3535], Loss: 1.7835\n",
      "Epoch [3/30], Step [2900/3535], Loss: 1.3014\n",
      "Epoch [3/30], Step [2920/3535], Loss: 1.0315\n",
      "Epoch [3/30], Step [2940/3535], Loss: 1.5976\n",
      "Epoch [3/30], Step [2960/3535], Loss: 1.1198\n",
      "Epoch [3/30], Step [2980/3535], Loss: 1.1149\n",
      "Epoch [3/30], Step [3000/3535], Loss: 1.4138\n",
      "Epoch [3/30], Step [3020/3535], Loss: 1.7734\n",
      "Epoch [3/30], Step [3040/3535], Loss: 1.3881\n",
      "Epoch [3/30], Step [3060/3535], Loss: 1.0655\n",
      "Epoch [3/30], Step [3080/3535], Loss: 1.3148\n",
      "Epoch [3/30], Step [3100/3535], Loss: 0.6125\n",
      "Epoch [3/30], Step [3120/3535], Loss: 0.9474\n",
      "Epoch [3/30], Step [3140/3535], Loss: 1.3921\n",
      "Epoch [3/30], Step [3160/3535], Loss: 1.5048\n",
      "Epoch [3/30], Step [3180/3535], Loss: 1.2560\n",
      "Epoch [3/30], Step [3200/3535], Loss: 1.4353\n",
      "Epoch [3/30], Step [3220/3535], Loss: 1.3579\n",
      "Epoch [3/30], Step [3240/3535], Loss: 1.6837\n",
      "Epoch [3/30], Step [3260/3535], Loss: 1.3281\n",
      "Epoch [3/30], Step [3280/3535], Loss: 1.3664\n",
      "Epoch [3/30], Step [3300/3535], Loss: 1.0482\n",
      "Epoch [3/30], Step [3320/3535], Loss: 1.4005\n",
      "Epoch [3/30], Step [3340/3535], Loss: 1.4875\n",
      "Epoch [3/30], Step [3360/3535], Loss: 1.0956\n",
      "Epoch [3/30], Step [3380/3535], Loss: 1.1785\n",
      "Epoch [3/30], Step [3400/3535], Loss: 1.3324\n",
      "Epoch [3/30], Step [3420/3535], Loss: 1.0918\n",
      "Epoch [3/30], Step [3440/3535], Loss: 1.6716\n",
      "Epoch [3/30], Step [3460/3535], Loss: 0.7151\n",
      "Epoch [3/30], Step [3480/3535], Loss: 1.1131\n",
      "Epoch [3/30], Step [3500/3535], Loss: 1.2134\n",
      "Epoch [3/30], Step [3520/3535], Loss: 1.3374\n",
      "\n",
      "train-loss: 1.5805, train-acc: 47.6815\n",
      "validation loss: 1.4442, validation acc: 52.2428\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 4\n",
      "\n",
      "Epoch [4/30], Step [0/3535], Loss: 1.2164\n",
      "Epoch [4/30], Step [20/3535], Loss: 1.1369\n",
      "Epoch [4/30], Step [40/3535], Loss: 1.5893\n",
      "Epoch [4/30], Step [60/3535], Loss: 1.1572\n",
      "Epoch [4/30], Step [80/3535], Loss: 1.0540\n",
      "Epoch [4/30], Step [100/3535], Loss: 1.4377\n",
      "Epoch [4/30], Step [120/3535], Loss: 1.1528\n",
      "Epoch [4/30], Step [140/3535], Loss: 1.1667\n",
      "Epoch [4/30], Step [160/3535], Loss: 2.0813\n",
      "Epoch [4/30], Step [180/3535], Loss: 1.3410\n",
      "Epoch [4/30], Step [200/3535], Loss: 1.0660\n",
      "Epoch [4/30], Step [220/3535], Loss: 1.0992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30], Step [240/3535], Loss: 1.4351\n",
      "Epoch [4/30], Step [260/3535], Loss: 1.0547\n",
      "Epoch [4/30], Step [280/3535], Loss: 1.0577\n",
      "Epoch [4/30], Step [300/3535], Loss: 1.2299\n",
      "Epoch [4/30], Step [320/3535], Loss: 0.8473\n",
      "Epoch [4/30], Step [340/3535], Loss: 0.8995\n",
      "Epoch [4/30], Step [360/3535], Loss: 1.7943\n",
      "Epoch [4/30], Step [380/3535], Loss: 1.2832\n",
      "Epoch [4/30], Step [400/3535], Loss: 1.4476\n",
      "Epoch [4/30], Step [420/3535], Loss: 1.0985\n",
      "Epoch [4/30], Step [440/3535], Loss: 1.1595\n",
      "Epoch [4/30], Step [460/3535], Loss: 1.2165\n",
      "Epoch [4/30], Step [480/3535], Loss: 1.2100\n",
      "Epoch [4/30], Step [500/3535], Loss: 0.7375\n",
      "Epoch [4/30], Step [520/3535], Loss: 1.3640\n",
      "Epoch [4/30], Step [540/3535], Loss: 1.4294\n",
      "Epoch [4/30], Step [560/3535], Loss: 1.3315\n",
      "Epoch [4/30], Step [580/3535], Loss: 1.4867\n",
      "Epoch [4/30], Step [600/3535], Loss: 1.2851\n",
      "Epoch [4/30], Step [620/3535], Loss: 1.1041\n",
      "Epoch [4/30], Step [640/3535], Loss: 0.8898\n",
      "Epoch [4/30], Step [660/3535], Loss: 1.0616\n",
      "Epoch [4/30], Step [680/3535], Loss: 1.6025\n",
      "Epoch [4/30], Step [700/3535], Loss: 0.8715\n",
      "Epoch [4/30], Step [720/3535], Loss: 1.4126\n",
      "Epoch [4/30], Step [740/3535], Loss: 1.2269\n",
      "Epoch [4/30], Step [760/3535], Loss: 1.3452\n",
      "Epoch [4/30], Step [780/3535], Loss: 1.0040\n",
      "Epoch [4/30], Step [800/3535], Loss: 1.5838\n",
      "Epoch [4/30], Step [820/3535], Loss: 0.8860\n",
      "Epoch [4/30], Step [840/3535], Loss: 1.0165\n",
      "Epoch [4/30], Step [860/3535], Loss: 1.5910\n",
      "Epoch [4/30], Step [880/3535], Loss: 0.9734\n",
      "Epoch [4/30], Step [900/3535], Loss: 0.8663\n",
      "Epoch [4/30], Step [920/3535], Loss: 0.9966\n",
      "Epoch [4/30], Step [940/3535], Loss: 1.1032\n",
      "Epoch [4/30], Step [960/3535], Loss: 1.2166\n",
      "Epoch [4/30], Step [980/3535], Loss: 1.1382\n",
      "Epoch [4/30], Step [1000/3535], Loss: 1.5408\n",
      "Epoch [4/30], Step [1020/3535], Loss: 1.0052\n",
      "Epoch [4/30], Step [1040/3535], Loss: 1.1051\n",
      "Epoch [4/30], Step [1060/3535], Loss: 1.0669\n",
      "Epoch [4/30], Step [1080/3535], Loss: 0.9191\n",
      "Epoch [4/30], Step [1100/3535], Loss: 0.9757\n",
      "Epoch [4/30], Step [1120/3535], Loss: 0.9473\n",
      "Epoch [4/30], Step [1140/3535], Loss: 1.0100\n",
      "Epoch [4/30], Step [1160/3535], Loss: 1.0012\n",
      "Epoch [4/30], Step [1180/3535], Loss: 1.4107\n",
      "Epoch [4/30], Step [1200/3535], Loss: 1.3782\n",
      "Epoch [4/30], Step [1220/3535], Loss: 1.0933\n",
      "Epoch [4/30], Step [1240/3535], Loss: 1.0323\n",
      "Epoch [4/30], Step [1260/3535], Loss: 0.8838\n",
      "Epoch [4/30], Step [1280/3535], Loss: 1.0823\n",
      "Epoch [4/30], Step [1300/3535], Loss: 1.3777\n",
      "Epoch [4/30], Step [1320/3535], Loss: 1.1514\n",
      "Epoch [4/30], Step [1340/3535], Loss: 1.2018\n",
      "Epoch [4/30], Step [1360/3535], Loss: 1.2603\n",
      "Epoch [4/30], Step [1380/3535], Loss: 1.3937\n",
      "Epoch [4/30], Step [1400/3535], Loss: 0.8899\n",
      "Epoch [4/30], Step [1420/3535], Loss: 0.6600\n",
      "Epoch [4/30], Step [1440/3535], Loss: 1.3077\n",
      "Epoch [4/30], Step [1460/3535], Loss: 1.2810\n",
      "Epoch [4/30], Step [1480/3535], Loss: 1.3729\n",
      "Epoch [4/30], Step [1500/3535], Loss: 1.0821\n",
      "Epoch [4/30], Step [1520/3535], Loss: 0.9792\n",
      "Epoch [4/30], Step [1540/3535], Loss: 1.0449\n",
      "Epoch [4/30], Step [1560/3535], Loss: 1.3526\n",
      "Epoch [4/30], Step [1580/3535], Loss: 0.6479\n",
      "Epoch [4/30], Step [1600/3535], Loss: 1.2347\n",
      "Epoch [4/30], Step [1620/3535], Loss: 1.0009\n",
      "Epoch [4/30], Step [1640/3535], Loss: 0.8290\n",
      "Epoch [4/30], Step [1660/3535], Loss: 1.1899\n",
      "Epoch [4/30], Step [1680/3535], Loss: 2.0798\n",
      "Epoch [4/30], Step [1700/3535], Loss: 1.2007\n",
      "Epoch [4/30], Step [1720/3535], Loss: 1.6744\n",
      "Epoch [4/30], Step [1740/3535], Loss: 1.5273\n",
      "Epoch [4/30], Step [1760/3535], Loss: 0.9115\n",
      "Epoch [4/30], Step [1780/3535], Loss: 1.1207\n",
      "Epoch [4/30], Step [1800/3535], Loss: 1.1785\n",
      "Epoch [4/30], Step [1820/3535], Loss: 1.4488\n",
      "Epoch [4/30], Step [1840/3535], Loss: 1.5519\n",
      "Epoch [4/30], Step [1860/3535], Loss: 1.4584\n",
      "Epoch [4/30], Step [1880/3535], Loss: 0.9041\n",
      "Epoch [4/30], Step [1900/3535], Loss: 1.1032\n",
      "Epoch [4/30], Step [1920/3535], Loss: 1.0037\n",
      "Epoch [4/30], Step [1940/3535], Loss: 0.6583\n",
      "Epoch [4/30], Step [1960/3535], Loss: 0.9907\n",
      "Epoch [4/30], Step [1980/3535], Loss: 1.1984\n",
      "Epoch [4/30], Step [2000/3535], Loss: 1.9983\n",
      "Epoch [4/30], Step [2020/3535], Loss: 1.7469\n",
      "Epoch [4/30], Step [2040/3535], Loss: 1.2412\n",
      "Epoch [4/30], Step [2060/3535], Loss: 1.8832\n",
      "Epoch [4/30], Step [2080/3535], Loss: 1.1507\n",
      "Epoch [4/30], Step [2100/3535], Loss: 1.6861\n",
      "Epoch [4/30], Step [2120/3535], Loss: 1.1268\n",
      "Epoch [4/30], Step [2140/3535], Loss: 1.6269\n",
      "Epoch [4/30], Step [2160/3535], Loss: 0.4582\n",
      "Epoch [4/30], Step [2180/3535], Loss: 1.2227\n",
      "Epoch [4/30], Step [2200/3535], Loss: 0.7215\n",
      "Epoch [4/30], Step [2220/3535], Loss: 1.3768\n",
      "Epoch [4/30], Step [2240/3535], Loss: 1.1557\n",
      "Epoch [4/30], Step [2260/3535], Loss: 1.4000\n",
      "Epoch [4/30], Step [2280/3535], Loss: 1.0046\n",
      "Epoch [4/30], Step [2300/3535], Loss: 1.3501\n",
      "Epoch [4/30], Step [2320/3535], Loss: 1.2652\n",
      "Epoch [4/30], Step [2340/3535], Loss: 1.9111\n",
      "Epoch [4/30], Step [2360/3535], Loss: 1.3964\n",
      "Epoch [4/30], Step [2380/3535], Loss: 2.2427\n",
      "Epoch [4/30], Step [2400/3535], Loss: 1.4769\n",
      "Epoch [4/30], Step [2420/3535], Loss: 1.2175\n",
      "Epoch [4/30], Step [2440/3535], Loss: 1.3176\n",
      "Epoch [4/30], Step [2460/3535], Loss: 0.9537\n",
      "Epoch [4/30], Step [2480/3535], Loss: 1.2020\n",
      "Epoch [4/30], Step [2500/3535], Loss: 1.1248\n",
      "Epoch [4/30], Step [2520/3535], Loss: 0.8817\n",
      "Epoch [4/30], Step [2540/3535], Loss: 0.8585\n",
      "Epoch [4/30], Step [2560/3535], Loss: 1.1923\n",
      "Epoch [4/30], Step [2580/3535], Loss: 1.9573\n",
      "Epoch [4/30], Step [2600/3535], Loss: 0.8141\n",
      "Epoch [4/30], Step [2620/3535], Loss: 0.7185\n",
      "Epoch [4/30], Step [2640/3535], Loss: 1.1527\n",
      "Epoch [4/30], Step [2660/3535], Loss: 1.2925\n",
      "Epoch [4/30], Step [2680/3535], Loss: 1.4013\n",
      "Epoch [4/30], Step [2700/3535], Loss: 1.2576\n",
      "Epoch [4/30], Step [2720/3535], Loss: 1.2628\n",
      "Epoch [4/30], Step [2740/3535], Loss: 1.0237\n",
      "Epoch [4/30], Step [2760/3535], Loss: 1.4333\n",
      "Epoch [4/30], Step [2780/3535], Loss: 0.8995\n",
      "Epoch [4/30], Step [2800/3535], Loss: 0.9882\n",
      "Epoch [4/30], Step [2820/3535], Loss: 0.9809\n",
      "Epoch [4/30], Step [2840/3535], Loss: 0.9198\n",
      "Epoch [4/30], Step [2860/3535], Loss: 1.2180\n",
      "Epoch [4/30], Step [2880/3535], Loss: 1.0564\n",
      "Epoch [4/30], Step [2900/3535], Loss: 1.1518\n",
      "Epoch [4/30], Step [2920/3535], Loss: 1.3581\n",
      "Epoch [4/30], Step [2940/3535], Loss: 0.9664\n",
      "Epoch [4/30], Step [2960/3535], Loss: 1.1424\n",
      "Epoch [4/30], Step [2980/3535], Loss: 1.0657\n",
      "Epoch [4/30], Step [3000/3535], Loss: 1.3475\n",
      "Epoch [4/30], Step [3020/3535], Loss: 1.0434\n",
      "Epoch [4/30], Step [3040/3535], Loss: 1.0120\n",
      "Epoch [4/30], Step [3060/3535], Loss: 1.1473\n",
      "Epoch [4/30], Step [3080/3535], Loss: 1.2997\n",
      "Epoch [4/30], Step [3100/3535], Loss: 1.3897\n",
      "Epoch [4/30], Step [3120/3535], Loss: 1.1986\n",
      "Epoch [4/30], Step [3140/3535], Loss: 1.4573\n",
      "Epoch [4/30], Step [3160/3535], Loss: 1.0897\n",
      "Epoch [4/30], Step [3180/3535], Loss: 0.9345\n",
      "Epoch [4/30], Step [3200/3535], Loss: 1.8381\n",
      "Epoch [4/30], Step [3220/3535], Loss: 1.0849\n",
      "Epoch [4/30], Step [3240/3535], Loss: 1.1797\n",
      "Epoch [4/30], Step [3260/3535], Loss: 1.1282\n",
      "Epoch [4/30], Step [3280/3535], Loss: 0.8687\n",
      "Epoch [4/30], Step [3300/3535], Loss: 1.1932\n",
      "Epoch [4/30], Step [3320/3535], Loss: 1.8487\n",
      "Epoch [4/30], Step [3340/3535], Loss: 1.1448\n",
      "Epoch [4/30], Step [3360/3535], Loss: 1.5405\n",
      "Epoch [4/30], Step [3380/3535], Loss: 1.2640\n",
      "Epoch [4/30], Step [3400/3535], Loss: 1.0286\n",
      "Epoch [4/30], Step [3420/3535], Loss: 0.7056\n",
      "Epoch [4/30], Step [3440/3535], Loss: 1.2857\n",
      "Epoch [4/30], Step [3460/3535], Loss: 0.8157\n",
      "Epoch [4/30], Step [3480/3535], Loss: 0.6548\n",
      "Epoch [4/30], Step [3500/3535], Loss: 0.8924\n",
      "Epoch [4/30], Step [3520/3535], Loss: 1.3080\n",
      "\n",
      "train-loss: 1.4966, train-acc: 51.6783\n",
      "validation loss: 1.3800, validation acc: 53.7427\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 5\n",
      "\n",
      "Epoch [5/30], Step [0/3535], Loss: 1.2637\n",
      "Epoch [5/30], Step [20/3535], Loss: 1.5831\n",
      "Epoch [5/30], Step [40/3535], Loss: 1.1908\n",
      "Epoch [5/30], Step [60/3535], Loss: 1.5193\n",
      "Epoch [5/30], Step [80/3535], Loss: 1.2209\n",
      "Epoch [5/30], Step [100/3535], Loss: 0.8962\n",
      "Epoch [5/30], Step [120/3535], Loss: 0.9876\n",
      "Epoch [5/30], Step [140/3535], Loss: 1.4313\n",
      "Epoch [5/30], Step [160/3535], Loss: 0.8777\n",
      "Epoch [5/30], Step [180/3535], Loss: 1.1749\n",
      "Epoch [5/30], Step [200/3535], Loss: 1.6757\n",
      "Epoch [5/30], Step [220/3535], Loss: 1.1396\n",
      "Epoch [5/30], Step [240/3535], Loss: 0.7679\n",
      "Epoch [5/30], Step [260/3535], Loss: 0.7573\n",
      "Epoch [5/30], Step [280/3535], Loss: 1.8614\n",
      "Epoch [5/30], Step [300/3535], Loss: 1.0879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Step [320/3535], Loss: 0.9674\n",
      "Epoch [5/30], Step [340/3535], Loss: 1.3619\n",
      "Epoch [5/30], Step [360/3535], Loss: 0.8894\n",
      "Epoch [5/30], Step [380/3535], Loss: 1.0366\n",
      "Epoch [5/30], Step [400/3535], Loss: 1.5616\n",
      "Epoch [5/30], Step [420/3535], Loss: 0.7034\n",
      "Epoch [5/30], Step [440/3535], Loss: 0.9175\n",
      "Epoch [5/30], Step [460/3535], Loss: 1.6946\n",
      "Epoch [5/30], Step [480/3535], Loss: 0.7981\n",
      "Epoch [5/30], Step [500/3535], Loss: 0.7654\n",
      "Epoch [5/30], Step [520/3535], Loss: 0.9405\n",
      "Epoch [5/30], Step [540/3535], Loss: 1.1327\n",
      "Epoch [5/30], Step [560/3535], Loss: 0.5479\n",
      "Epoch [5/30], Step [580/3535], Loss: 0.9513\n",
      "Epoch [5/30], Step [600/3535], Loss: 0.7529\n",
      "Epoch [5/30], Step [620/3535], Loss: 0.9517\n",
      "Epoch [5/30], Step [640/3535], Loss: 1.1200\n",
      "Epoch [5/30], Step [660/3535], Loss: 1.4606\n",
      "Epoch [5/30], Step [680/3535], Loss: 1.0050\n",
      "Epoch [5/30], Step [700/3535], Loss: 1.6478\n",
      "Epoch [5/30], Step [720/3535], Loss: 0.6620\n",
      "Epoch [5/30], Step [740/3535], Loss: 0.8438\n",
      "Epoch [5/30], Step [760/3535], Loss: 0.6626\n",
      "Epoch [5/30], Step [780/3535], Loss: 1.2956\n",
      "Epoch [5/30], Step [800/3535], Loss: 0.9202\n",
      "Epoch [5/30], Step [820/3535], Loss: 1.2983\n",
      "Epoch [5/30], Step [840/3535], Loss: 0.4773\n",
      "Epoch [5/30], Step [860/3535], Loss: 1.2453\n",
      "Epoch [5/30], Step [880/3535], Loss: 0.8724\n",
      "Epoch [5/30], Step [900/3535], Loss: 1.1978\n",
      "Epoch [5/30], Step [920/3535], Loss: 0.9232\n",
      "Epoch [5/30], Step [940/3535], Loss: 1.4511\n",
      "Epoch [5/30], Step [960/3535], Loss: 0.7623\n",
      "Epoch [5/30], Step [980/3535], Loss: 1.0745\n",
      "Epoch [5/30], Step [1000/3535], Loss: 0.8090\n",
      "Epoch [5/30], Step [1020/3535], Loss: 2.2938\n",
      "Epoch [5/30], Step [1040/3535], Loss: 1.3183\n",
      "Epoch [5/30], Step [1060/3535], Loss: 1.2033\n",
      "Epoch [5/30], Step [1080/3535], Loss: 0.7133\n",
      "Epoch [5/30], Step [1100/3535], Loss: 1.0921\n",
      "Epoch [5/30], Step [1120/3535], Loss: 1.0663\n",
      "Epoch [5/30], Step [1140/3535], Loss: 1.2003\n",
      "Epoch [5/30], Step [1160/3535], Loss: 0.8371\n",
      "Epoch [5/30], Step [1180/3535], Loss: 1.2607\n",
      "Epoch [5/30], Step [1200/3535], Loss: 1.3506\n",
      "Epoch [5/30], Step [1220/3535], Loss: 0.6571\n",
      "Epoch [5/30], Step [1240/3535], Loss: 1.4133\n",
      "Epoch [5/30], Step [1260/3535], Loss: 0.9489\n",
      "Epoch [5/30], Step [1280/3535], Loss: 1.1914\n",
      "Epoch [5/30], Step [1300/3535], Loss: 1.7359\n",
      "Epoch [5/30], Step [1320/3535], Loss: 1.4264\n",
      "Epoch [5/30], Step [1340/3535], Loss: 1.0040\n",
      "Epoch [5/30], Step [1360/3535], Loss: 1.1764\n",
      "Epoch [5/30], Step [1380/3535], Loss: 0.8437\n",
      "Epoch [5/30], Step [1400/3535], Loss: 0.7864\n",
      "Epoch [5/30], Step [1420/3535], Loss: 1.0224\n",
      "Epoch [5/30], Step [1440/3535], Loss: 0.6247\n",
      "Epoch [5/30], Step [1460/3535], Loss: 1.1942\n",
      "Epoch [5/30], Step [1480/3535], Loss: 1.2376\n",
      "Epoch [5/30], Step [1500/3535], Loss: 1.2460\n",
      "Epoch [5/30], Step [1520/3535], Loss: 1.2061\n",
      "Epoch [5/30], Step [1540/3535], Loss: 1.5774\n",
      "Epoch [5/30], Step [1560/3535], Loss: 1.0427\n",
      "Epoch [5/30], Step [1580/3535], Loss: 0.8599\n",
      "Epoch [5/30], Step [1600/3535], Loss: 1.0469\n",
      "Epoch [5/30], Step [1620/3535], Loss: 1.9961\n",
      "Epoch [5/30], Step [1640/3535], Loss: 0.8525\n",
      "Epoch [5/30], Step [1660/3535], Loss: 1.1667\n",
      "Epoch [5/30], Step [1680/3535], Loss: 1.6472\n",
      "Epoch [5/30], Step [1700/3535], Loss: 0.8436\n",
      "Epoch [5/30], Step [1720/3535], Loss: 1.0862\n",
      "Epoch [5/30], Step [1740/3535], Loss: 1.1804\n",
      "Epoch [5/30], Step [1760/3535], Loss: 1.1602\n",
      "Epoch [5/30], Step [1780/3535], Loss: 1.3233\n",
      "Epoch [5/30], Step [1800/3535], Loss: 1.3725\n",
      "Epoch [5/30], Step [1820/3535], Loss: 1.2700\n",
      "Epoch [5/30], Step [1840/3535], Loss: 0.7357\n",
      "Epoch [5/30], Step [1860/3535], Loss: 1.5402\n",
      "Epoch [5/30], Step [1880/3535], Loss: 0.9058\n",
      "Epoch [5/30], Step [1900/3535], Loss: 1.2853\n",
      "Epoch [5/30], Step [1920/3535], Loss: 1.5304\n",
      "Epoch [5/30], Step [1940/3535], Loss: 1.6180\n",
      "Epoch [5/30], Step [1960/3535], Loss: 1.0936\n",
      "Epoch [5/30], Step [1980/3535], Loss: 1.0732\n",
      "Epoch [5/30], Step [2000/3535], Loss: 0.9020\n",
      "Epoch [5/30], Step [2020/3535], Loss: 1.2657\n",
      "Epoch [5/30], Step [2040/3535], Loss: 0.9998\n",
      "Epoch [5/30], Step [2060/3535], Loss: 0.7720\n",
      "Epoch [5/30], Step [2080/3535], Loss: 0.9182\n",
      "Epoch [5/30], Step [2100/3535], Loss: 1.2023\n",
      "Epoch [5/30], Step [2120/3535], Loss: 1.6665\n",
      "Epoch [5/30], Step [2140/3535], Loss: 0.8293\n",
      "Epoch [5/30], Step [2160/3535], Loss: 0.8427\n",
      "Epoch [5/30], Step [2180/3535], Loss: 0.8028\n",
      "Epoch [5/30], Step [2200/3535], Loss: 1.1164\n",
      "Epoch [5/30], Step [2220/3535], Loss: 0.9455\n",
      "Epoch [5/30], Step [2240/3535], Loss: 1.3224\n",
      "Epoch [5/30], Step [2260/3535], Loss: 1.1933\n",
      "Epoch [5/30], Step [2280/3535], Loss: 1.5406\n",
      "Epoch [5/30], Step [2300/3535], Loss: 1.2243\n",
      "Epoch [5/30], Step [2320/3535], Loss: 0.8402\n",
      "Epoch [5/30], Step [2340/3535], Loss: 1.2452\n",
      "Epoch [5/30], Step [2360/3535], Loss: 1.0316\n",
      "Epoch [5/30], Step [2380/3535], Loss: 1.0687\n",
      "Epoch [5/30], Step [2400/3535], Loss: 1.2986\n",
      "Epoch [5/30], Step [2420/3535], Loss: 1.4372\n",
      "Epoch [5/30], Step [2440/3535], Loss: 1.2811\n",
      "Epoch [5/30], Step [2460/3535], Loss: 1.2539\n",
      "Epoch [5/30], Step [2480/3535], Loss: 1.8884\n",
      "Epoch [5/30], Step [2500/3535], Loss: 1.4500\n",
      "Epoch [5/30], Step [2520/3535], Loss: 1.6267\n",
      "Epoch [5/30], Step [2540/3535], Loss: 0.7506\n",
      "Epoch [5/30], Step [2560/3535], Loss: 1.1516\n",
      "Epoch [5/30], Step [2580/3535], Loss: 1.1343\n",
      "Epoch [5/30], Step [2600/3535], Loss: 1.3430\n",
      "Epoch [5/30], Step [2620/3535], Loss: 1.2510\n",
      "Epoch [5/30], Step [2640/3535], Loss: 1.2404\n",
      "Epoch [5/30], Step [2660/3535], Loss: 1.8959\n",
      "Epoch [5/30], Step [2680/3535], Loss: 1.0466\n",
      "Epoch [5/30], Step [2700/3535], Loss: 1.0871\n",
      "Epoch [5/30], Step [2720/3535], Loss: 1.2927\n",
      "Epoch [5/30], Step [2740/3535], Loss: 1.5504\n",
      "Epoch [5/30], Step [2760/3535], Loss: 0.6206\n",
      "Epoch [5/30], Step [2780/3535], Loss: 0.9922\n",
      "Epoch [5/30], Step [2800/3535], Loss: 1.7629\n",
      "Epoch [5/30], Step [2820/3535], Loss: 1.1612\n",
      "Epoch [5/30], Step [2840/3535], Loss: 1.3175\n",
      "Epoch [5/30], Step [2860/3535], Loss: 1.5036\n",
      "Epoch [5/30], Step [2880/3535], Loss: 1.6683\n",
      "Epoch [5/30], Step [2900/3535], Loss: 0.8915\n",
      "Epoch [5/30], Step [2920/3535], Loss: 1.4756\n",
      "Epoch [5/30], Step [2940/3535], Loss: 0.9302\n",
      "Epoch [5/30], Step [2960/3535], Loss: 1.1636\n",
      "Epoch [5/30], Step [2980/3535], Loss: 0.8469\n",
      "Epoch [5/30], Step [3000/3535], Loss: 0.8519\n",
      "Epoch [5/30], Step [3020/3535], Loss: 0.8615\n",
      "Epoch [5/30], Step [3040/3535], Loss: 1.8123\n",
      "Epoch [5/30], Step [3060/3535], Loss: 1.6988\n",
      "Epoch [5/30], Step [3080/3535], Loss: 0.7580\n",
      "Epoch [5/30], Step [3100/3535], Loss: 1.1095\n",
      "Epoch [5/30], Step [3120/3535], Loss: 1.1707\n",
      "Epoch [5/30], Step [3140/3535], Loss: 0.8670\n",
      "Epoch [5/30], Step [3160/3535], Loss: 1.0837\n",
      "Epoch [5/30], Step [3180/3535], Loss: 0.7959\n",
      "Epoch [5/30], Step [3200/3535], Loss: 1.3701\n",
      "Epoch [5/30], Step [3220/3535], Loss: 0.9898\n",
      "Epoch [5/30], Step [3240/3535], Loss: 0.8233\n",
      "Epoch [5/30], Step [3260/3535], Loss: 1.3043\n",
      "Epoch [5/30], Step [3280/3535], Loss: 1.3626\n",
      "Epoch [5/30], Step [3300/3535], Loss: 1.2567\n",
      "Epoch [5/30], Step [3320/3535], Loss: 1.0129\n",
      "Epoch [5/30], Step [3340/3535], Loss: 0.7695\n",
      "Epoch [5/30], Step [3360/3535], Loss: 1.0671\n",
      "Epoch [5/30], Step [3380/3535], Loss: 1.0489\n",
      "Epoch [5/30], Step [3400/3535], Loss: 1.3587\n",
      "Epoch [5/30], Step [3420/3535], Loss: 1.9827\n",
      "Epoch [5/30], Step [3440/3535], Loss: 0.9186\n",
      "Epoch [5/30], Step [3460/3535], Loss: 0.7829\n",
      "Epoch [5/30], Step [3480/3535], Loss: 0.8475\n",
      "Epoch [5/30], Step [3500/3535], Loss: 1.5792\n",
      "Epoch [5/30], Step [3520/3535], Loss: 0.8874\n",
      "\n",
      "train-loss: 1.4308, train-acc: 54.7554\n",
      "validation loss: 1.3364, validation acc: 55.1295\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 6\n",
      "\n",
      "Epoch [6/30], Step [0/3535], Loss: 0.9852\n",
      "Epoch [6/30], Step [20/3535], Loss: 1.2542\n",
      "Epoch [6/30], Step [40/3535], Loss: 1.4351\n",
      "Epoch [6/30], Step [60/3535], Loss: 1.2358\n",
      "Epoch [6/30], Step [80/3535], Loss: 1.2980\n",
      "Epoch [6/30], Step [100/3535], Loss: 0.7826\n",
      "Epoch [6/30], Step [120/3535], Loss: 1.1319\n",
      "Epoch [6/30], Step [140/3535], Loss: 0.8916\n",
      "Epoch [6/30], Step [160/3535], Loss: 1.1954\n",
      "Epoch [6/30], Step [180/3535], Loss: 1.1325\n",
      "Epoch [6/30], Step [200/3535], Loss: 0.8255\n",
      "Epoch [6/30], Step [220/3535], Loss: 1.7761\n",
      "Epoch [6/30], Step [240/3535], Loss: 0.7564\n",
      "Epoch [6/30], Step [260/3535], Loss: 1.0093\n",
      "Epoch [6/30], Step [280/3535], Loss: 0.8738\n",
      "Epoch [6/30], Step [300/3535], Loss: 2.2377\n",
      "Epoch [6/30], Step [320/3535], Loss: 0.8526\n",
      "Epoch [6/30], Step [340/3535], Loss: 1.2179\n",
      "Epoch [6/30], Step [360/3535], Loss: 0.7601\n",
      "Epoch [6/30], Step [380/3535], Loss: 0.7310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30], Step [400/3535], Loss: 0.5365\n",
      "Epoch [6/30], Step [420/3535], Loss: 1.1833\n",
      "Epoch [6/30], Step [440/3535], Loss: 1.3216\n",
      "Epoch [6/30], Step [460/3535], Loss: 1.5488\n",
      "Epoch [6/30], Step [480/3535], Loss: 1.0387\n",
      "Epoch [6/30], Step [500/3535], Loss: 0.6876\n",
      "Epoch [6/30], Step [520/3535], Loss: 1.0149\n",
      "Epoch [6/30], Step [540/3535], Loss: 1.2174\n",
      "Epoch [6/30], Step [560/3535], Loss: 0.8386\n",
      "Epoch [6/30], Step [580/3535], Loss: 1.6195\n",
      "Epoch [6/30], Step [600/3535], Loss: 1.3632\n",
      "Epoch [6/30], Step [620/3535], Loss: 0.5621\n",
      "Epoch [6/30], Step [640/3535], Loss: 1.3113\n",
      "Epoch [6/30], Step [660/3535], Loss: 1.0567\n",
      "Epoch [6/30], Step [680/3535], Loss: 1.5896\n",
      "Epoch [6/30], Step [700/3535], Loss: 1.9497\n",
      "Epoch [6/30], Step [720/3535], Loss: 1.5275\n",
      "Epoch [6/30], Step [740/3535], Loss: 1.1490\n",
      "Epoch [6/30], Step [760/3535], Loss: 1.0894\n",
      "Epoch [6/30], Step [780/3535], Loss: 1.9200\n",
      "Epoch [6/30], Step [800/3535], Loss: 1.1914\n",
      "Epoch [6/30], Step [820/3535], Loss: 1.3247\n",
      "Epoch [6/30], Step [840/3535], Loss: 0.9399\n",
      "Epoch [6/30], Step [860/3535], Loss: 1.3148\n",
      "Epoch [6/30], Step [880/3535], Loss: 0.7061\n",
      "Epoch [6/30], Step [900/3535], Loss: 1.0394\n",
      "Epoch [6/30], Step [920/3535], Loss: 0.9218\n",
      "Epoch [6/30], Step [940/3535], Loss: 1.2164\n",
      "Epoch [6/30], Step [960/3535], Loss: 1.0298\n",
      "Epoch [6/30], Step [980/3535], Loss: 1.5641\n",
      "Epoch [6/30], Step [1000/3535], Loss: 1.5290\n",
      "Epoch [6/30], Step [1020/3535], Loss: 0.8507\n",
      "Epoch [6/30], Step [1040/3535], Loss: 1.8001\n",
      "Epoch [6/30], Step [1060/3535], Loss: 1.1133\n",
      "Epoch [6/30], Step [1080/3535], Loss: 0.6798\n",
      "Epoch [6/30], Step [1100/3535], Loss: 0.7554\n",
      "Epoch [6/30], Step [1120/3535], Loss: 1.2314\n",
      "Epoch [6/30], Step [1140/3535], Loss: 1.2824\n",
      "Epoch [6/30], Step [1160/3535], Loss: 1.2183\n",
      "Epoch [6/30], Step [1180/3535], Loss: 1.0344\n",
      "Epoch [6/30], Step [1200/3535], Loss: 1.0737\n",
      "Epoch [6/30], Step [1220/3535], Loss: 0.9766\n",
      "Epoch [6/30], Step [1240/3535], Loss: 1.8018\n",
      "Epoch [6/30], Step [1260/3535], Loss: 1.0023\n",
      "Epoch [6/30], Step [1280/3535], Loss: 1.2348\n",
      "Epoch [6/30], Step [1300/3535], Loss: 1.2030\n",
      "Epoch [6/30], Step [1320/3535], Loss: 1.6465\n",
      "Epoch [6/30], Step [1340/3535], Loss: 1.4334\n",
      "Epoch [6/30], Step [1360/3535], Loss: 1.3195\n",
      "Epoch [6/30], Step [1380/3535], Loss: 0.6373\n",
      "Epoch [6/30], Step [1400/3535], Loss: 1.5269\n",
      "Epoch [6/30], Step [1420/3535], Loss: 0.7777\n",
      "Epoch [6/30], Step [1440/3535], Loss: 1.3270\n",
      "Epoch [6/30], Step [1460/3535], Loss: 0.7787\n",
      "Epoch [6/30], Step [1480/3535], Loss: 0.6039\n",
      "Epoch [6/30], Step [1500/3535], Loss: 1.0461\n",
      "Epoch [6/30], Step [1520/3535], Loss: 1.3010\n",
      "Epoch [6/30], Step [1540/3535], Loss: 0.9062\n",
      "Epoch [6/30], Step [1560/3535], Loss: 1.0579\n",
      "Epoch [6/30], Step [1580/3535], Loss: 1.0146\n",
      "Epoch [6/30], Step [1600/3535], Loss: 1.6353\n",
      "Epoch [6/30], Step [1620/3535], Loss: 0.8719\n",
      "Epoch [6/30], Step [1640/3535], Loss: 1.3876\n",
      "Epoch [6/30], Step [1660/3535], Loss: 0.9689\n",
      "Epoch [6/30], Step [1680/3535], Loss: 0.7156\n",
      "Epoch [6/30], Step [1700/3535], Loss: 1.5581\n",
      "Epoch [6/30], Step [1720/3535], Loss: 0.8508\n",
      "Epoch [6/30], Step [1740/3535], Loss: 1.4160\n",
      "Epoch [6/30], Step [1760/3535], Loss: 1.8436\n",
      "Epoch [6/30], Step [1780/3535], Loss: 1.1981\n",
      "Epoch [6/30], Step [1800/3535], Loss: 0.8171\n",
      "Epoch [6/30], Step [1820/3535], Loss: 1.3589\n",
      "Epoch [6/30], Step [1840/3535], Loss: 1.8445\n",
      "Epoch [6/30], Step [1860/3535], Loss: 1.0794\n",
      "Epoch [6/30], Step [1880/3535], Loss: 1.5293\n",
      "Epoch [6/30], Step [1900/3535], Loss: 1.1311\n",
      "Epoch [6/30], Step [1920/3535], Loss: 0.6978\n",
      "Epoch [6/30], Step [1940/3535], Loss: 0.8070\n",
      "Epoch [6/30], Step [1960/3535], Loss: 1.4092\n",
      "Epoch [6/30], Step [1980/3535], Loss: 0.9947\n",
      "Epoch [6/30], Step [2000/3535], Loss: 1.5947\n",
      "Epoch [6/30], Step [2020/3535], Loss: 1.3891\n",
      "Epoch [6/30], Step [2040/3535], Loss: 1.8196\n",
      "Epoch [6/30], Step [2060/3535], Loss: 2.0215\n",
      "Epoch [6/30], Step [2080/3535], Loss: 1.1355\n",
      "Epoch [6/30], Step [2100/3535], Loss: 1.1168\n",
      "Epoch [6/30], Step [2120/3535], Loss: 0.9056\n",
      "Epoch [6/30], Step [2140/3535], Loss: 1.0082\n",
      "Epoch [6/30], Step [2160/3535], Loss: 1.0330\n",
      "Epoch [6/30], Step [2180/3535], Loss: 1.2589\n",
      "Epoch [6/30], Step [2200/3535], Loss: 0.8874\n",
      "Epoch [6/30], Step [2220/3535], Loss: 0.6526\n",
      "Epoch [6/30], Step [2240/3535], Loss: 0.6535\n",
      "Epoch [6/30], Step [2260/3535], Loss: 0.6403\n",
      "Epoch [6/30], Step [2280/3535], Loss: 1.2278\n",
      "Epoch [6/30], Step [2300/3535], Loss: 1.3702\n",
      "Epoch [6/30], Step [2320/3535], Loss: 0.9925\n",
      "Epoch [6/30], Step [2340/3535], Loss: 0.7221\n",
      "Epoch [6/30], Step [2360/3535], Loss: 0.6480\n",
      "Epoch [6/30], Step [2380/3535], Loss: 1.0015\n",
      "Epoch [6/30], Step [2400/3535], Loss: 1.5655\n",
      "Epoch [6/30], Step [2420/3535], Loss: 0.7700\n",
      "Epoch [6/30], Step [2440/3535], Loss: 1.7412\n",
      "Epoch [6/30], Step [2460/3535], Loss: 1.6633\n",
      "Epoch [6/30], Step [2480/3535], Loss: 1.5359\n",
      "Epoch [6/30], Step [2500/3535], Loss: 0.9310\n",
      "Epoch [6/30], Step [2520/3535], Loss: 1.0542\n",
      "Epoch [6/30], Step [2540/3535], Loss: 1.2008\n",
      "Epoch [6/30], Step [2560/3535], Loss: 1.1502\n",
      "Epoch [6/30], Step [2580/3535], Loss: 1.0349\n",
      "Epoch [6/30], Step [2600/3535], Loss: 1.0846\n",
      "Epoch [6/30], Step [2620/3535], Loss: 0.9554\n",
      "Epoch [6/30], Step [2640/3535], Loss: 0.9740\n",
      "Epoch [6/30], Step [2660/3535], Loss: 0.8921\n",
      "Epoch [6/30], Step [2680/3535], Loss: 1.4105\n",
      "Epoch [6/30], Step [2700/3535], Loss: 0.6882\n",
      "Epoch [6/30], Step [2720/3535], Loss: 1.3627\n",
      "Epoch [6/30], Step [2740/3535], Loss: 1.0352\n",
      "Epoch [6/30], Step [2760/3535], Loss: 1.6620\n",
      "Epoch [6/30], Step [2780/3535], Loss: 0.8325\n",
      "Epoch [6/30], Step [2800/3535], Loss: 1.6348\n",
      "Epoch [6/30], Step [2820/3535], Loss: 1.0119\n",
      "Epoch [6/30], Step [2840/3535], Loss: 1.1424\n",
      "Epoch [6/30], Step [2860/3535], Loss: 0.6537\n",
      "Epoch [6/30], Step [2880/3535], Loss: 1.1188\n",
      "Epoch [6/30], Step [2900/3535], Loss: 1.0872\n",
      "Epoch [6/30], Step [2920/3535], Loss: 1.4192\n",
      "Epoch [6/30], Step [2940/3535], Loss: 1.6050\n",
      "Epoch [6/30], Step [2960/3535], Loss: 1.2452\n",
      "Epoch [6/30], Step [2980/3535], Loss: 0.7054\n",
      "Epoch [6/30], Step [3000/3535], Loss: 0.8744\n",
      "Epoch [6/30], Step [3020/3535], Loss: 1.1786\n",
      "Epoch [6/30], Step [3040/3535], Loss: 1.2318\n",
      "Epoch [6/30], Step [3060/3535], Loss: 0.8211\n",
      "Epoch [6/30], Step [3080/3535], Loss: 1.3658\n",
      "Epoch [6/30], Step [3100/3535], Loss: 1.7066\n",
      "Epoch [6/30], Step [3120/3535], Loss: 0.6643\n",
      "Epoch [6/30], Step [3140/3535], Loss: 1.2871\n",
      "Epoch [6/30], Step [3160/3535], Loss: 1.1478\n",
      "Epoch [6/30], Step [3180/3535], Loss: 1.0636\n",
      "Epoch [6/30], Step [3200/3535], Loss: 0.9817\n",
      "Epoch [6/30], Step [3220/3535], Loss: 1.0332\n",
      "Epoch [6/30], Step [3240/3535], Loss: 0.9204\n",
      "Epoch [6/30], Step [3260/3535], Loss: 0.8031\n",
      "Epoch [6/30], Step [3280/3535], Loss: 0.8411\n",
      "Epoch [6/30], Step [3300/3535], Loss: 1.6252\n",
      "Epoch [6/30], Step [3320/3535], Loss: 1.2965\n",
      "Epoch [6/30], Step [3340/3535], Loss: 1.0569\n",
      "Epoch [6/30], Step [3360/3535], Loss: 1.2024\n",
      "Epoch [6/30], Step [3380/3535], Loss: 1.1081\n",
      "Epoch [6/30], Step [3400/3535], Loss: 1.1551\n",
      "Epoch [6/30], Step [3420/3535], Loss: 1.6870\n",
      "Epoch [6/30], Step [3440/3535], Loss: 1.1172\n",
      "Epoch [6/30], Step [3460/3535], Loss: 1.0881\n",
      "Epoch [6/30], Step [3480/3535], Loss: 0.7771\n",
      "Epoch [6/30], Step [3500/3535], Loss: 0.7762\n",
      "Epoch [6/30], Step [3520/3535], Loss: 1.1712\n",
      "\n",
      "train-loss: 1.3755, train-acc: 57.9387\n",
      "validation loss: 1.3026, validation acc: 56.2473\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 7\n",
      "\n",
      "Epoch [7/30], Step [0/3535], Loss: 0.6957\n",
      "Epoch [7/30], Step [20/3535], Loss: 1.4946\n",
      "Epoch [7/30], Step [40/3535], Loss: 1.2420\n",
      "Epoch [7/30], Step [60/3535], Loss: 1.0296\n",
      "Epoch [7/30], Step [80/3535], Loss: 2.0018\n",
      "Epoch [7/30], Step [100/3535], Loss: 0.9337\n",
      "Epoch [7/30], Step [120/3535], Loss: 1.5034\n",
      "Epoch [7/30], Step [140/3535], Loss: 0.6648\n",
      "Epoch [7/30], Step [160/3535], Loss: 1.1547\n",
      "Epoch [7/30], Step [180/3535], Loss: 1.1739\n",
      "Epoch [7/30], Step [200/3535], Loss: 0.8085\n",
      "Epoch [7/30], Step [220/3535], Loss: 1.5398\n",
      "Epoch [7/30], Step [240/3535], Loss: 1.0940\n",
      "Epoch [7/30], Step [260/3535], Loss: 0.8960\n",
      "Epoch [7/30], Step [280/3535], Loss: 0.9437\n",
      "Epoch [7/30], Step [300/3535], Loss: 1.4617\n",
      "Epoch [7/30], Step [320/3535], Loss: 0.4306\n",
      "Epoch [7/30], Step [340/3535], Loss: 1.2148\n",
      "Epoch [7/30], Step [360/3535], Loss: 0.6382\n",
      "Epoch [7/30], Step [380/3535], Loss: 0.9765\n",
      "Epoch [7/30], Step [400/3535], Loss: 1.6207\n",
      "Epoch [7/30], Step [420/3535], Loss: 0.8875\n",
      "Epoch [7/30], Step [440/3535], Loss: 0.7773\n",
      "Epoch [7/30], Step [460/3535], Loss: 0.5816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], Step [480/3535], Loss: 1.5126\n",
      "Epoch [7/30], Step [500/3535], Loss: 1.3923\n",
      "Epoch [7/30], Step [520/3535], Loss: 1.3995\n",
      "Epoch [7/30], Step [540/3535], Loss: 1.4393\n",
      "Epoch [7/30], Step [560/3535], Loss: 0.7096\n",
      "Epoch [7/30], Step [580/3535], Loss: 1.2674\n",
      "Epoch [7/30], Step [600/3535], Loss: 0.8093\n",
      "Epoch [7/30], Step [620/3535], Loss: 0.8980\n",
      "Epoch [7/30], Step [640/3535], Loss: 0.7915\n",
      "Epoch [7/30], Step [660/3535], Loss: 1.8060\n",
      "Epoch [7/30], Step [680/3535], Loss: 0.9040\n",
      "Epoch [7/30], Step [700/3535], Loss: 0.7435\n",
      "Epoch [7/30], Step [720/3535], Loss: 0.8580\n",
      "Epoch [7/30], Step [740/3535], Loss: 0.9831\n",
      "Epoch [7/30], Step [760/3535], Loss: 0.8872\n",
      "Epoch [7/30], Step [780/3535], Loss: 1.1935\n",
      "Epoch [7/30], Step [800/3535], Loss: 1.2831\n",
      "Epoch [7/30], Step [820/3535], Loss: 1.1350\n",
      "Epoch [7/30], Step [840/3535], Loss: 1.0044\n",
      "Epoch [7/30], Step [860/3535], Loss: 0.5527\n",
      "Epoch [7/30], Step [880/3535], Loss: 1.1622\n",
      "Epoch [7/30], Step [900/3535], Loss: 1.4267\n",
      "Epoch [7/30], Step [920/3535], Loss: 0.6375\n",
      "Epoch [7/30], Step [940/3535], Loss: 0.5854\n",
      "Epoch [7/30], Step [960/3535], Loss: 1.4026\n",
      "Epoch [7/30], Step [980/3535], Loss: 1.4241\n",
      "Epoch [7/30], Step [1000/3535], Loss: 0.8000\n",
      "Epoch [7/30], Step [1020/3535], Loss: 1.1188\n",
      "Epoch [7/30], Step [1040/3535], Loss: 1.1911\n",
      "Epoch [7/30], Step [1060/3535], Loss: 1.0913\n",
      "Epoch [7/30], Step [1080/3535], Loss: 0.6664\n",
      "Epoch [7/30], Step [1100/3535], Loss: 1.2214\n",
      "Epoch [7/30], Step [1120/3535], Loss: 0.9907\n",
      "Epoch [7/30], Step [1140/3535], Loss: 0.7852\n",
      "Epoch [7/30], Step [1160/3535], Loss: 1.6613\n",
      "Epoch [7/30], Step [1180/3535], Loss: 1.4942\n",
      "Epoch [7/30], Step [1200/3535], Loss: 0.7333\n",
      "Epoch [7/30], Step [1220/3535], Loss: 1.4882\n",
      "Epoch [7/30], Step [1240/3535], Loss: 0.4491\n",
      "Epoch [7/30], Step [1260/3535], Loss: 1.2271\n",
      "Epoch [7/30], Step [1280/3535], Loss: 1.3207\n",
      "Epoch [7/30], Step [1300/3535], Loss: 0.6582\n",
      "Epoch [7/30], Step [1320/3535], Loss: 0.8248\n",
      "Epoch [7/30], Step [1340/3535], Loss: 0.5923\n",
      "Epoch [7/30], Step [1360/3535], Loss: 1.0449\n",
      "Epoch [7/30], Step [1380/3535], Loss: 1.2408\n",
      "Epoch [7/30], Step [1400/3535], Loss: 1.1827\n",
      "Epoch [7/30], Step [1420/3535], Loss: 0.9529\n",
      "Epoch [7/30], Step [1440/3535], Loss: 0.4343\n",
      "Epoch [7/30], Step [1460/3535], Loss: 2.0351\n",
      "Epoch [7/30], Step [1480/3535], Loss: 1.3900\n",
      "Epoch [7/30], Step [1500/3535], Loss: 1.1536\n",
      "Epoch [7/30], Step [1520/3535], Loss: 1.2299\n",
      "Epoch [7/30], Step [1540/3535], Loss: 0.9181\n",
      "Epoch [7/30], Step [1560/3535], Loss: 0.5151\n",
      "Epoch [7/30], Step [1580/3535], Loss: 0.9869\n",
      "Epoch [7/30], Step [1600/3535], Loss: 0.7297\n",
      "Epoch [7/30], Step [1620/3535], Loss: 1.0496\n",
      "Epoch [7/30], Step [1640/3535], Loss: 0.9766\n",
      "Epoch [7/30], Step [1660/3535], Loss: 0.3792\n",
      "Epoch [7/30], Step [1680/3535], Loss: 1.2742\n",
      "Epoch [7/30], Step [1700/3535], Loss: 1.0366\n",
      "Epoch [7/30], Step [1720/3535], Loss: 1.1110\n",
      "Epoch [7/30], Step [1740/3535], Loss: 0.8635\n",
      "Epoch [7/30], Step [1760/3535], Loss: 0.9023\n",
      "Epoch [7/30], Step [1780/3535], Loss: 0.8909\n",
      "Epoch [7/30], Step [1800/3535], Loss: 0.8876\n",
      "Epoch [7/30], Step [1820/3535], Loss: 0.4207\n",
      "Epoch [7/30], Step [1840/3535], Loss: 0.7836\n",
      "Epoch [7/30], Step [1860/3535], Loss: 1.0635\n",
      "Epoch [7/30], Step [1880/3535], Loss: 1.0865\n",
      "Epoch [7/30], Step [1900/3535], Loss: 0.7892\n",
      "Epoch [7/30], Step [1920/3535], Loss: 1.4553\n",
      "Epoch [7/30], Step [1940/3535], Loss: 0.9212\n",
      "Epoch [7/30], Step [1960/3535], Loss: 0.8495\n",
      "Epoch [7/30], Step [1980/3535], Loss: 1.5178\n",
      "Epoch [7/30], Step [2000/3535], Loss: 1.1927\n",
      "Epoch [7/30], Step [2020/3535], Loss: 1.6849\n",
      "Epoch [7/30], Step [2040/3535], Loss: 0.8062\n",
      "Epoch [7/30], Step [2060/3535], Loss: 1.3155\n",
      "Epoch [7/30], Step [2080/3535], Loss: 0.8281\n",
      "Epoch [7/30], Step [2100/3535], Loss: 0.5076\n",
      "Epoch [7/30], Step [2120/3535], Loss: 0.8339\n",
      "Epoch [7/30], Step [2140/3535], Loss: 0.9002\n",
      "Epoch [7/30], Step [2160/3535], Loss: 1.0165\n",
      "Epoch [7/30], Step [2180/3535], Loss: 0.7456\n",
      "Epoch [7/30], Step [2200/3535], Loss: 1.0439\n",
      "Epoch [7/30], Step [2220/3535], Loss: 0.9926\n",
      "Epoch [7/30], Step [2240/3535], Loss: 1.1128\n",
      "Epoch [7/30], Step [2260/3535], Loss: 1.1543\n",
      "Epoch [7/30], Step [2280/3535], Loss: 0.8051\n",
      "Epoch [7/30], Step [2300/3535], Loss: 0.8335\n",
      "Epoch [7/30], Step [2320/3535], Loss: 0.7490\n",
      "Epoch [7/30], Step [2340/3535], Loss: 0.8565\n",
      "Epoch [7/30], Step [2360/3535], Loss: 0.8836\n",
      "Epoch [7/30], Step [2380/3535], Loss: 0.7653\n",
      "Epoch [7/30], Step [2400/3535], Loss: 1.0314\n",
      "Epoch [7/30], Step [2420/3535], Loss: 0.9732\n",
      "Epoch [7/30], Step [2440/3535], Loss: 1.1102\n",
      "Epoch [7/30], Step [2460/3535], Loss: 0.8574\n",
      "Epoch [7/30], Step [2480/3535], Loss: 1.8962\n",
      "Epoch [7/30], Step [2500/3535], Loss: 1.4383\n",
      "Epoch [7/30], Step [2520/3535], Loss: 0.6256\n",
      "Epoch [7/30], Step [2540/3535], Loss: 0.3689\n",
      "Epoch [7/30], Step [2560/3535], Loss: 0.7939\n",
      "Epoch [7/30], Step [2580/3535], Loss: 0.8848\n",
      "Epoch [7/30], Step [2600/3535], Loss: 1.1350\n",
      "Epoch [7/30], Step [2620/3535], Loss: 0.8716\n",
      "Epoch [7/30], Step [2640/3535], Loss: 0.6624\n",
      "Epoch [7/30], Step [2660/3535], Loss: 0.9104\n",
      "Epoch [7/30], Step [2680/3535], Loss: 1.2773\n",
      "Epoch [7/30], Step [2700/3535], Loss: 1.7009\n",
      "Epoch [7/30], Step [2720/3535], Loss: 0.4116\n",
      "Epoch [7/30], Step [2740/3535], Loss: 0.9460\n",
      "Epoch [7/30], Step [2760/3535], Loss: 1.5034\n",
      "Epoch [7/30], Step [2780/3535], Loss: 0.5439\n",
      "Epoch [7/30], Step [2800/3535], Loss: 1.1698\n",
      "Epoch [7/30], Step [2820/3535], Loss: 0.9142\n",
      "Epoch [7/30], Step [2840/3535], Loss: 1.1771\n",
      "Epoch [7/30], Step [2860/3535], Loss: 1.1054\n",
      "Epoch [7/30], Step [2880/3535], Loss: 0.7622\n",
      "Epoch [7/30], Step [2900/3535], Loss: 0.8242\n",
      "Epoch [7/30], Step [2920/3535], Loss: 0.8827\n",
      "Epoch [7/30], Step [2940/3535], Loss: 0.9557\n",
      "Epoch [7/30], Step [2960/3535], Loss: 0.5682\n",
      "Epoch [7/30], Step [2980/3535], Loss: 1.6967\n",
      "Epoch [7/30], Step [3000/3535], Loss: 0.8056\n",
      "Epoch [7/30], Step [3020/3535], Loss: 0.5433\n",
      "Epoch [7/30], Step [3040/3535], Loss: 0.9158\n",
      "Epoch [7/30], Step [3060/3535], Loss: 1.0666\n",
      "Epoch [7/30], Step [3080/3535], Loss: 1.1550\n",
      "Epoch [7/30], Step [3100/3535], Loss: 1.2211\n",
      "Epoch [7/30], Step [3120/3535], Loss: 0.7883\n",
      "Epoch [7/30], Step [3140/3535], Loss: 1.5491\n",
      "Epoch [7/30], Step [3160/3535], Loss: 0.9138\n",
      "Epoch [7/30], Step [3180/3535], Loss: 1.2336\n",
      "Epoch [7/30], Step [3200/3535], Loss: 0.6884\n",
      "Epoch [7/30], Step [3220/3535], Loss: 0.7059\n",
      "Epoch [7/30], Step [3240/3535], Loss: 0.7614\n",
      "Epoch [7/30], Step [3260/3535], Loss: 1.2658\n",
      "Epoch [7/30], Step [3280/3535], Loss: 1.6109\n",
      "Epoch [7/30], Step [3300/3535], Loss: 1.1077\n",
      "Epoch [7/30], Step [3320/3535], Loss: 1.1894\n",
      "Epoch [7/30], Step [3340/3535], Loss: 1.8099\n",
      "Epoch [7/30], Step [3360/3535], Loss: 1.3941\n",
      "Epoch [7/30], Step [3380/3535], Loss: 0.9884\n",
      "Epoch [7/30], Step [3400/3535], Loss: 1.3355\n",
      "Epoch [7/30], Step [3420/3535], Loss: 1.2047\n",
      "Epoch [7/30], Step [3440/3535], Loss: 1.1374\n",
      "Epoch [7/30], Step [3460/3535], Loss: 0.6002\n",
      "Epoch [7/30], Step [3480/3535], Loss: 1.6794\n",
      "Epoch [7/30], Step [3500/3535], Loss: 0.9547\n",
      "Epoch [7/30], Step [3520/3535], Loss: 0.9478\n",
      "\n",
      "train-loss: 1.3290, train-acc: 59.7850\n",
      "validation loss: 1.2676, validation acc: 59.4170\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 8\n",
      "\n",
      "Epoch [8/30], Step [0/3535], Loss: 0.8420\n",
      "Epoch [8/30], Step [20/3535], Loss: 1.0988\n",
      "Epoch [8/30], Step [40/3535], Loss: 1.0723\n",
      "Epoch [8/30], Step [60/3535], Loss: 0.5851\n",
      "Epoch [8/30], Step [80/3535], Loss: 1.1436\n",
      "Epoch [8/30], Step [100/3535], Loss: 1.0333\n",
      "Epoch [8/30], Step [120/3535], Loss: 0.6015\n",
      "Epoch [8/30], Step [140/3535], Loss: 0.6753\n",
      "Epoch [8/30], Step [160/3535], Loss: 1.2049\n",
      "Epoch [8/30], Step [180/3535], Loss: 1.2659\n",
      "Epoch [8/30], Step [200/3535], Loss: 0.5249\n",
      "Epoch [8/30], Step [220/3535], Loss: 1.0142\n",
      "Epoch [8/30], Step [240/3535], Loss: 0.6377\n",
      "Epoch [8/30], Step [260/3535], Loss: 1.5122\n",
      "Epoch [8/30], Step [280/3535], Loss: 1.0910\n",
      "Epoch [8/30], Step [300/3535], Loss: 1.1716\n",
      "Epoch [8/30], Step [320/3535], Loss: 1.1417\n",
      "Epoch [8/30], Step [340/3535], Loss: 0.9798\n",
      "Epoch [8/30], Step [360/3535], Loss: 1.2549\n",
      "Epoch [8/30], Step [380/3535], Loss: 1.0715\n",
      "Epoch [8/30], Step [400/3535], Loss: 0.5145\n",
      "Epoch [8/30], Step [420/3535], Loss: 1.6799\n",
      "Epoch [8/30], Step [440/3535], Loss: 0.9602\n",
      "Epoch [8/30], Step [460/3535], Loss: 1.3373\n",
      "Epoch [8/30], Step [480/3535], Loss: 0.9215\n",
      "Epoch [8/30], Step [500/3535], Loss: 1.0008\n",
      "Epoch [8/30], Step [520/3535], Loss: 0.8728\n",
      "Epoch [8/30], Step [540/3535], Loss: 0.5012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30], Step [560/3535], Loss: 0.6033\n",
      "Epoch [8/30], Step [580/3535], Loss: 0.6337\n",
      "Epoch [8/30], Step [600/3535], Loss: 1.2541\n",
      "Epoch [8/30], Step [620/3535], Loss: 0.8705\n",
      "Epoch [8/30], Step [640/3535], Loss: 0.6250\n",
      "Epoch [8/30], Step [660/3535], Loss: 0.5052\n",
      "Epoch [8/30], Step [680/3535], Loss: 0.8235\n",
      "Epoch [8/30], Step [700/3535], Loss: 1.2491\n",
      "Epoch [8/30], Step [720/3535], Loss: 1.0592\n",
      "Epoch [8/30], Step [740/3535], Loss: 0.9243\n",
      "Epoch [8/30], Step [760/3535], Loss: 0.5162\n",
      "Epoch [8/30], Step [780/3535], Loss: 0.9700\n",
      "Epoch [8/30], Step [800/3535], Loss: 1.2335\n",
      "Epoch [8/30], Step [820/3535], Loss: 0.5713\n",
      "Epoch [8/30], Step [840/3535], Loss: 0.8890\n",
      "Epoch [8/30], Step [860/3535], Loss: 0.8812\n",
      "Epoch [8/30], Step [880/3535], Loss: 1.2669\n",
      "Epoch [8/30], Step [900/3535], Loss: 0.9058\n",
      "Epoch [8/30], Step [920/3535], Loss: 0.7699\n",
      "Epoch [8/30], Step [940/3535], Loss: 0.3293\n",
      "Epoch [8/30], Step [960/3535], Loss: 0.7131\n",
      "Epoch [8/30], Step [980/3535], Loss: 0.8415\n",
      "Epoch [8/30], Step [1000/3535], Loss: 0.4075\n",
      "Epoch [8/30], Step [1020/3535], Loss: 1.0359\n",
      "Epoch [8/30], Step [1040/3535], Loss: 0.8694\n",
      "Epoch [8/30], Step [1060/3535], Loss: 1.0539\n",
      "Epoch [8/30], Step [1080/3535], Loss: 1.0472\n",
      "Epoch [8/30], Step [1100/3535], Loss: 1.1172\n",
      "Epoch [8/30], Step [1120/3535], Loss: 1.1660\n",
      "Epoch [8/30], Step [1140/3535], Loss: 1.1675\n",
      "Epoch [8/30], Step [1160/3535], Loss: 0.8384\n",
      "Epoch [8/30], Step [1180/3535], Loss: 1.4064\n",
      "Epoch [8/30], Step [1200/3535], Loss: 0.6788\n",
      "Epoch [8/30], Step [1220/3535], Loss: 0.9055\n",
      "Epoch [8/30], Step [1240/3535], Loss: 1.1768\n",
      "Epoch [8/30], Step [1260/3535], Loss: 1.1848\n",
      "Epoch [8/30], Step [1280/3535], Loss: 1.5830\n",
      "Epoch [8/30], Step [1300/3535], Loss: 0.7585\n",
      "Epoch [8/30], Step [1320/3535], Loss: 1.1858\n",
      "Epoch [8/30], Step [1340/3535], Loss: 1.1528\n",
      "Epoch [8/30], Step [1360/3535], Loss: 1.4783\n",
      "Epoch [8/30], Step [1380/3535], Loss: 1.5391\n",
      "Epoch [8/30], Step [1400/3535], Loss: 0.9696\n",
      "Epoch [8/30], Step [1420/3535], Loss: 0.5920\n",
      "Epoch [8/30], Step [1440/3535], Loss: 0.9901\n",
      "Epoch [8/30], Step [1460/3535], Loss: 0.4604\n",
      "Epoch [8/30], Step [1480/3535], Loss: 0.6947\n",
      "Epoch [8/30], Step [1500/3535], Loss: 0.7550\n",
      "Epoch [8/30], Step [1520/3535], Loss: 1.2513\n",
      "Epoch [8/30], Step [1540/3535], Loss: 0.8237\n",
      "Epoch [8/30], Step [1560/3535], Loss: 1.6100\n",
      "Epoch [8/30], Step [1580/3535], Loss: 1.1255\n",
      "Epoch [8/30], Step [1600/3535], Loss: 1.3695\n",
      "Epoch [8/30], Step [1620/3535], Loss: 0.5232\n",
      "Epoch [8/30], Step [1640/3535], Loss: 1.3351\n",
      "Epoch [8/30], Step [1660/3535], Loss: 0.6043\n",
      "Epoch [8/30], Step [1680/3535], Loss: 0.9615\n",
      "Epoch [8/30], Step [1700/3535], Loss: 1.4168\n",
      "Epoch [8/30], Step [1720/3535], Loss: 0.7353\n",
      "Epoch [8/30], Step [1740/3535], Loss: 1.0106\n",
      "Epoch [8/30], Step [1760/3535], Loss: 1.3899\n",
      "Epoch [8/30], Step [1780/3535], Loss: 0.6807\n",
      "Epoch [8/30], Step [1800/3535], Loss: 1.0287\n",
      "Epoch [8/30], Step [1820/3535], Loss: 0.5221\n",
      "Epoch [8/30], Step [1840/3535], Loss: 1.3165\n",
      "Epoch [8/30], Step [1860/3535], Loss: 0.4704\n",
      "Epoch [8/30], Step [1880/3535], Loss: 1.3249\n",
      "Epoch [8/30], Step [1900/3535], Loss: 0.7179\n",
      "Epoch [8/30], Step [1920/3535], Loss: 0.6784\n",
      "Epoch [8/30], Step [1940/3535], Loss: 0.7121\n",
      "Epoch [8/30], Step [1960/3535], Loss: 0.7798\n",
      "Epoch [8/30], Step [1980/3535], Loss: 0.7371\n",
      "Epoch [8/30], Step [2000/3535], Loss: 1.9665\n",
      "Epoch [8/30], Step [2020/3535], Loss: 0.7002\n",
      "Epoch [8/30], Step [2040/3535], Loss: 0.9413\n",
      "Epoch [8/30], Step [2060/3535], Loss: 1.2110\n",
      "Epoch [8/30], Step [2080/3535], Loss: 1.0764\n",
      "Epoch [8/30], Step [2100/3535], Loss: 0.8170\n",
      "Epoch [8/30], Step [2120/3535], Loss: 0.9213\n",
      "Epoch [8/30], Step [2140/3535], Loss: 0.4850\n",
      "Epoch [8/30], Step [2160/3535], Loss: 0.9674\n",
      "Epoch [8/30], Step [2180/3535], Loss: 0.3230\n",
      "Epoch [8/30], Step [2200/3535], Loss: 1.1963\n",
      "Epoch [8/30], Step [2220/3535], Loss: 1.5158\n",
      "Epoch [8/30], Step [2240/3535], Loss: 1.3740\n",
      "Epoch [8/30], Step [2260/3535], Loss: 1.4130\n",
      "Epoch [8/30], Step [2280/3535], Loss: 1.8692\n",
      "Epoch [8/30], Step [2300/3535], Loss: 0.8208\n",
      "Epoch [8/30], Step [2320/3535], Loss: 1.2570\n",
      "Epoch [8/30], Step [2340/3535], Loss: 1.1273\n",
      "Epoch [8/30], Step [2360/3535], Loss: 1.3731\n",
      "Epoch [8/30], Step [2380/3535], Loss: 0.4547\n",
      "Epoch [8/30], Step [2400/3535], Loss: 1.5151\n",
      "Epoch [8/30], Step [2420/3535], Loss: 1.0529\n",
      "Epoch [8/30], Step [2440/3535], Loss: 1.4249\n",
      "Epoch [8/30], Step [2460/3535], Loss: 1.2838\n",
      "Epoch [8/30], Step [2480/3535], Loss: 1.4621\n",
      "Epoch [8/30], Step [2500/3535], Loss: 1.3222\n",
      "Epoch [8/30], Step [2520/3535], Loss: 1.5677\n",
      "Epoch [8/30], Step [2540/3535], Loss: 1.4929\n",
      "Epoch [8/30], Step [2560/3535], Loss: 0.9120\n",
      "Epoch [8/30], Step [2580/3535], Loss: 1.1009\n",
      "Epoch [8/30], Step [2600/3535], Loss: 1.1748\n",
      "Epoch [8/30], Step [2620/3535], Loss: 0.8868\n",
      "Epoch [8/30], Step [2640/3535], Loss: 1.0262\n",
      "Epoch [8/30], Step [2660/3535], Loss: 0.4884\n",
      "Epoch [8/30], Step [2680/3535], Loss: 1.1476\n",
      "Epoch [8/30], Step [2700/3535], Loss: 0.6601\n",
      "Epoch [8/30], Step [2720/3535], Loss: 1.1194\n",
      "Epoch [8/30], Step [2740/3535], Loss: 1.2944\n",
      "Epoch [8/30], Step [2760/3535], Loss: 1.2598\n",
      "Epoch [8/30], Step [2780/3535], Loss: 0.6830\n",
      "Epoch [8/30], Step [2800/3535], Loss: 1.0907\n",
      "Epoch [8/30], Step [2820/3535], Loss: 1.2043\n",
      "Epoch [8/30], Step [2840/3535], Loss: 1.3239\n",
      "Epoch [8/30], Step [2860/3535], Loss: 1.2221\n",
      "Epoch [8/30], Step [2880/3535], Loss: 1.1493\n",
      "Epoch [8/30], Step [2900/3535], Loss: 0.9779\n",
      "Epoch [8/30], Step [2920/3535], Loss: 1.1429\n",
      "Epoch [8/30], Step [2940/3535], Loss: 1.2688\n",
      "Epoch [8/30], Step [2960/3535], Loss: 0.8828\n",
      "Epoch [8/30], Step [2980/3535], Loss: 1.2661\n",
      "Epoch [8/30], Step [3000/3535], Loss: 1.3201\n",
      "Epoch [8/30], Step [3020/3535], Loss: 0.8052\n",
      "Epoch [8/30], Step [3040/3535], Loss: 0.7544\n",
      "Epoch [8/30], Step [3060/3535], Loss: 1.0734\n",
      "Epoch [8/30], Step [3080/3535], Loss: 0.8359\n",
      "Epoch [8/30], Step [3100/3535], Loss: 0.8674\n",
      "Epoch [8/30], Step [3120/3535], Loss: 0.8804\n",
      "Epoch [8/30], Step [3140/3535], Loss: 0.9323\n",
      "Epoch [8/30], Step [3160/3535], Loss: 1.6417\n",
      "Epoch [8/30], Step [3180/3535], Loss: 0.8441\n",
      "Epoch [8/30], Step [3200/3535], Loss: 1.6193\n",
      "Epoch [8/30], Step [3220/3535], Loss: 0.9896\n",
      "Epoch [8/30], Step [3240/3535], Loss: 0.6800\n",
      "Epoch [8/30], Step [3260/3535], Loss: 1.2908\n",
      "Epoch [8/30], Step [3280/3535], Loss: 0.8755\n",
      "Epoch [8/30], Step [3300/3535], Loss: 0.6789\n",
      "Epoch [8/30], Step [3320/3535], Loss: 1.4772\n",
      "Epoch [8/30], Step [3340/3535], Loss: 1.0284\n",
      "Epoch [8/30], Step [3360/3535], Loss: 0.8000\n",
      "Epoch [8/30], Step [3380/3535], Loss: 1.4858\n",
      "Epoch [8/30], Step [3400/3535], Loss: 1.0369\n",
      "Epoch [8/30], Step [3420/3535], Loss: 0.8590\n",
      "Epoch [8/30], Step [3440/3535], Loss: 0.8960\n",
      "Epoch [8/30], Step [3460/3535], Loss: 0.8403\n",
      "Epoch [8/30], Step [3480/3535], Loss: 0.9118\n",
      "Epoch [8/30], Step [3500/3535], Loss: 0.6819\n",
      "Epoch [8/30], Step [3520/3535], Loss: 1.4610\n",
      "\n",
      "train-loss: 1.2883, train-acc: 61.7020\n",
      "validation loss: 1.2424, validation acc: 58.4972\n",
      "\n",
      "Epoch 9\n",
      "\n",
      "Epoch [9/30], Step [0/3535], Loss: 1.2042\n",
      "Epoch [9/30], Step [20/3535], Loss: 0.8270\n",
      "Epoch [9/30], Step [40/3535], Loss: 0.7210\n",
      "Epoch [9/30], Step [60/3535], Loss: 1.2386\n",
      "Epoch [9/30], Step [80/3535], Loss: 0.8011\n",
      "Epoch [9/30], Step [100/3535], Loss: 0.6093\n",
      "Epoch [9/30], Step [120/3535], Loss: 1.3315\n",
      "Epoch [9/30], Step [140/3535], Loss: 0.6638\n",
      "Epoch [9/30], Step [160/3535], Loss: 1.2304\n",
      "Epoch [9/30], Step [180/3535], Loss: 0.6529\n",
      "Epoch [9/30], Step [200/3535], Loss: 1.1302\n",
      "Epoch [9/30], Step [220/3535], Loss: 0.8551\n",
      "Epoch [9/30], Step [240/3535], Loss: 0.8332\n",
      "Epoch [9/30], Step [260/3535], Loss: 0.5232\n",
      "Epoch [9/30], Step [280/3535], Loss: 0.8835\n",
      "Epoch [9/30], Step [300/3535], Loss: 1.2356\n",
      "Epoch [9/30], Step [320/3535], Loss: 1.1382\n",
      "Epoch [9/30], Step [340/3535], Loss: 0.8739\n",
      "Epoch [9/30], Step [360/3535], Loss: 1.0527\n",
      "Epoch [9/30], Step [380/3535], Loss: 0.7826\n",
      "Epoch [9/30], Step [400/3535], Loss: 0.6819\n",
      "Epoch [9/30], Step [420/3535], Loss: 0.8164\n",
      "Epoch [9/30], Step [440/3535], Loss: 1.3913\n",
      "Epoch [9/30], Step [460/3535], Loss: 0.8340\n",
      "Epoch [9/30], Step [480/3535], Loss: 1.6194\n",
      "Epoch [9/30], Step [500/3535], Loss: 1.0914\n",
      "Epoch [9/30], Step [520/3535], Loss: 0.9521\n",
      "Epoch [9/30], Step [540/3535], Loss: 0.3306\n",
      "Epoch [9/30], Step [560/3535], Loss: 1.0710\n",
      "Epoch [9/30], Step [580/3535], Loss: 0.5275\n",
      "Epoch [9/30], Step [600/3535], Loss: 1.6900\n",
      "Epoch [9/30], Step [620/3535], Loss: 0.9292\n",
      "Epoch [9/30], Step [640/3535], Loss: 0.8247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30], Step [660/3535], Loss: 0.6907\n",
      "Epoch [9/30], Step [680/3535], Loss: 1.8335\n",
      "Epoch [9/30], Step [700/3535], Loss: 0.6339\n",
      "Epoch [9/30], Step [720/3535], Loss: 0.6596\n",
      "Epoch [9/30], Step [740/3535], Loss: 0.4212\n",
      "Epoch [9/30], Step [760/3535], Loss: 1.5490\n",
      "Epoch [9/30], Step [780/3535], Loss: 1.1063\n",
      "Epoch [9/30], Step [800/3535], Loss: 0.6248\n",
      "Epoch [9/30], Step [820/3535], Loss: 0.9915\n",
      "Epoch [9/30], Step [840/3535], Loss: 1.2360\n",
      "Epoch [9/30], Step [860/3535], Loss: 0.5324\n",
      "Epoch [9/30], Step [880/3535], Loss: 0.6795\n",
      "Epoch [9/30], Step [900/3535], Loss: 1.0149\n",
      "Epoch [9/30], Step [920/3535], Loss: 0.8158\n",
      "Epoch [9/30], Step [940/3535], Loss: 1.4321\n",
      "Epoch [9/30], Step [960/3535], Loss: 0.6135\n",
      "Epoch [9/30], Step [980/3535], Loss: 0.6545\n",
      "Epoch [9/30], Step [1000/3535], Loss: 1.0343\n",
      "Epoch [9/30], Step [1020/3535], Loss: 1.0700\n",
      "Epoch [9/30], Step [1040/3535], Loss: 1.6071\n",
      "Epoch [9/30], Step [1060/3535], Loss: 1.1383\n",
      "Epoch [9/30], Step [1080/3535], Loss: 0.3288\n",
      "Epoch [9/30], Step [1100/3535], Loss: 0.6693\n",
      "Epoch [9/30], Step [1120/3535], Loss: 1.0860\n",
      "Epoch [9/30], Step [1140/3535], Loss: 0.6114\n",
      "Epoch [9/30], Step [1160/3535], Loss: 0.7105\n",
      "Epoch [9/30], Step [1180/3535], Loss: 1.4341\n",
      "Epoch [9/30], Step [1200/3535], Loss: 0.5820\n",
      "Epoch [9/30], Step [1220/3535], Loss: 0.8247\n",
      "Epoch [9/30], Step [1240/3535], Loss: 0.8997\n",
      "Epoch [9/30], Step [1260/3535], Loss: 1.3990\n",
      "Epoch [9/30], Step [1280/3535], Loss: 0.6971\n",
      "Epoch [9/30], Step [1300/3535], Loss: 0.9721\n",
      "Epoch [9/30], Step [1320/3535], Loss: 1.2045\n",
      "Epoch [9/30], Step [1340/3535], Loss: 0.8166\n",
      "Epoch [9/30], Step [1360/3535], Loss: 0.9833\n",
      "Epoch [9/30], Step [1380/3535], Loss: 0.8040\n",
      "Epoch [9/30], Step [1400/3535], Loss: 0.6810\n",
      "Epoch [9/30], Step [1420/3535], Loss: 1.0546\n",
      "Epoch [9/30], Step [1440/3535], Loss: 0.7436\n",
      "Epoch [9/30], Step [1460/3535], Loss: 1.1530\n",
      "Epoch [9/30], Step [1480/3535], Loss: 1.4343\n",
      "Epoch [9/30], Step [1500/3535], Loss: 0.7447\n",
      "Epoch [9/30], Step [1520/3535], Loss: 0.8247\n",
      "Epoch [9/30], Step [1540/3535], Loss: 0.6725\n",
      "Epoch [9/30], Step [1560/3535], Loss: 1.2025\n",
      "Epoch [9/30], Step [1580/3535], Loss: 1.2234\n",
      "Epoch [9/30], Step [1600/3535], Loss: 0.7891\n",
      "Epoch [9/30], Step [1620/3535], Loss: 0.4367\n",
      "Epoch [9/30], Step [1640/3535], Loss: 0.9617\n",
      "Epoch [9/30], Step [1660/3535], Loss: 0.7988\n",
      "Epoch [9/30], Step [1680/3535], Loss: 1.2368\n",
      "Epoch [9/30], Step [1700/3535], Loss: 0.5906\n",
      "Epoch [9/30], Step [1720/3535], Loss: 0.8374\n",
      "Epoch [9/30], Step [1740/3535], Loss: 0.8812\n",
      "Epoch [9/30], Step [1760/3535], Loss: 1.5661\n",
      "Epoch [9/30], Step [1780/3535], Loss: 1.6730\n",
      "Epoch [9/30], Step [1800/3535], Loss: 1.3479\n",
      "Epoch [9/30], Step [1820/3535], Loss: 1.5503\n",
      "Epoch [9/30], Step [1840/3535], Loss: 0.8404\n",
      "Epoch [9/30], Step [1860/3535], Loss: 1.1431\n",
      "Epoch [9/30], Step [1880/3535], Loss: 0.8772\n",
      "Epoch [9/30], Step [1900/3535], Loss: 1.1333\n",
      "Epoch [9/30], Step [1920/3535], Loss: 1.8434\n",
      "Epoch [9/30], Step [1940/3535], Loss: 1.2803\n",
      "Epoch [9/30], Step [1960/3535], Loss: 0.7596\n",
      "Epoch [9/30], Step [1980/3535], Loss: 1.0566\n",
      "Epoch [9/30], Step [2000/3535], Loss: 0.9915\n",
      "Epoch [9/30], Step [2020/3535], Loss: 1.0996\n",
      "Epoch [9/30], Step [2040/3535], Loss: 0.9083\n",
      "Epoch [9/30], Step [2060/3535], Loss: 1.3544\n",
      "Epoch [9/30], Step [2080/3535], Loss: 0.5485\n",
      "Epoch [9/30], Step [2100/3535], Loss: 1.1595\n",
      "Epoch [9/30], Step [2120/3535], Loss: 1.2839\n",
      "Epoch [9/30], Step [2140/3535], Loss: 0.6193\n",
      "Epoch [9/30], Step [2160/3535], Loss: 0.6165\n",
      "Epoch [9/30], Step [2180/3535], Loss: 1.5041\n",
      "Epoch [9/30], Step [2200/3535], Loss: 1.3361\n",
      "Epoch [9/30], Step [2220/3535], Loss: 0.8010\n",
      "Epoch [9/30], Step [2240/3535], Loss: 0.7961\n",
      "Epoch [9/30], Step [2260/3535], Loss: 0.8350\n",
      "Epoch [9/30], Step [2280/3535], Loss: 0.9644\n",
      "Epoch [9/30], Step [2300/3535], Loss: 1.2159\n",
      "Epoch [9/30], Step [2320/3535], Loss: 0.7655\n",
      "Epoch [9/30], Step [2340/3535], Loss: 1.2384\n",
      "Epoch [9/30], Step [2360/3535], Loss: 0.5411\n",
      "Epoch [9/30], Step [2380/3535], Loss: 0.8562\n",
      "Epoch [9/30], Step [2400/3535], Loss: 2.0264\n",
      "Epoch [9/30], Step [2420/3535], Loss: 1.3289\n",
      "Epoch [9/30], Step [2440/3535], Loss: 0.9074\n",
      "Epoch [9/30], Step [2460/3535], Loss: 0.6331\n",
      "Epoch [9/30], Step [2480/3535], Loss: 0.9168\n",
      "Epoch [9/30], Step [2500/3535], Loss: 1.1513\n",
      "Epoch [9/30], Step [2520/3535], Loss: 1.0651\n",
      "Epoch [9/30], Step [2540/3535], Loss: 0.7852\n",
      "Epoch [9/30], Step [2560/3535], Loss: 0.7186\n",
      "Epoch [9/30], Step [2580/3535], Loss: 0.4646\n",
      "Epoch [9/30], Step [2600/3535], Loss: 0.6268\n",
      "Epoch [9/30], Step [2620/3535], Loss: 0.7341\n",
      "Epoch [9/30], Step [2640/3535], Loss: 0.6618\n",
      "Epoch [9/30], Step [2660/3535], Loss: 0.9342\n",
      "Epoch [9/30], Step [2680/3535], Loss: 0.7623\n",
      "Epoch [9/30], Step [2700/3535], Loss: 0.6957\n",
      "Epoch [9/30], Step [2720/3535], Loss: 0.8117\n",
      "Epoch [9/30], Step [2740/3535], Loss: 0.7929\n",
      "Epoch [9/30], Step [2760/3535], Loss: 0.8549\n",
      "Epoch [9/30], Step [2780/3535], Loss: 1.7303\n",
      "Epoch [9/30], Step [2800/3535], Loss: 0.5477\n",
      "Epoch [9/30], Step [2820/3535], Loss: 0.8741\n",
      "Epoch [9/30], Step [2840/3535], Loss: 1.0203\n",
      "Epoch [9/30], Step [2860/3535], Loss: 0.3171\n",
      "Epoch [9/30], Step [2880/3535], Loss: 0.5152\n",
      "Epoch [9/30], Step [2900/3535], Loss: 0.4647\n",
      "Epoch [9/30], Step [2920/3535], Loss: 0.9467\n",
      "Epoch [9/30], Step [2940/3535], Loss: 0.9943\n",
      "Epoch [9/30], Step [2960/3535], Loss: 0.7035\n",
      "Epoch [9/30], Step [2980/3535], Loss: 0.7739\n",
      "Epoch [9/30], Step [3000/3535], Loss: 1.1966\n",
      "Epoch [9/30], Step [3020/3535], Loss: 0.6004\n",
      "Epoch [9/30], Step [3040/3535], Loss: 1.2341\n",
      "Epoch [9/30], Step [3060/3535], Loss: 0.9504\n",
      "Epoch [9/30], Step [3080/3535], Loss: 1.2649\n",
      "Epoch [9/30], Step [3100/3535], Loss: 0.9552\n",
      "Epoch [9/30], Step [3120/3535], Loss: 1.0178\n",
      "Epoch [9/30], Step [3140/3535], Loss: 0.7316\n",
      "Epoch [9/30], Step [3160/3535], Loss: 1.5695\n",
      "Epoch [9/30], Step [3180/3535], Loss: 1.0796\n",
      "Epoch [9/30], Step [3200/3535], Loss: 1.0524\n",
      "Epoch [9/30], Step [3220/3535], Loss: 0.8046\n",
      "Epoch [9/30], Step [3240/3535], Loss: 0.5239\n",
      "Epoch [9/30], Step [3260/3535], Loss: 1.2120\n",
      "Epoch [9/30], Step [3280/3535], Loss: 0.9292\n",
      "Epoch [9/30], Step [3300/3535], Loss: 1.1255\n",
      "Epoch [9/30], Step [3320/3535], Loss: 1.4256\n",
      "Epoch [9/30], Step [3340/3535], Loss: 1.4099\n",
      "Epoch [9/30], Step [3360/3535], Loss: 1.0193\n",
      "Epoch [9/30], Step [3380/3535], Loss: 0.9669\n",
      "Epoch [9/30], Step [3400/3535], Loss: 0.8470\n",
      "Epoch [9/30], Step [3420/3535], Loss: 0.8806\n",
      "Epoch [9/30], Step [3440/3535], Loss: 1.2742\n",
      "Epoch [9/30], Step [3460/3535], Loss: 0.6247\n",
      "Epoch [9/30], Step [3480/3535], Loss: 0.7305\n",
      "Epoch [9/30], Step [3500/3535], Loss: 0.6702\n",
      "Epoch [9/30], Step [3520/3535], Loss: 0.9993\n",
      "\n",
      "train-loss: 1.2518, train-acc: 63.6013\n",
      "validation loss: 1.2220, validation acc: 59.5727\n",
      "\n",
      "Epoch 10\n",
      "\n",
      "Epoch [10/30], Step [0/3535], Loss: 1.0993\n",
      "Epoch [10/30], Step [20/3535], Loss: 0.8535\n",
      "Epoch [10/30], Step [40/3535], Loss: 0.7814\n",
      "Epoch [10/30], Step [60/3535], Loss: 0.4968\n",
      "Epoch [10/30], Step [80/3535], Loss: 0.6843\n",
      "Epoch [10/30], Step [100/3535], Loss: 0.9067\n",
      "Epoch [10/30], Step [120/3535], Loss: 0.9867\n",
      "Epoch [10/30], Step [140/3535], Loss: 1.1809\n",
      "Epoch [10/30], Step [160/3535], Loss: 1.7003\n",
      "Epoch [10/30], Step [180/3535], Loss: 0.9947\n",
      "Epoch [10/30], Step [200/3535], Loss: 0.9434\n",
      "Epoch [10/30], Step [220/3535], Loss: 0.9123\n",
      "Epoch [10/30], Step [240/3535], Loss: 0.5445\n",
      "Epoch [10/30], Step [260/3535], Loss: 0.9063\n",
      "Epoch [10/30], Step [280/3535], Loss: 1.5199\n",
      "Epoch [10/30], Step [300/3535], Loss: 0.9566\n",
      "Epoch [10/30], Step [320/3535], Loss: 0.7576\n",
      "Epoch [10/30], Step [340/3535], Loss: 0.8415\n",
      "Epoch [10/30], Step [360/3535], Loss: 1.0567\n",
      "Epoch [10/30], Step [380/3535], Loss: 0.3943\n",
      "Epoch [10/30], Step [400/3535], Loss: 1.1791\n",
      "Epoch [10/30], Step [420/3535], Loss: 0.7111\n",
      "Epoch [10/30], Step [440/3535], Loss: 0.9226\n",
      "Epoch [10/30], Step [460/3535], Loss: 1.2435\n",
      "Epoch [10/30], Step [480/3535], Loss: 0.9328\n",
      "Epoch [10/30], Step [500/3535], Loss: 0.7895\n",
      "Epoch [10/30], Step [520/3535], Loss: 0.4482\n",
      "Epoch [10/30], Step [540/3535], Loss: 0.5887\n",
      "Epoch [10/30], Step [560/3535], Loss: 0.9771\n",
      "Epoch [10/30], Step [580/3535], Loss: 0.7343\n",
      "Epoch [10/30], Step [600/3535], Loss: 0.9624\n",
      "Epoch [10/30], Step [620/3535], Loss: 0.7253\n",
      "Epoch [10/30], Step [640/3535], Loss: 1.2358\n",
      "Epoch [10/30], Step [660/3535], Loss: 1.2289\n",
      "Epoch [10/30], Step [680/3535], Loss: 0.5149\n",
      "Epoch [10/30], Step [700/3535], Loss: 0.8934\n",
      "Epoch [10/30], Step [720/3535], Loss: 0.9498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Step [740/3535], Loss: 0.5863\n",
      "Epoch [10/30], Step [760/3535], Loss: 0.4444\n",
      "Epoch [10/30], Step [780/3535], Loss: 0.4631\n",
      "Epoch [10/30], Step [800/3535], Loss: 1.0813\n",
      "Epoch [10/30], Step [820/3535], Loss: 1.0090\n",
      "Epoch [10/30], Step [840/3535], Loss: 1.1064\n",
      "Epoch [10/30], Step [860/3535], Loss: 1.1199\n",
      "Epoch [10/30], Step [880/3535], Loss: 1.6745\n",
      "Epoch [10/30], Step [900/3535], Loss: 0.6323\n",
      "Epoch [10/30], Step [920/3535], Loss: 1.1213\n",
      "Epoch [10/30], Step [940/3535], Loss: 2.7244\n",
      "Epoch [10/30], Step [960/3535], Loss: 0.4699\n",
      "Epoch [10/30], Step [980/3535], Loss: 1.1917\n",
      "Epoch [10/30], Step [1000/3535], Loss: 0.8516\n",
      "Epoch [10/30], Step [1020/3535], Loss: 1.3668\n",
      "Epoch [10/30], Step [1040/3535], Loss: 0.5508\n",
      "Epoch [10/30], Step [1060/3535], Loss: 1.3021\n",
      "Epoch [10/30], Step [1080/3535], Loss: 1.1778\n",
      "Epoch [10/30], Step [1100/3535], Loss: 1.1572\n",
      "Epoch [10/30], Step [1120/3535], Loss: 0.8577\n",
      "Epoch [10/30], Step [1140/3535], Loss: 1.2878\n",
      "Epoch [10/30], Step [1160/3535], Loss: 0.8491\n",
      "Epoch [10/30], Step [1180/3535], Loss: 1.0330\n",
      "Epoch [10/30], Step [1200/3535], Loss: 1.1677\n",
      "Epoch [10/30], Step [1220/3535], Loss: 0.9050\n",
      "Epoch [10/30], Step [1240/3535], Loss: 0.7385\n",
      "Epoch [10/30], Step [1260/3535], Loss: 0.5486\n",
      "Epoch [10/30], Step [1280/3535], Loss: 0.6387\n",
      "Epoch [10/30], Step [1300/3535], Loss: 1.4543\n",
      "Epoch [10/30], Step [1320/3535], Loss: 0.4457\n",
      "Epoch [10/30], Step [1340/3535], Loss: 0.7937\n",
      "Epoch [10/30], Step [1360/3535], Loss: 0.8321\n",
      "Epoch [10/30], Step [1380/3535], Loss: 0.4651\n",
      "Epoch [10/30], Step [1400/3535], Loss: 0.6863\n",
      "Epoch [10/30], Step [1420/3535], Loss: 1.1853\n",
      "Epoch [10/30], Step [1440/3535], Loss: 0.6566\n",
      "Epoch [10/30], Step [1460/3535], Loss: 0.8589\n",
      "Epoch [10/30], Step [1480/3535], Loss: 0.4833\n",
      "Epoch [10/30], Step [1500/3535], Loss: 0.6848\n",
      "Epoch [10/30], Step [1520/3535], Loss: 0.7635\n",
      "Epoch [10/30], Step [1540/3535], Loss: 0.9004\n",
      "Epoch [10/30], Step [1560/3535], Loss: 1.1434\n",
      "Epoch [10/30], Step [1580/3535], Loss: 0.3579\n",
      "Epoch [10/30], Step [1600/3535], Loss: 0.7112\n",
      "Epoch [10/30], Step [1620/3535], Loss: 0.4414\n",
      "Epoch [10/30], Step [1640/3535], Loss: 1.4976\n",
      "Epoch [10/30], Step [1660/3535], Loss: 1.0269\n",
      "Epoch [10/30], Step [1680/3535], Loss: 0.9258\n",
      "Epoch [10/30], Step [1700/3535], Loss: 0.8384\n",
      "Epoch [10/30], Step [1720/3535], Loss: 0.3764\n",
      "Epoch [10/30], Step [1740/3535], Loss: 0.3782\n",
      "Epoch [10/30], Step [1760/3535], Loss: 0.9149\n",
      "Epoch [10/30], Step [1780/3535], Loss: 0.3231\n",
      "Epoch [10/30], Step [1800/3535], Loss: 1.2389\n",
      "Epoch [10/30], Step [1820/3535], Loss: 0.9366\n",
      "Epoch [10/30], Step [1840/3535], Loss: 1.4893\n",
      "Epoch [10/30], Step [1860/3535], Loss: 0.8848\n",
      "Epoch [10/30], Step [1880/3535], Loss: 0.7390\n",
      "Epoch [10/30], Step [1900/3535], Loss: 1.0894\n",
      "Epoch [10/30], Step [1920/3535], Loss: 1.9421\n",
      "Epoch [10/30], Step [1940/3535], Loss: 1.0921\n",
      "Epoch [10/30], Step [1960/3535], Loss: 1.1742\n",
      "Epoch [10/30], Step [1980/3535], Loss: 1.4625\n",
      "Epoch [10/30], Step [2000/3535], Loss: 0.8151\n",
      "Epoch [10/30], Step [2020/3535], Loss: 0.5000\n",
      "Epoch [10/30], Step [2040/3535], Loss: 1.6807\n",
      "Epoch [10/30], Step [2060/3535], Loss: 0.5425\n",
      "Epoch [10/30], Step [2080/3535], Loss: 1.0332\n",
      "Epoch [10/30], Step [2100/3535], Loss: 1.0734\n",
      "Epoch [10/30], Step [2120/3535], Loss: 0.7592\n",
      "Epoch [10/30], Step [2140/3535], Loss: 1.0257\n",
      "Epoch [10/30], Step [2160/3535], Loss: 1.1568\n",
      "Epoch [10/30], Step [2180/3535], Loss: 0.7061\n",
      "Epoch [10/30], Step [2200/3535], Loss: 0.6734\n",
      "Epoch [10/30], Step [2220/3535], Loss: 0.4650\n",
      "Epoch [10/30], Step [2240/3535], Loss: 0.8594\n",
      "Epoch [10/30], Step [2260/3535], Loss: 0.8501\n",
      "Epoch [10/30], Step [2280/3535], Loss: 1.1957\n",
      "Epoch [10/30], Step [2300/3535], Loss: 0.9746\n",
      "Epoch [10/30], Step [2320/3535], Loss: 1.0982\n",
      "Epoch [10/30], Step [2340/3535], Loss: 1.3249\n",
      "Epoch [10/30], Step [2360/3535], Loss: 0.8998\n",
      "Epoch [10/30], Step [2380/3535], Loss: 0.8782\n",
      "Epoch [10/30], Step [2400/3535], Loss: 1.1854\n",
      "Epoch [10/30], Step [2420/3535], Loss: 0.8457\n",
      "Epoch [10/30], Step [2440/3535], Loss: 0.3864\n",
      "Epoch [10/30], Step [2460/3535], Loss: 1.0177\n",
      "Epoch [10/30], Step [2480/3535], Loss: 1.9038\n",
      "Epoch [10/30], Step [2500/3535], Loss: 0.3315\n",
      "Epoch [10/30], Step [2520/3535], Loss: 1.3706\n",
      "Epoch [10/30], Step [2540/3535], Loss: 1.5220\n",
      "Epoch [10/30], Step [2560/3535], Loss: 1.1908\n",
      "Epoch [10/30], Step [2580/3535], Loss: 0.4679\n",
      "Epoch [10/30], Step [2600/3535], Loss: 1.0063\n",
      "Epoch [10/30], Step [2620/3535], Loss: 1.0277\n",
      "Epoch [10/30], Step [2640/3535], Loss: 1.1726\n",
      "Epoch [10/30], Step [2660/3535], Loss: 0.5279\n",
      "Epoch [10/30], Step [2680/3535], Loss: 0.5023\n",
      "Epoch [10/30], Step [2700/3535], Loss: 1.0619\n",
      "Epoch [10/30], Step [2720/3535], Loss: 1.0572\n",
      "Epoch [10/30], Step [2740/3535], Loss: 0.7342\n",
      "Epoch [10/30], Step [2760/3535], Loss: 0.4692\n",
      "Epoch [10/30], Step [2780/3535], Loss: 0.8627\n",
      "Epoch [10/30], Step [2800/3535], Loss: 0.5651\n",
      "Epoch [10/30], Step [2820/3535], Loss: 0.6565\n",
      "Epoch [10/30], Step [2840/3535], Loss: 0.8812\n",
      "Epoch [10/30], Step [2860/3535], Loss: 0.9235\n",
      "Epoch [10/30], Step [2880/3535], Loss: 0.5390\n",
      "Epoch [10/30], Step [2900/3535], Loss: 0.9630\n",
      "Epoch [10/30], Step [2920/3535], Loss: 1.0542\n",
      "Epoch [10/30], Step [2940/3535], Loss: 0.7218\n",
      "Epoch [10/30], Step [2960/3535], Loss: 0.4163\n",
      "Epoch [10/30], Step [2980/3535], Loss: 1.0489\n",
      "Epoch [10/30], Step [3000/3535], Loss: 1.6999\n",
      "Epoch [10/30], Step [3020/3535], Loss: 0.4000\n",
      "Epoch [10/30], Step [3040/3535], Loss: 0.7155\n",
      "Epoch [10/30], Step [3060/3535], Loss: 0.9398\n",
      "Epoch [10/30], Step [3080/3535], Loss: 0.6656\n",
      "Epoch [10/30], Step [3100/3535], Loss: 0.8434\n",
      "Epoch [10/30], Step [3120/3535], Loss: 0.8707\n",
      "Epoch [10/30], Step [3140/3535], Loss: 1.0701\n",
      "Epoch [10/30], Step [3160/3535], Loss: 0.9473\n",
      "Epoch [10/30], Step [3180/3535], Loss: 0.5699\n",
      "Epoch [10/30], Step [3200/3535], Loss: 0.9281\n",
      "Epoch [10/30], Step [3220/3535], Loss: 0.7018\n",
      "Epoch [10/30], Step [3240/3535], Loss: 1.0738\n",
      "Epoch [10/30], Step [3260/3535], Loss: 0.5832\n",
      "Epoch [10/30], Step [3280/3535], Loss: 1.0133\n",
      "Epoch [10/30], Step [3300/3535], Loss: 0.9519\n",
      "Epoch [10/30], Step [3320/3535], Loss: 0.4950\n",
      "Epoch [10/30], Step [3340/3535], Loss: 0.5772\n",
      "Epoch [10/30], Step [3360/3535], Loss: 1.3970\n",
      "Epoch [10/30], Step [3380/3535], Loss: 1.1848\n",
      "Epoch [10/30], Step [3400/3535], Loss: 1.1156\n",
      "Epoch [10/30], Step [3420/3535], Loss: 0.6861\n",
      "Epoch [10/30], Step [3440/3535], Loss: 0.8491\n",
      "Epoch [10/30], Step [3460/3535], Loss: 1.0347\n",
      "Epoch [10/30], Step [3480/3535], Loss: 1.1483\n",
      "Epoch [10/30], Step [3500/3535], Loss: 0.6510\n",
      "Epoch [10/30], Step [3520/3535], Loss: 1.4926\n",
      "\n",
      "train-loss: 1.2177, train-acc: 65.5466\n",
      "validation loss: 1.2070, validation acc: 59.6010\n",
      "\n",
      "Epoch 11\n",
      "\n",
      "Epoch [11/30], Step [0/3535], Loss: 1.1417\n",
      "Epoch [11/30], Step [20/3535], Loss: 0.5423\n",
      "Epoch [11/30], Step [40/3535], Loss: 0.8993\n",
      "Epoch [11/30], Step [60/3535], Loss: 0.6925\n",
      "Epoch [11/30], Step [80/3535], Loss: 0.6004\n",
      "Epoch [11/30], Step [100/3535], Loss: 0.8715\n",
      "Epoch [11/30], Step [120/3535], Loss: 0.7315\n",
      "Epoch [11/30], Step [140/3535], Loss: 0.7034\n",
      "Epoch [11/30], Step [160/3535], Loss: 1.0551\n",
      "Epoch [11/30], Step [180/3535], Loss: 0.8859\n",
      "Epoch [11/30], Step [200/3535], Loss: 0.7150\n",
      "Epoch [11/30], Step [220/3535], Loss: 0.3067\n",
      "Epoch [11/30], Step [240/3535], Loss: 0.8864\n",
      "Epoch [11/30], Step [260/3535], Loss: 0.9038\n",
      "Epoch [11/30], Step [280/3535], Loss: 1.2584\n",
      "Epoch [11/30], Step [300/3535], Loss: 0.4824\n",
      "Epoch [11/30], Step [320/3535], Loss: 0.6206\n",
      "Epoch [11/30], Step [340/3535], Loss: 1.0614\n",
      "Epoch [11/30], Step [360/3535], Loss: 1.3138\n",
      "Epoch [11/30], Step [380/3535], Loss: 0.5729\n",
      "Epoch [11/30], Step [400/3535], Loss: 1.2171\n",
      "Epoch [11/30], Step [420/3535], Loss: 0.2588\n",
      "Epoch [11/30], Step [440/3535], Loss: 1.4711\n",
      "Epoch [11/30], Step [460/3535], Loss: 0.6558\n",
      "Epoch [11/30], Step [480/3535], Loss: 0.8863\n",
      "Epoch [11/30], Step [500/3535], Loss: 1.1482\n",
      "Epoch [11/30], Step [520/3535], Loss: 1.0178\n",
      "Epoch [11/30], Step [540/3535], Loss: 0.5184\n",
      "Epoch [11/30], Step [560/3535], Loss: 1.4962\n",
      "Epoch [11/30], Step [580/3535], Loss: 0.4672\n",
      "Epoch [11/30], Step [600/3535], Loss: 1.4675\n",
      "Epoch [11/30], Step [620/3535], Loss: 1.4912\n",
      "Epoch [11/30], Step [640/3535], Loss: 0.8632\n",
      "Epoch [11/30], Step [660/3535], Loss: 0.7418\n",
      "Epoch [11/30], Step [680/3535], Loss: 1.1288\n",
      "Epoch [11/30], Step [700/3535], Loss: 1.6312\n",
      "Epoch [11/30], Step [720/3535], Loss: 0.5447\n",
      "Epoch [11/30], Step [740/3535], Loss: 0.5753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], Step [760/3535], Loss: 0.8074\n",
      "Epoch [11/30], Step [780/3535], Loss: 0.6904\n",
      "Epoch [11/30], Step [800/3535], Loss: 0.8113\n",
      "Epoch [11/30], Step [820/3535], Loss: 0.5505\n",
      "Epoch [11/30], Step [840/3535], Loss: 0.7446\n",
      "Epoch [11/30], Step [860/3535], Loss: 1.2208\n",
      "Epoch [11/30], Step [880/3535], Loss: 0.7898\n",
      "Epoch [11/30], Step [900/3535], Loss: 0.4879\n",
      "Epoch [11/30], Step [920/3535], Loss: 1.7714\n",
      "Epoch [11/30], Step [940/3535], Loss: 1.1960\n",
      "Epoch [11/30], Step [960/3535], Loss: 0.7757\n",
      "Epoch [11/30], Step [980/3535], Loss: 1.3332\n",
      "Epoch [11/30], Step [1000/3535], Loss: 0.5235\n",
      "Epoch [11/30], Step [1020/3535], Loss: 0.6155\n",
      "Epoch [11/30], Step [1040/3535], Loss: 0.5186\n",
      "Epoch [11/30], Step [1060/3535], Loss: 0.5932\n",
      "Epoch [11/30], Step [1080/3535], Loss: 0.8460\n",
      "Epoch [11/30], Step [1100/3535], Loss: 0.8711\n",
      "Epoch [11/30], Step [1120/3535], Loss: 1.0749\n",
      "Epoch [11/30], Step [1140/3535], Loss: 0.9728\n",
      "Epoch [11/30], Step [1160/3535], Loss: 0.5394\n",
      "Epoch [11/30], Step [1180/3535], Loss: 0.9698\n",
      "Epoch [11/30], Step [1200/3535], Loss: 0.4618\n",
      "Epoch [11/30], Step [1220/3535], Loss: 1.0369\n",
      "Epoch [11/30], Step [1240/3535], Loss: 0.6075\n",
      "Epoch [11/30], Step [1260/3535], Loss: 0.9892\n",
      "Epoch [11/30], Step [1280/3535], Loss: 1.1903\n",
      "Epoch [11/30], Step [1300/3535], Loss: 0.8863\n",
      "Epoch [11/30], Step [1320/3535], Loss: 0.8047\n",
      "Epoch [11/30], Step [1340/3535], Loss: 0.3626\n",
      "Epoch [11/30], Step [1360/3535], Loss: 0.8643\n",
      "Epoch [11/30], Step [1380/3535], Loss: 0.6819\n",
      "Epoch [11/30], Step [1400/3535], Loss: 0.8842\n",
      "Epoch [11/30], Step [1420/3535], Loss: 1.0207\n",
      "Epoch [11/30], Step [1440/3535], Loss: 1.1062\n",
      "Epoch [11/30], Step [1460/3535], Loss: 0.6236\n",
      "Epoch [11/30], Step [1480/3535], Loss: 1.4822\n",
      "Epoch [11/30], Step [1500/3535], Loss: 0.7524\n",
      "Epoch [11/30], Step [1520/3535], Loss: 1.0667\n",
      "Epoch [11/30], Step [1540/3535], Loss: 1.3094\n",
      "Epoch [11/30], Step [1560/3535], Loss: 1.2301\n",
      "Epoch [11/30], Step [1580/3535], Loss: 0.3191\n",
      "Epoch [11/30], Step [1600/3535], Loss: 1.1163\n",
      "Epoch [11/30], Step [1620/3535], Loss: 0.9200\n",
      "Epoch [11/30], Step [1640/3535], Loss: 0.2368\n",
      "Epoch [11/30], Step [1660/3535], Loss: 1.4646\n",
      "Epoch [11/30], Step [1680/3535], Loss: 0.4157\n",
      "Epoch [11/30], Step [1700/3535], Loss: 1.0198\n",
      "Epoch [11/30], Step [1720/3535], Loss: 0.7870\n",
      "Epoch [11/30], Step [1740/3535], Loss: 0.4044\n",
      "Epoch [11/30], Step [1760/3535], Loss: 0.4695\n",
      "Epoch [11/30], Step [1780/3535], Loss: 1.0986\n",
      "Epoch [11/30], Step [1800/3535], Loss: 0.5513\n",
      "Epoch [11/30], Step [1820/3535], Loss: 0.5226\n",
      "Epoch [11/30], Step [1840/3535], Loss: 1.1664\n",
      "Epoch [11/30], Step [1860/3535], Loss: 0.8244\n",
      "Epoch [11/30], Step [1880/3535], Loss: 0.9439\n",
      "Epoch [11/30], Step [1900/3535], Loss: 0.6400\n",
      "Epoch [11/30], Step [1920/3535], Loss: 1.0609\n",
      "Epoch [11/30], Step [1940/3535], Loss: 0.9255\n",
      "Epoch [11/30], Step [1960/3535], Loss: 1.0124\n",
      "Epoch [11/30], Step [1980/3535], Loss: 0.8962\n",
      "Epoch [11/30], Step [2000/3535], Loss: 1.1583\n",
      "Epoch [11/30], Step [2020/3535], Loss: 0.7720\n",
      "Epoch [11/30], Step [2040/3535], Loss: 1.2054\n",
      "Epoch [11/30], Step [2060/3535], Loss: 1.2219\n",
      "Epoch [11/30], Step [2080/3535], Loss: 0.5180\n",
      "Epoch [11/30], Step [2100/3535], Loss: 0.7324\n",
      "Epoch [11/30], Step [2120/3535], Loss: 0.7166\n",
      "Epoch [11/30], Step [2140/3535], Loss: 0.5058\n",
      "Epoch [11/30], Step [2160/3535], Loss: 0.6619\n",
      "Epoch [11/30], Step [2180/3535], Loss: 0.9167\n",
      "Epoch [11/30], Step [2200/3535], Loss: 1.8321\n",
      "Epoch [11/30], Step [2220/3535], Loss: 0.7794\n",
      "Epoch [11/30], Step [2240/3535], Loss: 1.3560\n",
      "Epoch [11/30], Step [2260/3535], Loss: 0.8211\n",
      "Epoch [11/30], Step [2280/3535], Loss: 0.3630\n",
      "Epoch [11/30], Step [2300/3535], Loss: 0.7693\n",
      "Epoch [11/30], Step [2320/3535], Loss: 0.7418\n",
      "Epoch [11/30], Step [2340/3535], Loss: 1.1459\n",
      "Epoch [11/30], Step [2360/3535], Loss: 0.1732\n",
      "Epoch [11/30], Step [2380/3535], Loss: 0.7644\n",
      "Epoch [11/30], Step [2400/3535], Loss: 1.1340\n",
      "Epoch [11/30], Step [2420/3535], Loss: 0.4030\n",
      "Epoch [11/30], Step [2440/3535], Loss: 1.5560\n",
      "Epoch [11/30], Step [2460/3535], Loss: 1.0371\n",
      "Epoch [11/30], Step [2480/3535], Loss: 1.1823\n",
      "Epoch [11/30], Step [2500/3535], Loss: 0.9383\n",
      "Epoch [11/30], Step [2520/3535], Loss: 0.7136\n",
      "Epoch [11/30], Step [2540/3535], Loss: 0.4541\n",
      "Epoch [11/30], Step [2560/3535], Loss: 0.6685\n",
      "Epoch [11/30], Step [2580/3535], Loss: 0.3376\n",
      "Epoch [11/30], Step [2600/3535], Loss: 0.6504\n",
      "Epoch [11/30], Step [2620/3535], Loss: 0.7152\n",
      "Epoch [11/30], Step [2640/3535], Loss: 1.1360\n",
      "Epoch [11/30], Step [2660/3535], Loss: 1.2170\n",
      "Epoch [11/30], Step [2680/3535], Loss: 0.7772\n",
      "Epoch [11/30], Step [2700/3535], Loss: 1.2695\n",
      "Epoch [11/30], Step [2720/3535], Loss: 0.5579\n",
      "Epoch [11/30], Step [2740/3535], Loss: 0.9674\n",
      "Epoch [11/30], Step [2760/3535], Loss: 0.9406\n",
      "Epoch [11/30], Step [2780/3535], Loss: 0.6936\n",
      "Epoch [11/30], Step [2800/3535], Loss: 1.0011\n",
      "Epoch [11/30], Step [2820/3535], Loss: 0.5944\n",
      "Epoch [11/30], Step [2840/3535], Loss: 0.9045\n",
      "Epoch [11/30], Step [2860/3535], Loss: 2.0783\n",
      "Epoch [11/30], Step [2880/3535], Loss: 2.0737\n",
      "Epoch [11/30], Step [2900/3535], Loss: 0.4601\n",
      "Epoch [11/30], Step [2920/3535], Loss: 0.4512\n",
      "Epoch [11/30], Step [2940/3535], Loss: 0.8790\n",
      "Epoch [11/30], Step [2960/3535], Loss: 1.0431\n",
      "Epoch [11/30], Step [2980/3535], Loss: 0.7062\n",
      "Epoch [11/30], Step [3000/3535], Loss: 0.4221\n",
      "Epoch [11/30], Step [3020/3535], Loss: 1.2564\n",
      "Epoch [11/30], Step [3040/3535], Loss: 1.2336\n",
      "Epoch [11/30], Step [3060/3535], Loss: 0.7679\n",
      "Epoch [11/30], Step [3080/3535], Loss: 0.3723\n",
      "Epoch [11/30], Step [3100/3535], Loss: 0.7557\n",
      "Epoch [11/30], Step [3120/3535], Loss: 0.7364\n",
      "Epoch [11/30], Step [3140/3535], Loss: 0.9136\n",
      "Epoch [11/30], Step [3160/3535], Loss: 0.9052\n",
      "Epoch [11/30], Step [3180/3535], Loss: 0.4379\n",
      "Epoch [11/30], Step [3200/3535], Loss: 0.4920\n",
      "Epoch [11/30], Step [3220/3535], Loss: 1.0339\n",
      "Epoch [11/30], Step [3240/3535], Loss: 0.9979\n",
      "Epoch [11/30], Step [3260/3535], Loss: 0.7171\n",
      "Epoch [11/30], Step [3280/3535], Loss: 0.5404\n",
      "Epoch [11/30], Step [3300/3535], Loss: 1.0304\n",
      "Epoch [11/30], Step [3320/3535], Loss: 0.9544\n",
      "Epoch [11/30], Step [3340/3535], Loss: 0.6107\n",
      "Epoch [11/30], Step [3360/3535], Loss: 1.3000\n",
      "Epoch [11/30], Step [3380/3535], Loss: 0.9972\n",
      "Epoch [11/30], Step [3400/3535], Loss: 0.6150\n",
      "Epoch [11/30], Step [3420/3535], Loss: 0.6221\n",
      "Epoch [11/30], Step [3440/3535], Loss: 1.0758\n",
      "Epoch [11/30], Step [3460/3535], Loss: 0.8853\n",
      "Epoch [11/30], Step [3480/3535], Loss: 0.8740\n",
      "Epoch [11/30], Step [3500/3535], Loss: 1.1913\n",
      "Epoch [11/30], Step [3520/3535], Loss: 0.6272\n",
      "\n",
      "train-loss: 1.1849, train-acc: 67.7572\n",
      "validation loss: 1.1945, validation acc: 59.4878\n",
      "\n",
      "Epoch 12\n",
      "\n",
      "Epoch [12/30], Step [0/3535], Loss: 0.9355\n",
      "Epoch [12/30], Step [20/3535], Loss: 0.5618\n",
      "Epoch [12/30], Step [40/3535], Loss: 0.7455\n",
      "Epoch [12/30], Step [60/3535], Loss: 0.5355\n",
      "Epoch [12/30], Step [80/3535], Loss: 0.6888\n",
      "Epoch [12/30], Step [100/3535], Loss: 0.5559\n",
      "Epoch [12/30], Step [120/3535], Loss: 0.6961\n",
      "Epoch [12/30], Step [140/3535], Loss: 0.5212\n",
      "Epoch [12/30], Step [160/3535], Loss: 0.7657\n",
      "Epoch [12/30], Step [180/3535], Loss: 0.4634\n",
      "Epoch [12/30], Step [200/3535], Loss: 0.5982\n",
      "Epoch [12/30], Step [220/3535], Loss: 0.8803\n",
      "Epoch [12/30], Step [240/3535], Loss: 1.5601\n",
      "Epoch [12/30], Step [260/3535], Loss: 1.2020\n",
      "Epoch [12/30], Step [280/3535], Loss: 0.3772\n",
      "Epoch [12/30], Step [300/3535], Loss: 1.2933\n",
      "Epoch [12/30], Step [320/3535], Loss: 0.2848\n",
      "Epoch [12/30], Step [340/3535], Loss: 0.5960\n",
      "Epoch [12/30], Step [360/3535], Loss: 1.3041\n",
      "Epoch [12/30], Step [380/3535], Loss: 0.6646\n",
      "Epoch [12/30], Step [400/3535], Loss: 0.7198\n",
      "Epoch [12/30], Step [420/3535], Loss: 1.1494\n",
      "Epoch [12/30], Step [440/3535], Loss: 0.8024\n",
      "Epoch [12/30], Step [460/3535], Loss: 0.4736\n",
      "Epoch [12/30], Step [480/3535], Loss: 1.5273\n",
      "Epoch [12/30], Step [500/3535], Loss: 0.6235\n",
      "Epoch [12/30], Step [520/3535], Loss: 0.6346\n",
      "Epoch [12/30], Step [540/3535], Loss: 0.6904\n",
      "Epoch [12/30], Step [560/3535], Loss: 0.9547\n",
      "Epoch [12/30], Step [580/3535], Loss: 0.6075\n",
      "Epoch [12/30], Step [600/3535], Loss: 0.4200\n",
      "Epoch [12/30], Step [620/3535], Loss: 0.7090\n",
      "Epoch [12/30], Step [640/3535], Loss: 0.9901\n",
      "Epoch [12/30], Step [660/3535], Loss: 1.3479\n",
      "Epoch [12/30], Step [680/3535], Loss: 0.6687\n",
      "Epoch [12/30], Step [700/3535], Loss: 0.4104\n",
      "Epoch [12/30], Step [720/3535], Loss: 0.7846\n",
      "Epoch [12/30], Step [740/3535], Loss: 0.5328\n",
      "Epoch [12/30], Step [760/3535], Loss: 0.6287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30], Step [780/3535], Loss: 0.2467\n",
      "Epoch [12/30], Step [800/3535], Loss: 0.8676\n",
      "Epoch [12/30], Step [820/3535], Loss: 0.8305\n",
      "Epoch [12/30], Step [840/3535], Loss: 0.8678\n",
      "Epoch [12/30], Step [860/3535], Loss: 1.7282\n",
      "Epoch [12/30], Step [880/3535], Loss: 0.3275\n",
      "Epoch [12/30], Step [900/3535], Loss: 0.2777\n",
      "Epoch [12/30], Step [920/3535], Loss: 1.0398\n",
      "Epoch [12/30], Step [940/3535], Loss: 0.5001\n",
      "Epoch [12/30], Step [960/3535], Loss: 1.2180\n",
      "Epoch [12/30], Step [980/3535], Loss: 0.1872\n",
      "Epoch [12/30], Step [1000/3535], Loss: 1.1211\n",
      "Epoch [12/30], Step [1020/3535], Loss: 1.0346\n",
      "Epoch [12/30], Step [1040/3535], Loss: 1.2499\n",
      "Epoch [12/30], Step [1060/3535], Loss: 0.7726\n",
      "Epoch [12/30], Step [1080/3535], Loss: 0.7347\n",
      "Epoch [12/30], Step [1100/3535], Loss: 0.5349\n",
      "Epoch [12/30], Step [1120/3535], Loss: 0.8655\n",
      "Epoch [12/30], Step [1140/3535], Loss: 0.9849\n",
      "Epoch [12/30], Step [1160/3535], Loss: 0.8650\n",
      "Epoch [12/30], Step [1180/3535], Loss: 1.0429\n",
      "Epoch [12/30], Step [1200/3535], Loss: 0.6126\n",
      "Epoch [12/30], Step [1220/3535], Loss: 0.7296\n",
      "Epoch [12/30], Step [1240/3535], Loss: 0.4732\n",
      "Epoch [12/30], Step [1260/3535], Loss: 1.5885\n",
      "Epoch [12/30], Step [1280/3535], Loss: 1.5280\n",
      "Epoch [12/30], Step [1300/3535], Loss: 0.5519\n",
      "Epoch [12/30], Step [1320/3535], Loss: 1.0128\n",
      "Epoch [12/30], Step [1340/3535], Loss: 0.7943\n",
      "Epoch [12/30], Step [1360/3535], Loss: 0.8111\n",
      "Epoch [12/30], Step [1380/3535], Loss: 0.6413\n",
      "Epoch [12/30], Step [1400/3535], Loss: 0.8461\n",
      "Epoch [12/30], Step [1420/3535], Loss: 0.9078\n",
      "Epoch [12/30], Step [1440/3535], Loss: 0.5537\n",
      "Epoch [12/30], Step [1460/3535], Loss: 0.9389\n",
      "Epoch [12/30], Step [1480/3535], Loss: 0.9417\n",
      "Epoch [12/30], Step [1500/3535], Loss: 0.4845\n",
      "Epoch [12/30], Step [1520/3535], Loss: 0.5435\n",
      "Epoch [12/30], Step [1540/3535], Loss: 1.1916\n",
      "Epoch [12/30], Step [1560/3535], Loss: 0.8958\n",
      "Epoch [12/30], Step [1580/3535], Loss: 0.4549\n",
      "Epoch [12/30], Step [1600/3535], Loss: 0.6838\n",
      "Epoch [12/30], Step [1620/3535], Loss: 0.8829\n",
      "Epoch [12/30], Step [1640/3535], Loss: 0.9469\n",
      "Epoch [12/30], Step [1660/3535], Loss: 0.8445\n",
      "Epoch [12/30], Step [1680/3535], Loss: 0.6190\n",
      "Epoch [12/30], Step [1700/3535], Loss: 0.9842\n",
      "Epoch [12/30], Step [1720/3535], Loss: 1.0683\n",
      "Epoch [12/30], Step [1740/3535], Loss: 1.3329\n",
      "Epoch [12/30], Step [1760/3535], Loss: 0.5845\n",
      "Epoch [12/30], Step [1780/3535], Loss: 0.5547\n",
      "Epoch [12/30], Step [1800/3535], Loss: 0.7355\n",
      "Epoch [12/30], Step [1820/3535], Loss: 0.3587\n",
      "Epoch [12/30], Step [1840/3535], Loss: 1.0175\n",
      "Epoch [12/30], Step [1860/3535], Loss: 0.4951\n",
      "Epoch [12/30], Step [1880/3535], Loss: 1.2485\n",
      "Epoch [12/30], Step [1900/3535], Loss: 1.0805\n",
      "Epoch [12/30], Step [1920/3535], Loss: 0.7545\n",
      "Epoch [12/30], Step [1940/3535], Loss: 0.6088\n",
      "Epoch [12/30], Step [1960/3535], Loss: 0.9917\n",
      "Epoch [12/30], Step [1980/3535], Loss: 0.7633\n",
      "Epoch [12/30], Step [2000/3535], Loss: 1.3541\n",
      "Epoch [12/30], Step [2020/3535], Loss: 0.7735\n",
      "Epoch [12/30], Step [2040/3535], Loss: 0.5398\n",
      "Epoch [12/30], Step [2060/3535], Loss: 0.6697\n",
      "Epoch [12/30], Step [2080/3535], Loss: 0.5477\n",
      "Epoch [12/30], Step [2100/3535], Loss: 1.1162\n",
      "Epoch [12/30], Step [2120/3535], Loss: 0.9748\n",
      "Epoch [12/30], Step [2140/3535], Loss: 0.6495\n",
      "Epoch [12/30], Step [2160/3535], Loss: 0.9379\n",
      "Epoch [12/30], Step [2180/3535], Loss: 0.9838\n",
      "Epoch [12/30], Step [2200/3535], Loss: 0.2183\n",
      "Epoch [12/30], Step [2220/3535], Loss: 1.1971\n",
      "Epoch [12/30], Step [2240/3535], Loss: 1.1027\n",
      "Epoch [12/30], Step [2260/3535], Loss: 0.4782\n",
      "Epoch [12/30], Step [2280/3535], Loss: 0.4202\n",
      "Epoch [12/30], Step [2300/3535], Loss: 0.7171\n",
      "Epoch [12/30], Step [2320/3535], Loss: 0.7190\n",
      "Epoch [12/30], Step [2340/3535], Loss: 0.5706\n",
      "Epoch [12/30], Step [2360/3535], Loss: 0.3051\n",
      "Epoch [12/30], Step [2380/3535], Loss: 0.2797\n",
      "Epoch [12/30], Step [2400/3535], Loss: 1.1976\n",
      "Epoch [12/30], Step [2420/3535], Loss: 0.5518\n",
      "Epoch [12/30], Step [2440/3535], Loss: 0.8201\n",
      "Epoch [12/30], Step [2460/3535], Loss: 0.3019\n",
      "Epoch [12/30], Step [2480/3535], Loss: 1.7241\n",
      "Epoch [12/30], Step [2500/3535], Loss: 0.6838\n",
      "Epoch [12/30], Step [2520/3535], Loss: 0.8070\n",
      "Epoch [12/30], Step [2540/3535], Loss: 1.4538\n",
      "Epoch [12/30], Step [2560/3535], Loss: 0.8660\n",
      "Epoch [12/30], Step [2580/3535], Loss: 0.7538\n",
      "Epoch [12/30], Step [2600/3535], Loss: 0.6971\n",
      "Epoch [12/30], Step [2620/3535], Loss: 0.8394\n",
      "Epoch [12/30], Step [2640/3535], Loss: 0.7501\n",
      "Epoch [12/30], Step [2660/3535], Loss: 0.4520\n",
      "Epoch [12/30], Step [2680/3535], Loss: 0.8401\n",
      "Epoch [12/30], Step [2700/3535], Loss: 0.5454\n",
      "Epoch [12/30], Step [2720/3535], Loss: 0.8810\n",
      "Epoch [12/30], Step [2740/3535], Loss: 0.6389\n",
      "Epoch [12/30], Step [2760/3535], Loss: 1.3450\n",
      "Epoch [12/30], Step [2780/3535], Loss: 0.7116\n",
      "Epoch [12/30], Step [2800/3535], Loss: 0.4107\n",
      "Epoch [12/30], Step [2820/3535], Loss: 0.5717\n",
      "Epoch [12/30], Step [2840/3535], Loss: 1.1819\n",
      "Epoch [12/30], Step [2860/3535], Loss: 0.7431\n",
      "Epoch [12/30], Step [2880/3535], Loss: 1.0867\n",
      "Epoch [12/30], Step [2900/3535], Loss: 0.8650\n",
      "Epoch [12/30], Step [2920/3535], Loss: 1.1377\n",
      "Epoch [12/30], Step [2940/3535], Loss: 0.9914\n",
      "Epoch [12/30], Step [2960/3535], Loss: 1.0266\n",
      "Epoch [12/30], Step [2980/3535], Loss: 1.1389\n",
      "Epoch [12/30], Step [3000/3535], Loss: 1.1816\n",
      "Epoch [12/30], Step [3020/3535], Loss: 0.7380\n",
      "Epoch [12/30], Step [3040/3535], Loss: 0.7223\n",
      "Epoch [12/30], Step [3060/3535], Loss: 0.6045\n",
      "Epoch [12/30], Step [3080/3535], Loss: 0.7506\n",
      "Epoch [12/30], Step [3100/3535], Loss: 0.6364\n",
      "Epoch [12/30], Step [3120/3535], Loss: 0.8528\n",
      "Epoch [12/30], Step [3140/3535], Loss: 0.5518\n",
      "Epoch [12/30], Step [3160/3535], Loss: 0.6381\n",
      "Epoch [12/30], Step [3180/3535], Loss: 0.6640\n",
      "Epoch [12/30], Step [3200/3535], Loss: 0.3672\n",
      "Epoch [12/30], Step [3220/3535], Loss: 0.6685\n",
      "Epoch [12/30], Step [3240/3535], Loss: 0.9057\n",
      "Epoch [12/30], Step [3260/3535], Loss: 0.6769\n",
      "Epoch [12/30], Step [3280/3535], Loss: 0.9533\n",
      "Epoch [12/30], Step [3300/3535], Loss: 1.0656\n",
      "Epoch [12/30], Step [3320/3535], Loss: 0.5881\n",
      "Epoch [12/30], Step [3340/3535], Loss: 0.7115\n",
      "Epoch [12/30], Step [3360/3535], Loss: 0.7621\n",
      "Epoch [12/30], Step [3380/3535], Loss: 0.5735\n",
      "Epoch [12/30], Step [3400/3535], Loss: 0.9079\n",
      "Epoch [12/30], Step [3420/3535], Loss: 0.8848\n",
      "Epoch [12/30], Step [3440/3535], Loss: 1.1789\n",
      "Epoch [12/30], Step [3460/3535], Loss: 1.1554\n",
      "Epoch [12/30], Step [3480/3535], Loss: 0.8363\n",
      "Epoch [12/30], Step [3500/3535], Loss: 0.6947\n",
      "Epoch [12/30], Step [3520/3535], Loss: 0.9527\n",
      "\n",
      "train-loss: 1.1533, train-acc: 70.2119\n",
      "validation loss: 1.1809, validation acc: 61.9075\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 13\n",
      "\n",
      "Epoch [13/30], Step [0/3535], Loss: 0.4101\n",
      "Epoch [13/30], Step [20/3535], Loss: 0.8482\n",
      "Epoch [13/30], Step [40/3535], Loss: 0.6407\n",
      "Epoch [13/30], Step [60/3535], Loss: 0.5202\n",
      "Epoch [13/30], Step [80/3535], Loss: 0.8800\n",
      "Epoch [13/30], Step [100/3535], Loss: 0.3631\n",
      "Epoch [13/30], Step [120/3535], Loss: 0.9528\n",
      "Epoch [13/30], Step [140/3535], Loss: 0.9191\n",
      "Epoch [13/30], Step [160/3535], Loss: 0.3888\n",
      "Epoch [13/30], Step [180/3535], Loss: 0.3829\n",
      "Epoch [13/30], Step [200/3535], Loss: 0.9998\n",
      "Epoch [13/30], Step [220/3535], Loss: 0.1519\n",
      "Epoch [13/30], Step [240/3535], Loss: 0.6449\n",
      "Epoch [13/30], Step [260/3535], Loss: 1.1246\n",
      "Epoch [13/30], Step [280/3535], Loss: 0.1817\n",
      "Epoch [13/30], Step [300/3535], Loss: 1.1156\n",
      "Epoch [13/30], Step [320/3535], Loss: 1.2677\n",
      "Epoch [13/30], Step [340/3535], Loss: 0.9438\n",
      "Epoch [13/30], Step [360/3535], Loss: 0.6954\n",
      "Epoch [13/30], Step [380/3535], Loss: 0.5363\n",
      "Epoch [13/30], Step [400/3535], Loss: 0.6533\n",
      "Epoch [13/30], Step [420/3535], Loss: 0.5409\n",
      "Epoch [13/30], Step [440/3535], Loss: 1.0825\n",
      "Epoch [13/30], Step [460/3535], Loss: 0.5498\n",
      "Epoch [13/30], Step [480/3535], Loss: 1.0716\n",
      "Epoch [13/30], Step [500/3535], Loss: 1.0543\n",
      "Epoch [13/30], Step [520/3535], Loss: 0.4929\n",
      "Epoch [13/30], Step [540/3535], Loss: 0.5284\n",
      "Epoch [13/30], Step [560/3535], Loss: 0.3107\n",
      "Epoch [13/30], Step [580/3535], Loss: 0.4847\n",
      "Epoch [13/30], Step [600/3535], Loss: 1.0054\n",
      "Epoch [13/30], Step [620/3535], Loss: 1.1118\n",
      "Epoch [13/30], Step [640/3535], Loss: 0.8370\n",
      "Epoch [13/30], Step [660/3535], Loss: 0.9240\n",
      "Epoch [13/30], Step [680/3535], Loss: 1.2360\n",
      "Epoch [13/30], Step [700/3535], Loss: 0.8791\n",
      "Epoch [13/30], Step [720/3535], Loss: 0.4927\n",
      "Epoch [13/30], Step [740/3535], Loss: 0.6181\n",
      "Epoch [13/30], Step [760/3535], Loss: 0.2361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30], Step [780/3535], Loss: 0.3663\n",
      "Epoch [13/30], Step [800/3535], Loss: 0.6017\n",
      "Epoch [13/30], Step [820/3535], Loss: 0.8359\n",
      "Epoch [13/30], Step [840/3535], Loss: 0.7616\n",
      "Epoch [13/30], Step [860/3535], Loss: 0.7406\n",
      "Epoch [13/30], Step [880/3535], Loss: 0.2761\n",
      "Epoch [13/30], Step [900/3535], Loss: 0.4293\n",
      "Epoch [13/30], Step [920/3535], Loss: 0.3970\n",
      "Epoch [13/30], Step [940/3535], Loss: 0.3849\n",
      "Epoch [13/30], Step [960/3535], Loss: 0.5242\n",
      "Epoch [13/30], Step [980/3535], Loss: 0.6569\n",
      "Epoch [13/30], Step [1000/3535], Loss: 0.4208\n",
      "Epoch [13/30], Step [1020/3535], Loss: 0.9562\n",
      "Epoch [13/30], Step [1040/3535], Loss: 0.7393\n",
      "Epoch [13/30], Step [1060/3535], Loss: 0.3826\n",
      "Epoch [13/30], Step [1080/3535], Loss: 1.0933\n",
      "Epoch [13/30], Step [1100/3535], Loss: 0.5004\n",
      "Epoch [13/30], Step [1120/3535], Loss: 0.6064\n",
      "Epoch [13/30], Step [1140/3535], Loss: 0.9181\n",
      "Epoch [13/30], Step [1160/3535], Loss: 0.4659\n",
      "Epoch [13/30], Step [1180/3535], Loss: 0.6559\n",
      "Epoch [13/30], Step [1200/3535], Loss: 0.9381\n",
      "Epoch [13/30], Step [1220/3535], Loss: 0.4460\n",
      "Epoch [13/30], Step [1240/3535], Loss: 0.5664\n",
      "Epoch [13/30], Step [1260/3535], Loss: 1.3412\n",
      "Epoch [13/30], Step [1280/3535], Loss: 1.0978\n",
      "Epoch [13/30], Step [1300/3535], Loss: 0.9929\n",
      "Epoch [13/30], Step [1320/3535], Loss: 0.6139\n",
      "Epoch [13/30], Step [1340/3535], Loss: 0.5774\n",
      "Epoch [13/30], Step [1360/3535], Loss: 0.8973\n",
      "Epoch [13/30], Step [1380/3535], Loss: 0.8200\n",
      "Epoch [13/30], Step [1400/3535], Loss: 0.4864\n",
      "Epoch [13/30], Step [1420/3535], Loss: 0.8438\n",
      "Epoch [13/30], Step [1440/3535], Loss: 1.9635\n",
      "Epoch [13/30], Step [1460/3535], Loss: 0.6555\n",
      "Epoch [13/30], Step [1480/3535], Loss: 0.4918\n",
      "Epoch [13/30], Step [1500/3535], Loss: 0.6203\n",
      "Epoch [13/30], Step [1520/3535], Loss: 0.5166\n",
      "Epoch [13/30], Step [1540/3535], Loss: 1.4080\n",
      "Epoch [13/30], Step [1560/3535], Loss: 0.7790\n",
      "Epoch [13/30], Step [1580/3535], Loss: 0.1789\n",
      "Epoch [13/30], Step [1600/3535], Loss: 1.2744\n",
      "Epoch [13/30], Step [1620/3535], Loss: 1.0927\n",
      "Epoch [13/30], Step [1640/3535], Loss: 0.7883\n",
      "Epoch [13/30], Step [1660/3535], Loss: 0.4573\n",
      "Epoch [13/30], Step [1680/3535], Loss: 0.3896\n",
      "Epoch [13/30], Step [1700/3535], Loss: 0.3313\n",
      "Epoch [13/30], Step [1720/3535], Loss: 0.5571\n",
      "Epoch [13/30], Step [1740/3535], Loss: 1.2641\n",
      "Epoch [13/30], Step [1760/3535], Loss: 0.7205\n",
      "Epoch [13/30], Step [1780/3535], Loss: 0.8042\n",
      "Epoch [13/30], Step [1800/3535], Loss: 0.4431\n",
      "Epoch [13/30], Step [1820/3535], Loss: 0.7495\n",
      "Epoch [13/30], Step [1840/3535], Loss: 0.5021\n",
      "Epoch [13/30], Step [1860/3535], Loss: 0.5813\n",
      "Epoch [13/30], Step [1880/3535], Loss: 0.7175\n",
      "Epoch [13/30], Step [1900/3535], Loss: 1.2644\n",
      "Epoch [13/30], Step [1920/3535], Loss: 0.5848\n",
      "Epoch [13/30], Step [1940/3535], Loss: 0.5339\n",
      "Epoch [13/30], Step [1960/3535], Loss: 0.7816\n",
      "Epoch [13/30], Step [1980/3535], Loss: 0.6965\n",
      "Epoch [13/30], Step [2000/3535], Loss: 0.7456\n",
      "Epoch [13/30], Step [2020/3535], Loss: 0.9765\n",
      "Epoch [13/30], Step [2040/3535], Loss: 0.5690\n",
      "Epoch [13/30], Step [2060/3535], Loss: 1.3122\n",
      "Epoch [13/30], Step [2080/3535], Loss: 0.5176\n",
      "Epoch [13/30], Step [2100/3535], Loss: 0.9970\n",
      "Epoch [13/30], Step [2120/3535], Loss: 0.8559\n",
      "Epoch [13/30], Step [2140/3535], Loss: 0.9852\n",
      "Epoch [13/30], Step [2160/3535], Loss: 1.3632\n",
      "Epoch [13/30], Step [2180/3535], Loss: 0.7014\n",
      "Epoch [13/30], Step [2200/3535], Loss: 0.3166\n",
      "Epoch [13/30], Step [2220/3535], Loss: 0.6495\n",
      "Epoch [13/30], Step [2240/3535], Loss: 0.2874\n",
      "Epoch [13/30], Step [2260/3535], Loss: 0.7675\n",
      "Epoch [13/30], Step [2280/3535], Loss: 0.5000\n",
      "Epoch [13/30], Step [2300/3535], Loss: 1.0539\n",
      "Epoch [13/30], Step [2320/3535], Loss: 1.0964\n",
      "Epoch [13/30], Step [2340/3535], Loss: 0.5022\n",
      "Epoch [13/30], Step [2360/3535], Loss: 0.4905\n",
      "Epoch [13/30], Step [2380/3535], Loss: 1.3574\n",
      "Epoch [13/30], Step [2400/3535], Loss: 1.0221\n",
      "Epoch [13/30], Step [2420/3535], Loss: 1.4344\n",
      "Epoch [13/30], Step [2440/3535], Loss: 0.6353\n",
      "Epoch [13/30], Step [2460/3535], Loss: 0.6197\n",
      "Epoch [13/30], Step [2480/3535], Loss: 0.6829\n",
      "Epoch [13/30], Step [2500/3535], Loss: 0.7301\n",
      "Epoch [13/30], Step [2520/3535], Loss: 1.3157\n",
      "Epoch [13/30], Step [2540/3535], Loss: 0.5345\n",
      "Epoch [13/30], Step [2560/3535], Loss: 0.9662\n",
      "Epoch [13/30], Step [2580/3535], Loss: 0.3703\n",
      "Epoch [13/30], Step [2600/3535], Loss: 0.8648\n",
      "Epoch [13/30], Step [2620/3535], Loss: 0.5583\n",
      "Epoch [13/30], Step [2640/3535], Loss: 0.4143\n",
      "Epoch [13/30], Step [2660/3535], Loss: 0.2230\n",
      "Epoch [13/30], Step [2680/3535], Loss: 0.9181\n",
      "Epoch [13/30], Step [2700/3535], Loss: 1.0802\n",
      "Epoch [13/30], Step [2720/3535], Loss: 1.0691\n",
      "Epoch [13/30], Step [2740/3535], Loss: 0.9957\n",
      "Epoch [13/30], Step [2760/3535], Loss: 0.5011\n",
      "Epoch [13/30], Step [2780/3535], Loss: 0.6285\n",
      "Epoch [13/30], Step [2800/3535], Loss: 0.4535\n",
      "Epoch [13/30], Step [2820/3535], Loss: 0.8737\n",
      "Epoch [13/30], Step [2840/3535], Loss: 1.1197\n",
      "Epoch [13/30], Step [2860/3535], Loss: 0.6688\n",
      "Epoch [13/30], Step [2880/3535], Loss: 1.4559\n",
      "Epoch [13/30], Step [2900/3535], Loss: 0.7153\n",
      "Epoch [13/30], Step [2920/3535], Loss: 1.0971\n",
      "Epoch [13/30], Step [2940/3535], Loss: 0.3577\n",
      "Epoch [13/30], Step [2960/3535], Loss: 0.4644\n",
      "Epoch [13/30], Step [2980/3535], Loss: 0.2963\n",
      "Epoch [13/30], Step [3000/3535], Loss: 0.5845\n",
      "Epoch [13/30], Step [3020/3535], Loss: 0.7202\n",
      "Epoch [13/30], Step [3040/3535], Loss: 0.3361\n",
      "Epoch [13/30], Step [3060/3535], Loss: 1.1501\n",
      "Epoch [13/30], Step [3080/3535], Loss: 0.6141\n",
      "Epoch [13/30], Step [3100/3535], Loss: 0.9667\n",
      "Epoch [13/30], Step [3120/3535], Loss: 0.7801\n",
      "Epoch [13/30], Step [3140/3535], Loss: 0.5857\n",
      "Epoch [13/30], Step [3160/3535], Loss: 1.0134\n",
      "Epoch [13/30], Step [3180/3535], Loss: 1.5853\n",
      "Epoch [13/30], Step [3200/3535], Loss: 0.4788\n",
      "Epoch [13/30], Step [3220/3535], Loss: 0.7325\n",
      "Epoch [13/30], Step [3240/3535], Loss: 0.3313\n",
      "Epoch [13/30], Step [3260/3535], Loss: 0.8644\n",
      "Epoch [13/30], Step [3280/3535], Loss: 0.6959\n",
      "Epoch [13/30], Step [3300/3535], Loss: 0.4214\n",
      "Epoch [13/30], Step [3320/3535], Loss: 0.8842\n",
      "Epoch [13/30], Step [3340/3535], Loss: 0.4135\n",
      "Epoch [13/30], Step [3360/3535], Loss: 0.6728\n",
      "Epoch [13/30], Step [3380/3535], Loss: 1.6736\n",
      "Epoch [13/30], Step [3400/3535], Loss: 1.4740\n",
      "Epoch [13/30], Step [3420/3535], Loss: 0.6232\n",
      "Epoch [13/30], Step [3440/3535], Loss: 0.7404\n",
      "Epoch [13/30], Step [3460/3535], Loss: 0.5754\n",
      "Epoch [13/30], Step [3480/3535], Loss: 0.3972\n",
      "Epoch [13/30], Step [3500/3535], Loss: 0.6083\n",
      "Epoch [13/30], Step [3520/3535], Loss: 0.6884\n",
      "\n",
      "train-loss: 1.1222, train-acc: 72.0935\n",
      "validation loss: 1.1709, validation acc: 62.1197\n",
      "\n",
      "Epoch 14\n",
      "\n",
      "Epoch [14/30], Step [0/3535], Loss: 0.3785\n",
      "Epoch [14/30], Step [20/3535], Loss: 0.4915\n",
      "Epoch [14/30], Step [40/3535], Loss: 0.4515\n",
      "Epoch [14/30], Step [60/3535], Loss: 1.1490\n",
      "Epoch [14/30], Step [80/3535], Loss: 0.3896\n",
      "Epoch [14/30], Step [100/3535], Loss: 2.2810\n",
      "Epoch [14/30], Step [120/3535], Loss: 0.4287\n",
      "Epoch [14/30], Step [140/3535], Loss: 1.4050\n",
      "Epoch [14/30], Step [160/3535], Loss: 0.4386\n",
      "Epoch [14/30], Step [180/3535], Loss: 0.7122\n",
      "Epoch [14/30], Step [200/3535], Loss: 1.1424\n",
      "Epoch [14/30], Step [220/3535], Loss: 0.9208\n",
      "Epoch [14/30], Step [240/3535], Loss: 0.3856\n",
      "Epoch [14/30], Step [260/3535], Loss: 0.7860\n",
      "Epoch [14/30], Step [280/3535], Loss: 0.1993\n",
      "Epoch [14/30], Step [300/3535], Loss: 1.0288\n",
      "Epoch [14/30], Step [320/3535], Loss: 0.3871\n",
      "Epoch [14/30], Step [340/3535], Loss: 0.3653\n",
      "Epoch [14/30], Step [360/3535], Loss: 0.2276\n",
      "Epoch [14/30], Step [380/3535], Loss: 0.9335\n",
      "Epoch [14/30], Step [400/3535], Loss: 1.1203\n",
      "Epoch [14/30], Step [420/3535], Loss: 0.9442\n",
      "Epoch [14/30], Step [440/3535], Loss: 0.5495\n",
      "Epoch [14/30], Step [460/3535], Loss: 0.4596\n",
      "Epoch [14/30], Step [480/3535], Loss: 0.5977\n",
      "Epoch [14/30], Step [500/3535], Loss: 0.7408\n",
      "Epoch [14/30], Step [520/3535], Loss: 0.4148\n",
      "Epoch [14/30], Step [540/3535], Loss: 0.7886\n",
      "Epoch [14/30], Step [560/3535], Loss: 0.8642\n",
      "Epoch [14/30], Step [580/3535], Loss: 0.8095\n",
      "Epoch [14/30], Step [600/3535], Loss: 0.5073\n",
      "Epoch [14/30], Step [620/3535], Loss: 1.0963\n",
      "Epoch [14/30], Step [640/3535], Loss: 0.8756\n",
      "Epoch [14/30], Step [660/3535], Loss: 0.5277\n",
      "Epoch [14/30], Step [680/3535], Loss: 1.1911\n",
      "Epoch [14/30], Step [700/3535], Loss: 0.6791\n",
      "Epoch [14/30], Step [720/3535], Loss: 0.7795\n",
      "Epoch [14/30], Step [740/3535], Loss: 1.0754\n",
      "Epoch [14/30], Step [760/3535], Loss: 0.4899\n",
      "Epoch [14/30], Step [780/3535], Loss: 0.7762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30], Step [800/3535], Loss: 0.7745\n",
      "Epoch [14/30], Step [820/3535], Loss: 1.0461\n",
      "Epoch [14/30], Step [840/3535], Loss: 0.5645\n",
      "Epoch [14/30], Step [860/3535], Loss: 0.2710\n",
      "Epoch [14/30], Step [880/3535], Loss: 1.3190\n",
      "Epoch [14/30], Step [900/3535], Loss: 0.6112\n",
      "Epoch [14/30], Step [920/3535], Loss: 0.4479\n",
      "Epoch [14/30], Step [940/3535], Loss: 0.3580\n",
      "Epoch [14/30], Step [960/3535], Loss: 0.7266\n",
      "Epoch [14/30], Step [980/3535], Loss: 1.0051\n",
      "Epoch [14/30], Step [1000/3535], Loss: 0.2439\n",
      "Epoch [14/30], Step [1020/3535], Loss: 0.4247\n",
      "Epoch [14/30], Step [1040/3535], Loss: 0.3901\n",
      "Epoch [14/30], Step [1060/3535], Loss: 0.5452\n",
      "Epoch [14/30], Step [1080/3535], Loss: 0.5853\n",
      "Epoch [14/30], Step [1100/3535], Loss: 0.6542\n",
      "Epoch [14/30], Step [1120/3535], Loss: 0.8154\n",
      "Epoch [14/30], Step [1140/3535], Loss: 0.1783\n",
      "Epoch [14/30], Step [1160/3535], Loss: 0.8389\n",
      "Epoch [14/30], Step [1180/3535], Loss: 0.5198\n",
      "Epoch [14/30], Step [1200/3535], Loss: 1.0177\n",
      "Epoch [14/30], Step [1220/3535], Loss: 0.7527\n",
      "Epoch [14/30], Step [1240/3535], Loss: 0.7767\n",
      "Epoch [14/30], Step [1260/3535], Loss: 0.4265\n",
      "Epoch [14/30], Step [1280/3535], Loss: 0.8129\n",
      "Epoch [14/30], Step [1300/3535], Loss: 1.0051\n",
      "Epoch [14/30], Step [1320/3535], Loss: 0.6539\n",
      "Epoch [14/30], Step [1340/3535], Loss: 0.7756\n",
      "Epoch [14/30], Step [1360/3535], Loss: 0.9035\n",
      "Epoch [14/30], Step [1380/3535], Loss: 0.6372\n",
      "Epoch [14/30], Step [1400/3535], Loss: 0.8704\n",
      "Epoch [14/30], Step [1420/3535], Loss: 0.3875\n",
      "Epoch [14/30], Step [1440/3535], Loss: 0.6971\n",
      "Epoch [14/30], Step [1460/3535], Loss: 0.1776\n",
      "Epoch [14/30], Step [1480/3535], Loss: 0.3138\n",
      "Epoch [14/30], Step [1500/3535], Loss: 0.4227\n",
      "Epoch [14/30], Step [1520/3535], Loss: 0.5910\n",
      "Epoch [14/30], Step [1540/3535], Loss: 0.4383\n",
      "Epoch [14/30], Step [1560/3535], Loss: 1.1317\n",
      "Epoch [14/30], Step [1580/3535], Loss: 0.5480\n",
      "Epoch [14/30], Step [1600/3535], Loss: 0.5009\n",
      "Epoch [14/30], Step [1620/3535], Loss: 0.8929\n",
      "Epoch [14/30], Step [1640/3535], Loss: 0.4100\n",
      "Epoch [14/30], Step [1660/3535], Loss: 0.7823\n",
      "Epoch [14/30], Step [1680/3535], Loss: 0.5737\n",
      "Epoch [14/30], Step [1700/3535], Loss: 0.7743\n",
      "Epoch [14/30], Step [1720/3535], Loss: 0.6010\n",
      "Epoch [14/30], Step [1740/3535], Loss: 0.5822\n",
      "Epoch [14/30], Step [1760/3535], Loss: 0.1901\n",
      "Epoch [14/30], Step [1780/3535], Loss: 0.3999\n",
      "Epoch [14/30], Step [1800/3535], Loss: 0.8028\n",
      "Epoch [14/30], Step [1820/3535], Loss: 1.3328\n",
      "Epoch [14/30], Step [1840/3535], Loss: 0.2640\n",
      "Epoch [14/30], Step [1860/3535], Loss: 0.7234\n",
      "Epoch [14/30], Step [1880/3535], Loss: 0.4064\n",
      "Epoch [14/30], Step [1900/3535], Loss: 0.6913\n",
      "Epoch [14/30], Step [1920/3535], Loss: 1.3884\n",
      "Epoch [14/30], Step [1940/3535], Loss: 0.7363\n",
      "Epoch [14/30], Step [1960/3535], Loss: 0.7591\n",
      "Epoch [14/30], Step [1980/3535], Loss: 0.6745\n",
      "Epoch [14/30], Step [2000/3535], Loss: 0.8201\n",
      "Epoch [14/30], Step [2020/3535], Loss: 0.3081\n",
      "Epoch [14/30], Step [2040/3535], Loss: 0.4880\n",
      "Epoch [14/30], Step [2060/3535], Loss: 0.6793\n",
      "Epoch [14/30], Step [2080/3535], Loss: 0.4385\n",
      "Epoch [14/30], Step [2100/3535], Loss: 1.0361\n",
      "Epoch [14/30], Step [2120/3535], Loss: 1.6478\n",
      "Epoch [14/30], Step [2140/3535], Loss: 1.2752\n",
      "Epoch [14/30], Step [2160/3535], Loss: 0.5263\n",
      "Epoch [14/30], Step [2180/3535], Loss: 0.5550\n",
      "Epoch [14/30], Step [2200/3535], Loss: 0.8169\n",
      "Epoch [14/30], Step [2220/3535], Loss: 1.1648\n",
      "Epoch [14/30], Step [2240/3535], Loss: 1.2039\n",
      "Epoch [14/30], Step [2260/3535], Loss: 0.3876\n",
      "Epoch [14/30], Step [2280/3535], Loss: 0.5891\n",
      "Epoch [14/30], Step [2300/3535], Loss: 0.2494\n",
      "Epoch [14/30], Step [2320/3535], Loss: 0.7447\n",
      "Epoch [14/30], Step [2340/3535], Loss: 0.9983\n",
      "Epoch [14/30], Step [2360/3535], Loss: 1.0450\n",
      "Epoch [14/30], Step [2380/3535], Loss: 0.7077\n",
      "Epoch [14/30], Step [2400/3535], Loss: 1.1221\n",
      "Epoch [14/30], Step [2420/3535], Loss: 1.1373\n",
      "Epoch [14/30], Step [2440/3535], Loss: 0.5126\n",
      "Epoch [14/30], Step [2460/3535], Loss: 0.3080\n",
      "Epoch [14/30], Step [2480/3535], Loss: 0.9232\n",
      "Epoch [14/30], Step [2500/3535], Loss: 0.5306\n",
      "Epoch [14/30], Step [2520/3535], Loss: 0.4573\n",
      "Epoch [14/30], Step [2540/3535], Loss: 0.3902\n",
      "Epoch [14/30], Step [2560/3535], Loss: 1.2197\n",
      "Epoch [14/30], Step [2580/3535], Loss: 0.7155\n",
      "Epoch [14/30], Step [2600/3535], Loss: 0.7011\n",
      "Epoch [14/30], Step [2620/3535], Loss: 1.0943\n",
      "Epoch [14/30], Step [2640/3535], Loss: 0.3739\n",
      "Epoch [14/30], Step [2660/3535], Loss: 0.3319\n",
      "Epoch [14/30], Step [2680/3535], Loss: 0.4539\n",
      "Epoch [14/30], Step [2700/3535], Loss: 0.6718\n",
      "Epoch [14/30], Step [2720/3535], Loss: 0.7476\n",
      "Epoch [14/30], Step [2740/3535], Loss: 0.7693\n",
      "Epoch [14/30], Step [2760/3535], Loss: 0.8770\n",
      "Epoch [14/30], Step [2780/3535], Loss: 0.9120\n",
      "Epoch [14/30], Step [2800/3535], Loss: 0.1423\n",
      "Epoch [14/30], Step [2820/3535], Loss: 0.5181\n",
      "Epoch [14/30], Step [2840/3535], Loss: 0.8336\n",
      "Epoch [14/30], Step [2860/3535], Loss: 0.6591\n",
      "Epoch [14/30], Step [2880/3535], Loss: 1.2005\n",
      "Epoch [14/30], Step [2900/3535], Loss: 0.3096\n",
      "Epoch [14/30], Step [2920/3535], Loss: 0.9321\n",
      "Epoch [14/30], Step [2940/3535], Loss: 0.8579\n",
      "Epoch [14/30], Step [2960/3535], Loss: 0.4377\n",
      "Epoch [14/30], Step [2980/3535], Loss: 0.7030\n",
      "Epoch [14/30], Step [3000/3535], Loss: 0.6979\n",
      "Epoch [14/30], Step [3020/3535], Loss: 0.3933\n",
      "Epoch [14/30], Step [3040/3535], Loss: 0.6111\n",
      "Epoch [14/30], Step [3060/3535], Loss: 1.3672\n",
      "Epoch [14/30], Step [3080/3535], Loss: 0.4426\n",
      "Epoch [14/30], Step [3100/3535], Loss: 0.5535\n",
      "Epoch [14/30], Step [3120/3535], Loss: 0.4459\n",
      "Epoch [14/30], Step [3140/3535], Loss: 0.5401\n",
      "Epoch [14/30], Step [3160/3535], Loss: 1.3891\n",
      "Epoch [14/30], Step [3180/3535], Loss: 0.3540\n",
      "Epoch [14/30], Step [3200/3535], Loss: 0.4372\n",
      "Epoch [14/30], Step [3220/3535], Loss: 1.0602\n",
      "Epoch [14/30], Step [3240/3535], Loss: 0.4408\n",
      "Epoch [14/30], Step [3260/3535], Loss: 0.5805\n",
      "Epoch [14/30], Step [3280/3535], Loss: 0.7798\n",
      "Epoch [14/30], Step [3300/3535], Loss: 1.1948\n",
      "Epoch [14/30], Step [3320/3535], Loss: 1.3612\n",
      "Epoch [14/30], Step [3340/3535], Loss: 0.5929\n",
      "Epoch [14/30], Step [3360/3535], Loss: 0.5944\n",
      "Epoch [14/30], Step [3380/3535], Loss: 0.3537\n",
      "Epoch [14/30], Step [3400/3535], Loss: 0.4832\n",
      "Epoch [14/30], Step [3420/3535], Loss: 1.3391\n",
      "Epoch [14/30], Step [3440/3535], Loss: 0.2025\n",
      "Epoch [14/30], Step [3460/3535], Loss: 0.2226\n",
      "Epoch [14/30], Step [3480/3535], Loss: 0.7049\n",
      "Epoch [14/30], Step [3500/3535], Loss: 1.3404\n",
      "Epoch [14/30], Step [3520/3535], Loss: 1.4085\n",
      "\n",
      "train-loss: 1.0902, train-acc: 75.2732\n",
      "validation loss: 1.1695, validation acc: 59.6293\n",
      "\n",
      "Epoch 15\n",
      "\n",
      "Epoch [15/30], Step [0/3535], Loss: 0.6161\n",
      "Epoch [15/30], Step [20/3535], Loss: 0.9931\n",
      "Epoch [15/30], Step [40/3535], Loss: 0.8803\n",
      "Epoch [15/30], Step [60/3535], Loss: 0.5396\n",
      "Epoch [15/30], Step [80/3535], Loss: 0.5358\n",
      "Epoch [15/30], Step [100/3535], Loss: 0.1512\n",
      "Epoch [15/30], Step [120/3535], Loss: 0.3790\n",
      "Epoch [15/30], Step [140/3535], Loss: 0.3103\n",
      "Epoch [15/30], Step [160/3535], Loss: 0.1362\n",
      "Epoch [15/30], Step [180/3535], Loss: 0.3081\n",
      "Epoch [15/30], Step [200/3535], Loss: 0.6993\n",
      "Epoch [15/30], Step [220/3535], Loss: 0.5154\n",
      "Epoch [15/30], Step [240/3535], Loss: 0.6231\n",
      "Epoch [15/30], Step [260/3535], Loss: 0.9494\n",
      "Epoch [15/30], Step [280/3535], Loss: 0.2567\n",
      "Epoch [15/30], Step [300/3535], Loss: 0.1158\n",
      "Epoch [15/30], Step [320/3535], Loss: 0.6010\n",
      "Epoch [15/30], Step [340/3535], Loss: 0.3139\n",
      "Epoch [15/30], Step [360/3535], Loss: 0.4907\n",
      "Epoch [15/30], Step [380/3535], Loss: 0.4133\n",
      "Epoch [15/30], Step [400/3535], Loss: 0.4911\n",
      "Epoch [15/30], Step [420/3535], Loss: 0.5018\n",
      "Epoch [15/30], Step [440/3535], Loss: 0.4618\n",
      "Epoch [15/30], Step [460/3535], Loss: 0.7440\n",
      "Epoch [15/30], Step [480/3535], Loss: 0.5932\n",
      "Epoch [15/30], Step [500/3535], Loss: 0.1411\n",
      "Epoch [15/30], Step [520/3535], Loss: 1.0301\n",
      "Epoch [15/30], Step [540/3535], Loss: 0.3864\n",
      "Epoch [15/30], Step [560/3535], Loss: 0.6322\n",
      "Epoch [15/30], Step [580/3535], Loss: 0.2930\n",
      "Epoch [15/30], Step [600/3535], Loss: 0.3738\n",
      "Epoch [15/30], Step [620/3535], Loss: 0.5037\n",
      "Epoch [15/30], Step [640/3535], Loss: 0.2972\n",
      "Epoch [15/30], Step [660/3535], Loss: 0.2034\n",
      "Epoch [15/30], Step [680/3535], Loss: 0.4834\n",
      "Epoch [15/30], Step [700/3535], Loss: 0.2669\n",
      "Epoch [15/30], Step [720/3535], Loss: 0.3685\n",
      "Epoch [15/30], Step [740/3535], Loss: 0.3598\n",
      "Epoch [15/30], Step [760/3535], Loss: 0.6071\n",
      "Epoch [15/30], Step [780/3535], Loss: 0.3641\n",
      "Epoch [15/30], Step [800/3535], Loss: 0.5693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Step [820/3535], Loss: 0.3675\n",
      "Epoch [15/30], Step [840/3535], Loss: 0.8067\n",
      "Epoch [15/30], Step [860/3535], Loss: 0.3083\n",
      "Epoch [15/30], Step [880/3535], Loss: 0.3753\n",
      "Epoch [15/30], Step [900/3535], Loss: 0.4825\n",
      "Epoch [15/30], Step [920/3535], Loss: 1.0095\n",
      "Epoch [15/30], Step [940/3535], Loss: 0.3274\n",
      "Epoch [15/30], Step [960/3535], Loss: 1.2396\n",
      "Epoch [15/30], Step [980/3535], Loss: 0.2455\n",
      "Epoch [15/30], Step [1000/3535], Loss: 0.1368\n",
      "Epoch [15/30], Step [1020/3535], Loss: 0.5376\n",
      "Epoch [15/30], Step [1040/3535], Loss: 0.2587\n",
      "Epoch [15/30], Step [1060/3535], Loss: 1.0584\n",
      "Epoch [15/30], Step [1080/3535], Loss: 0.4008\n",
      "Epoch [15/30], Step [1100/3535], Loss: 0.5506\n",
      "Epoch [15/30], Step [1120/3535], Loss: 0.7197\n",
      "Epoch [15/30], Step [1140/3535], Loss: 0.3818\n",
      "Epoch [15/30], Step [1160/3535], Loss: 0.2835\n",
      "Epoch [15/30], Step [1180/3535], Loss: 0.3260\n",
      "Epoch [15/30], Step [1200/3535], Loss: 0.9318\n",
      "Epoch [15/30], Step [1220/3535], Loss: 0.1849\n",
      "Epoch [15/30], Step [1240/3535], Loss: 0.3981\n",
      "Epoch [15/30], Step [1260/3535], Loss: 0.3306\n",
      "Epoch [15/30], Step [1280/3535], Loss: 0.7442\n",
      "Epoch [15/30], Step [1300/3535], Loss: 0.4201\n",
      "Epoch [15/30], Step [1320/3535], Loss: 0.1840\n",
      "Epoch [15/30], Step [1340/3535], Loss: 1.0576\n",
      "Epoch [15/30], Step [1360/3535], Loss: 0.5291\n",
      "Epoch [15/30], Step [1380/3535], Loss: 1.1788\n",
      "Epoch [15/30], Step [1400/3535], Loss: 0.6241\n",
      "Epoch [15/30], Step [1420/3535], Loss: 0.5756\n",
      "Epoch [15/30], Step [1440/3535], Loss: 0.9025\n",
      "Epoch [15/30], Step [1460/3535], Loss: 0.3325\n",
      "Epoch [15/30], Step [1480/3535], Loss: 0.5475\n",
      "Epoch [15/30], Step [1500/3535], Loss: 0.7375\n",
      "Epoch [15/30], Step [1520/3535], Loss: 0.7293\n",
      "Epoch [15/30], Step [1540/3535], Loss: 0.3759\n",
      "Epoch [15/30], Step [1560/3535], Loss: 0.3469\n",
      "Epoch [15/30], Step [1580/3535], Loss: 0.8510\n",
      "Epoch [15/30], Step [1600/3535], Loss: 1.2305\n",
      "Epoch [15/30], Step [1620/3535], Loss: 0.3158\n",
      "Epoch [15/30], Step [1640/3535], Loss: 0.0972\n",
      "Epoch [15/30], Step [1660/3535], Loss: 0.2272\n",
      "Epoch [15/30], Step [1680/3535], Loss: 0.5910\n",
      "Epoch [15/30], Step [1700/3535], Loss: 0.5069\n",
      "Epoch [15/30], Step [1720/3535], Loss: 0.8556\n",
      "Epoch [15/30], Step [1740/3535], Loss: 0.5781\n",
      "Epoch [15/30], Step [1760/3535], Loss: 0.7354\n",
      "Epoch [15/30], Step [1780/3535], Loss: 0.1687\n",
      "Epoch [15/30], Step [1800/3535], Loss: 0.4896\n",
      "Epoch [15/30], Step [1820/3535], Loss: 0.5222\n",
      "Epoch [15/30], Step [1840/3535], Loss: 0.3234\n",
      "Epoch [15/30], Step [1860/3535], Loss: 0.3111\n",
      "Epoch [15/30], Step [1880/3535], Loss: 0.4443\n",
      "Epoch [15/30], Step [1900/3535], Loss: 0.4793\n",
      "Epoch [15/30], Step [1920/3535], Loss: 0.3776\n",
      "Epoch [15/30], Step [1940/3535], Loss: 0.4635\n",
      "Epoch [15/30], Step [1960/3535], Loss: 0.4499\n",
      "Epoch [15/30], Step [1980/3535], Loss: 0.2750\n",
      "Epoch [15/30], Step [2000/3535], Loss: 0.2950\n",
      "Epoch [15/30], Step [2020/3535], Loss: 0.0759\n",
      "Epoch [15/30], Step [2040/3535], Loss: 0.8587\n",
      "Epoch [15/30], Step [2060/3535], Loss: 0.2563\n",
      "Epoch [15/30], Step [2080/3535], Loss: 0.4297\n",
      "Epoch [15/30], Step [2100/3535], Loss: 0.6331\n",
      "Epoch [15/30], Step [2120/3535], Loss: 0.6376\n",
      "Epoch [15/30], Step [2140/3535], Loss: 0.2806\n",
      "Epoch [15/30], Step [2160/3535], Loss: 0.5541\n",
      "Epoch [15/30], Step [2180/3535], Loss: 0.4430\n",
      "Epoch [15/30], Step [2200/3535], Loss: 1.0993\n",
      "Epoch [15/30], Step [2220/3535], Loss: 0.8481\n",
      "Epoch [15/30], Step [2240/3535], Loss: 0.3125\n",
      "Epoch [15/30], Step [2260/3535], Loss: 0.7056\n",
      "Epoch [15/30], Step [2280/3535], Loss: 0.3530\n",
      "Epoch [15/30], Step [2300/3535], Loss: 0.6114\n",
      "Epoch [15/30], Step [2320/3535], Loss: 0.2736\n",
      "Epoch [15/30], Step [2340/3535], Loss: 0.7265\n",
      "Epoch [15/30], Step [2360/3535], Loss: 0.3425\n",
      "Epoch [15/30], Step [2380/3535], Loss: 0.6232\n",
      "Epoch [15/30], Step [2400/3535], Loss: 0.2305\n",
      "Epoch [15/30], Step [2420/3535], Loss: 0.5732\n",
      "Epoch [15/30], Step [2440/3535], Loss: 0.8555\n",
      "Epoch [15/30], Step [2460/3535], Loss: 0.3288\n",
      "Epoch [15/30], Step [2480/3535], Loss: 0.4140\n",
      "Epoch [15/30], Step [2500/3535], Loss: 0.7754\n",
      "Epoch [15/30], Step [2520/3535], Loss: 1.6842\n",
      "Epoch [15/30], Step [2540/3535], Loss: 0.6541\n",
      "Epoch [15/30], Step [2560/3535], Loss: 0.5190\n",
      "Epoch [15/30], Step [2580/3535], Loss: 0.5654\n",
      "Epoch [15/30], Step [2600/3535], Loss: 0.3574\n",
      "Epoch [15/30], Step [2620/3535], Loss: 0.5149\n",
      "Epoch [15/30], Step [2640/3535], Loss: 1.0511\n",
      "Epoch [15/30], Step [2660/3535], Loss: 0.7734\n",
      "Epoch [15/30], Step [2680/3535], Loss: 0.7204\n",
      "Epoch [15/30], Step [2700/3535], Loss: 0.8860\n",
      "Epoch [15/30], Step [2720/3535], Loss: 0.6715\n",
      "Epoch [15/30], Step [2740/3535], Loss: 0.2708\n",
      "Epoch [15/30], Step [2760/3535], Loss: 0.6489\n",
      "Epoch [15/30], Step [2780/3535], Loss: 0.3220\n",
      "Epoch [15/30], Step [2800/3535], Loss: 0.3644\n",
      "Epoch [15/30], Step [2820/3535], Loss: 0.2448\n",
      "Epoch [15/30], Step [2840/3535], Loss: 1.4424\n",
      "Epoch [15/30], Step [2860/3535], Loss: 0.7238\n",
      "Epoch [15/30], Step [2880/3535], Loss: 1.0212\n",
      "Epoch [15/30], Step [2900/3535], Loss: 0.8180\n",
      "Epoch [15/30], Step [2920/3535], Loss: 0.3994\n",
      "Epoch [15/30], Step [2940/3535], Loss: 0.7921\n",
      "Epoch [15/30], Step [2960/3535], Loss: 0.5591\n",
      "Epoch [15/30], Step [2980/3535], Loss: 0.7048\n",
      "Epoch [15/30], Step [3000/3535], Loss: 0.0931\n",
      "Epoch [15/30], Step [3020/3535], Loss: 0.7362\n",
      "Epoch [15/30], Step [3040/3535], Loss: 0.3455\n",
      "Epoch [15/30], Step [3060/3535], Loss: 0.5884\n",
      "Epoch [15/30], Step [3080/3535], Loss: 1.0694\n",
      "Epoch [15/30], Step [3100/3535], Loss: 0.3769\n",
      "Epoch [15/30], Step [3120/3535], Loss: 1.2706\n",
      "Epoch [15/30], Step [3140/3535], Loss: 1.5862\n",
      "Epoch [15/30], Step [3160/3535], Loss: 0.9347\n",
      "Epoch [15/30], Step [3180/3535], Loss: 0.4249\n",
      "Epoch [15/30], Step [3200/3535], Loss: 0.3747\n",
      "Epoch [15/30], Step [3220/3535], Loss: 0.1870\n",
      "Epoch [15/30], Step [3240/3535], Loss: 0.3882\n",
      "Epoch [15/30], Step [3260/3535], Loss: 0.5762\n",
      "Epoch [15/30], Step [3280/3535], Loss: 0.3437\n",
      "Epoch [15/30], Step [3300/3535], Loss: 0.3910\n",
      "Epoch [15/30], Step [3320/3535], Loss: 0.6985\n",
      "Epoch [15/30], Step [3340/3535], Loss: 0.3369\n",
      "Epoch [15/30], Step [3360/3535], Loss: 0.4583\n",
      "Epoch [15/30], Step [3380/3535], Loss: 0.6786\n",
      "Epoch [15/30], Step [3400/3535], Loss: 1.1872\n",
      "Epoch [15/30], Step [3420/3535], Loss: 0.8145\n",
      "Epoch [15/30], Step [3440/3535], Loss: 0.2610\n",
      "Epoch [15/30], Step [3460/3535], Loss: 0.6139\n",
      "Epoch [15/30], Step [3480/3535], Loss: 0.3846\n",
      "Epoch [15/30], Step [3500/3535], Loss: 0.5677\n",
      "Epoch [15/30], Step [3520/3535], Loss: 0.2732\n",
      "\n",
      "train-loss: 1.0574, train-acc: 77.9613\n",
      "validation loss: 1.1715, validation acc: 59.7849\n",
      "\n",
      "Epoch 16\n",
      "\n",
      "Epoch [16/30], Step [0/3535], Loss: 0.7183\n",
      "Epoch [16/30], Step [20/3535], Loss: 0.7699\n",
      "Epoch [16/30], Step [40/3535], Loss: 0.4811\n",
      "Epoch [16/30], Step [60/3535], Loss: 0.2660\n",
      "Epoch [16/30], Step [80/3535], Loss: 0.3882\n",
      "Epoch [16/30], Step [100/3535], Loss: 0.4722\n",
      "Epoch [16/30], Step [120/3535], Loss: 0.1834\n",
      "Epoch [16/30], Step [140/3535], Loss: 0.3629\n",
      "Epoch [16/30], Step [160/3535], Loss: 0.2466\n",
      "Epoch [16/30], Step [180/3535], Loss: 0.6157\n",
      "Epoch [16/30], Step [200/3535], Loss: 0.0951\n",
      "Epoch [16/30], Step [220/3535], Loss: 0.3061\n",
      "Epoch [16/30], Step [240/3535], Loss: 0.5702\n",
      "Epoch [16/30], Step [260/3535], Loss: 0.2645\n",
      "Epoch [16/30], Step [280/3535], Loss: 0.3972\n",
      "Epoch [16/30], Step [300/3535], Loss: 0.4000\n",
      "Epoch [16/30], Step [320/3535], Loss: 0.6944\n",
      "Epoch [16/30], Step [340/3535], Loss: 0.2199\n",
      "Epoch [16/30], Step [360/3535], Loss: 0.2756\n",
      "Epoch [16/30], Step [380/3535], Loss: 0.2069\n",
      "Epoch [16/30], Step [400/3535], Loss: 0.3300\n",
      "Epoch [16/30], Step [420/3535], Loss: 0.7322\n",
      "Epoch [16/30], Step [440/3535], Loss: 0.4022\n",
      "Epoch [16/30], Step [460/3535], Loss: 0.3654\n",
      "Epoch [16/30], Step [480/3535], Loss: 0.8675\n",
      "Epoch [16/30], Step [500/3535], Loss: 0.4061\n",
      "Epoch [16/30], Step [520/3535], Loss: 0.4941\n",
      "Epoch [16/30], Step [540/3535], Loss: 0.2911\n",
      "Epoch [16/30], Step [560/3535], Loss: 0.2901\n",
      "Epoch [16/30], Step [580/3535], Loss: 0.6077\n",
      "Epoch [16/30], Step [600/3535], Loss: 0.6649\n",
      "Epoch [16/30], Step [620/3535], Loss: 0.5195\n",
      "Epoch [16/30], Step [640/3535], Loss: 0.3542\n",
      "Epoch [16/30], Step [660/3535], Loss: 0.3371\n",
      "Epoch [16/30], Step [680/3535], Loss: 0.1875\n",
      "Epoch [16/30], Step [700/3535], Loss: 0.5663\n",
      "Epoch [16/30], Step [720/3535], Loss: 0.8639\n",
      "Epoch [16/30], Step [740/3535], Loss: 0.7255\n",
      "Epoch [16/30], Step [760/3535], Loss: 0.1885\n",
      "Epoch [16/30], Step [780/3535], Loss: 0.2007\n",
      "Epoch [16/30], Step [800/3535], Loss: 0.7523\n",
      "Epoch [16/30], Step [820/3535], Loss: 0.8944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30], Step [840/3535], Loss: 1.1552\n",
      "Epoch [16/30], Step [860/3535], Loss: 0.3006\n",
      "Epoch [16/30], Step [880/3535], Loss: 0.9774\n",
      "Epoch [16/30], Step [900/3535], Loss: 0.2664\n",
      "Epoch [16/30], Step [920/3535], Loss: 0.2873\n",
      "Epoch [16/30], Step [940/3535], Loss: 0.2363\n",
      "Epoch [16/30], Step [960/3535], Loss: 1.0433\n",
      "Epoch [16/30], Step [980/3535], Loss: 0.6533\n",
      "Epoch [16/30], Step [1000/3535], Loss: 0.4061\n",
      "Epoch [16/30], Step [1020/3535], Loss: 0.7374\n",
      "Epoch [16/30], Step [1040/3535], Loss: 0.1880\n",
      "Epoch [16/30], Step [1060/3535], Loss: 0.7562\n",
      "Epoch [16/30], Step [1080/3535], Loss: 0.3968\n",
      "Epoch [16/30], Step [1100/3535], Loss: 0.8293\n",
      "Epoch [16/30], Step [1120/3535], Loss: 0.7096\n",
      "Epoch [16/30], Step [1140/3535], Loss: 0.4123\n",
      "Epoch [16/30], Step [1160/3535], Loss: 0.2980\n",
      "Epoch [16/30], Step [1180/3535], Loss: 0.1636\n",
      "Epoch [16/30], Step [1200/3535], Loss: 0.5198\n",
      "Epoch [16/30], Step [1220/3535], Loss: 0.1815\n",
      "Epoch [16/30], Step [1240/3535], Loss: 0.1040\n",
      "Epoch [16/30], Step [1260/3535], Loss: 0.8972\n",
      "Epoch [16/30], Step [1280/3535], Loss: 0.8184\n",
      "Epoch [16/30], Step [1300/3535], Loss: 0.3291\n",
      "Epoch [16/30], Step [1320/3535], Loss: 0.4173\n",
      "Epoch [16/30], Step [1340/3535], Loss: 0.7634\n",
      "Epoch [16/30], Step [1360/3535], Loss: 0.2710\n",
      "Epoch [16/30], Step [1380/3535], Loss: 0.3807\n",
      "Epoch [16/30], Step [1400/3535], Loss: 0.6228\n",
      "Epoch [16/30], Step [1420/3535], Loss: 0.2344\n",
      "Epoch [16/30], Step [1440/3535], Loss: 0.7636\n",
      "Epoch [16/30], Step [1460/3535], Loss: 0.5949\n",
      "Epoch [16/30], Step [1480/3535], Loss: 0.4003\n",
      "Epoch [16/30], Step [1500/3535], Loss: 0.4139\n",
      "Epoch [16/30], Step [1520/3535], Loss: 0.4811\n",
      "Epoch [16/30], Step [1540/3535], Loss: 0.4486\n",
      "Epoch [16/30], Step [1560/3535], Loss: 0.8264\n",
      "Epoch [16/30], Step [1580/3535], Loss: 0.0851\n",
      "Epoch [16/30], Step [1600/3535], Loss: 0.6448\n",
      "Epoch [16/30], Step [1620/3535], Loss: 0.4798\n",
      "Epoch [16/30], Step [1640/3535], Loss: 0.6883\n",
      "Epoch [16/30], Step [1660/3535], Loss: 0.5294\n",
      "Epoch [16/30], Step [1680/3535], Loss: 0.5475\n",
      "Epoch [16/30], Step [1700/3535], Loss: 0.7927\n",
      "Epoch [16/30], Step [1720/3535], Loss: 0.3125\n",
      "Epoch [16/30], Step [1740/3535], Loss: 0.6395\n",
      "Epoch [16/30], Step [1760/3535], Loss: 0.6102\n",
      "Epoch [16/30], Step [1780/3535], Loss: 0.3274\n",
      "Epoch [16/30], Step [1800/3535], Loss: 0.7941\n",
      "Epoch [16/30], Step [1820/3535], Loss: 0.2424\n",
      "Epoch [16/30], Step [1840/3535], Loss: 1.1508\n",
      "Epoch [16/30], Step [1860/3535], Loss: 0.3840\n",
      "Epoch [16/30], Step [1880/3535], Loss: 0.5142\n",
      "Epoch [16/30], Step [1900/3535], Loss: 0.4875\n",
      "Epoch [16/30], Step [1920/3535], Loss: 0.2663\n",
      "Epoch [16/30], Step [1940/3535], Loss: 0.4603\n",
      "Epoch [16/30], Step [1960/3535], Loss: 0.6180\n",
      "Epoch [16/30], Step [1980/3535], Loss: 0.0968\n",
      "Epoch [16/30], Step [2000/3535], Loss: 0.3708\n",
      "Epoch [16/30], Step [2020/3535], Loss: 0.3802\n",
      "Epoch [16/30], Step [2040/3535], Loss: 0.4135\n",
      "Epoch [16/30], Step [2060/3535], Loss: 0.9945\n",
      "Epoch [16/30], Step [2080/3535], Loss: 0.5027\n",
      "Epoch [16/30], Step [2100/3535], Loss: 0.6605\n",
      "Epoch [16/30], Step [2120/3535], Loss: 0.9166\n",
      "Epoch [16/30], Step [2140/3535], Loss: 0.2420\n",
      "Epoch [16/30], Step [2160/3535], Loss: 0.4977\n",
      "Epoch [16/30], Step [2180/3535], Loss: 1.1936\n",
      "Epoch [16/30], Step [2200/3535], Loss: 0.4693\n",
      "Epoch [16/30], Step [2220/3535], Loss: 0.4342\n",
      "Epoch [16/30], Step [2240/3535], Loss: 0.6068\n",
      "Epoch [16/30], Step [2260/3535], Loss: 0.6940\n",
      "Epoch [16/30], Step [2280/3535], Loss: 0.2025\n",
      "Epoch [16/30], Step [2300/3535], Loss: 0.6151\n",
      "Epoch [16/30], Step [2320/3535], Loss: 0.1988\n",
      "Epoch [16/30], Step [2340/3535], Loss: 0.3690\n",
      "Epoch [16/30], Step [2360/3535], Loss: 0.3490\n",
      "Epoch [16/30], Step [2380/3535], Loss: 0.5792\n",
      "Epoch [16/30], Step [2400/3535], Loss: 0.1200\n",
      "Epoch [16/30], Step [2420/3535], Loss: 0.3236\n",
      "Epoch [16/30], Step [2440/3535], Loss: 0.2010\n",
      "Epoch [16/30], Step [2460/3535], Loss: 0.1955\n",
      "Epoch [16/30], Step [2480/3535], Loss: 0.7216\n",
      "Epoch [16/30], Step [2500/3535], Loss: 0.9099\n",
      "Epoch [16/30], Step [2520/3535], Loss: 0.1983\n",
      "Epoch [16/30], Step [2540/3535], Loss: 0.9084\n",
      "Epoch [16/30], Step [2560/3535], Loss: 0.3024\n",
      "Epoch [16/30], Step [2580/3535], Loss: 0.5693\n",
      "Epoch [16/30], Step [2600/3535], Loss: 0.3612\n",
      "Epoch [16/30], Step [2620/3535], Loss: 0.7217\n",
      "Epoch [16/30], Step [2640/3535], Loss: 0.7198\n",
      "Epoch [16/30], Step [2660/3535], Loss: 0.4258\n",
      "Epoch [16/30], Step [2680/3535], Loss: 0.6945\n",
      "Epoch [16/30], Step [2700/3535], Loss: 0.7636\n",
      "Epoch [16/30], Step [2720/3535], Loss: 0.9974\n",
      "Epoch [16/30], Step [2740/3535], Loss: 0.3334\n",
      "Epoch [16/30], Step [2760/3535], Loss: 0.8860\n",
      "Epoch [16/30], Step [2780/3535], Loss: 0.9178\n",
      "Epoch [16/30], Step [2800/3535], Loss: 0.9034\n",
      "Epoch [16/30], Step [2820/3535], Loss: 0.7435\n",
      "Epoch [16/30], Step [2840/3535], Loss: 0.6832\n",
      "Epoch [16/30], Step [2860/3535], Loss: 0.2145\n",
      "Epoch [16/30], Step [2880/3535], Loss: 0.4858\n",
      "Epoch [16/30], Step [2900/3535], Loss: 0.2061\n",
      "Epoch [16/30], Step [2920/3535], Loss: 0.2665\n",
      "Epoch [16/30], Step [2940/3535], Loss: 0.5692\n",
      "Epoch [16/30], Step [2960/3535], Loss: 0.7044\n",
      "Epoch [16/30], Step [2980/3535], Loss: 0.3135\n",
      "Epoch [16/30], Step [3000/3535], Loss: 0.6161\n",
      "Epoch [16/30], Step [3020/3535], Loss: 0.7118\n",
      "Epoch [16/30], Step [3040/3535], Loss: 0.4526\n",
      "Epoch [16/30], Step [3060/3535], Loss: 0.3968\n",
      "Epoch [16/30], Step [3080/3535], Loss: 0.6540\n",
      "Epoch [16/30], Step [3100/3535], Loss: 0.5606\n",
      "Epoch [16/30], Step [3120/3535], Loss: 0.2328\n",
      "Epoch [16/30], Step [3140/3535], Loss: 0.5293\n",
      "Epoch [16/30], Step [3160/3535], Loss: 0.9394\n",
      "Epoch [16/30], Step [3180/3535], Loss: 0.1623\n",
      "Epoch [16/30], Step [3200/3535], Loss: 0.9126\n",
      "Epoch [16/30], Step [3220/3535], Loss: 0.5258\n",
      "Epoch [16/30], Step [3240/3535], Loss: 0.4371\n",
      "Epoch [16/30], Step [3260/3535], Loss: 0.2941\n",
      "Epoch [16/30], Step [3280/3535], Loss: 0.7646\n",
      "Epoch [16/30], Step [3300/3535], Loss: 1.0273\n",
      "Epoch [16/30], Step [3320/3535], Loss: 0.7321\n",
      "Epoch [16/30], Step [3340/3535], Loss: 0.2121\n",
      "Epoch [16/30], Step [3360/3535], Loss: 0.6489\n",
      "Epoch [16/30], Step [3380/3535], Loss: 0.3866\n",
      "Epoch [16/30], Step [3400/3535], Loss: 0.4299\n",
      "Epoch [16/30], Step [3420/3535], Loss: 0.5456\n",
      "Epoch [16/30], Step [3440/3535], Loss: 0.6944\n",
      "Epoch [16/30], Step [3460/3535], Loss: 0.4918\n",
      "Epoch [16/30], Step [3480/3535], Loss: 0.2535\n",
      "Epoch [16/30], Step [3500/3535], Loss: 0.6596\n",
      "Epoch [16/30], Step [3520/3535], Loss: 0.3010\n",
      "\n",
      "train-loss: 1.0237, train-acc: 81.2507\n",
      "validation loss: 1.1728, validation acc: 60.6764\n",
      "\n",
      "Epoch 17\n",
      "\n",
      "Epoch [17/30], Step [0/3535], Loss: 0.2699\n",
      "Epoch [17/30], Step [20/3535], Loss: 0.8068\n",
      "Epoch [17/30], Step [40/3535], Loss: 1.0067\n",
      "Epoch [17/30], Step [60/3535], Loss: 0.4138\n",
      "Epoch [17/30], Step [80/3535], Loss: 0.5118\n",
      "Epoch [17/30], Step [100/3535], Loss: 0.2514\n",
      "Epoch [17/30], Step [120/3535], Loss: 0.2805\n",
      "Epoch [17/30], Step [140/3535], Loss: 0.3289\n",
      "Epoch [17/30], Step [160/3535], Loss: 0.4077\n",
      "Epoch [17/30], Step [180/3535], Loss: 0.8028\n",
      "Epoch [17/30], Step [200/3535], Loss: 0.3248\n",
      "Epoch [17/30], Step [220/3535], Loss: 0.4453\n",
      "Epoch [17/30], Step [240/3535], Loss: 0.4366\n",
      "Epoch [17/30], Step [260/3535], Loss: 0.2969\n",
      "Epoch [17/30], Step [280/3535], Loss: 0.6198\n",
      "Epoch [17/30], Step [300/3535], Loss: 0.2387\n",
      "Epoch [17/30], Step [320/3535], Loss: 0.1844\n",
      "Epoch [17/30], Step [340/3535], Loss: 0.8151\n",
      "Epoch [17/30], Step [360/3535], Loss: 0.1021\n",
      "Epoch [17/30], Step [380/3535], Loss: 0.3902\n",
      "Epoch [17/30], Step [400/3535], Loss: 0.3473\n",
      "Epoch [17/30], Step [420/3535], Loss: 0.1097\n",
      "Epoch [17/30], Step [440/3535], Loss: 1.0089\n",
      "Epoch [17/30], Step [460/3535], Loss: 0.1310\n",
      "Epoch [17/30], Step [480/3535], Loss: 0.1026\n",
      "Epoch [17/30], Step [500/3535], Loss: 0.1145\n",
      "Epoch [17/30], Step [520/3535], Loss: 0.1136\n",
      "Epoch [17/30], Step [540/3535], Loss: 0.5164\n",
      "Epoch [17/30], Step [560/3535], Loss: 0.1272\n",
      "Epoch [17/30], Step [580/3535], Loss: 0.5039\n",
      "Epoch [17/30], Step [600/3535], Loss: 0.4289\n",
      "Epoch [17/30], Step [620/3535], Loss: 0.1056\n",
      "Epoch [17/30], Step [640/3535], Loss: 1.3170\n",
      "Epoch [17/30], Step [660/3535], Loss: 0.3542\n",
      "Epoch [17/30], Step [680/3535], Loss: 0.6850\n",
      "Epoch [17/30], Step [700/3535], Loss: 0.6956\n",
      "Epoch [17/30], Step [720/3535], Loss: 0.1148\n",
      "Epoch [17/30], Step [740/3535], Loss: 0.1267\n",
      "Epoch [17/30], Step [760/3535], Loss: 0.2101\n",
      "Epoch [17/30], Step [780/3535], Loss: 0.3034\n",
      "Epoch [17/30], Step [800/3535], Loss: 0.2305\n",
      "Epoch [17/30], Step [820/3535], Loss: 0.2821\n",
      "Epoch [17/30], Step [840/3535], Loss: 0.4187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30], Step [860/3535], Loss: 0.4975\n",
      "Epoch [17/30], Step [880/3535], Loss: 0.1333\n",
      "Epoch [17/30], Step [900/3535], Loss: 0.3448\n",
      "Epoch [17/30], Step [920/3535], Loss: 0.2613\n",
      "Epoch [17/30], Step [940/3535], Loss: 0.0490\n",
      "Epoch [17/30], Step [960/3535], Loss: 0.0631\n",
      "Epoch [17/30], Step [980/3535], Loss: 0.5044\n",
      "Epoch [17/30], Step [1000/3535], Loss: 0.1614\n",
      "Epoch [17/30], Step [1020/3535], Loss: 0.8457\n",
      "Epoch [17/30], Step [1040/3535], Loss: 1.3367\n",
      "Epoch [17/30], Step [1060/3535], Loss: 0.6915\n",
      "Epoch [17/30], Step [1080/3535], Loss: 0.5952\n",
      "Epoch [17/30], Step [1100/3535], Loss: 0.2917\n",
      "Epoch [17/30], Step [1120/3535], Loss: 0.3689\n",
      "Epoch [17/30], Step [1140/3535], Loss: 1.0069\n",
      "Epoch [17/30], Step [1160/3535], Loss: 0.3096\n",
      "Epoch [17/30], Step [1180/3535], Loss: 0.5447\n",
      "Epoch [17/30], Step [1200/3535], Loss: 0.4347\n",
      "Epoch [17/30], Step [1220/3535], Loss: 0.4113\n",
      "Epoch [17/30], Step [1240/3535], Loss: 0.4685\n",
      "Epoch [17/30], Step [1260/3535], Loss: 0.5973\n",
      "Epoch [17/30], Step [1280/3535], Loss: 0.3829\n",
      "Epoch [17/30], Step [1300/3535], Loss: 0.0995\n",
      "Epoch [17/30], Step [1320/3535], Loss: 0.5479\n",
      "Epoch [17/30], Step [1340/3535], Loss: 0.2038\n",
      "Epoch [17/30], Step [1360/3535], Loss: 0.3340\n",
      "Epoch [17/30], Step [1380/3535], Loss: 0.2824\n",
      "Epoch [17/30], Step [1400/3535], Loss: 0.4301\n",
      "Epoch [17/30], Step [1420/3535], Loss: 0.2343\n",
      "Epoch [17/30], Step [1440/3535], Loss: 1.0392\n",
      "Epoch [17/30], Step [1460/3535], Loss: 0.4154\n",
      "Epoch [17/30], Step [1480/3535], Loss: 0.2378\n",
      "Epoch [17/30], Step [1500/3535], Loss: 0.0772\n",
      "Epoch [17/30], Step [1520/3535], Loss: 0.1509\n",
      "Epoch [17/30], Step [1540/3535], Loss: 0.3944\n",
      "Epoch [17/30], Step [1560/3535], Loss: 1.0607\n",
      "Epoch [17/30], Step [1580/3535], Loss: 0.1382\n",
      "Epoch [17/30], Step [1600/3535], Loss: 0.2622\n",
      "Epoch [17/30], Step [1620/3535], Loss: 0.7449\n",
      "Epoch [17/30], Step [1640/3535], Loss: 0.3861\n",
      "Epoch [17/30], Step [1660/3535], Loss: 0.0888\n",
      "Epoch [17/30], Step [1680/3535], Loss: 0.9046\n",
      "Epoch [17/30], Step [1700/3535], Loss: 0.3122\n",
      "Epoch [17/30], Step [1720/3535], Loss: 0.5663\n",
      "Epoch [17/30], Step [1740/3535], Loss: 0.1030\n",
      "Epoch [17/30], Step [1760/3535], Loss: 0.1061\n",
      "Epoch [17/30], Step [1780/3535], Loss: 0.3775\n",
      "Epoch [17/30], Step [1800/3535], Loss: 0.4511\n",
      "Epoch [17/30], Step [1820/3535], Loss: 0.4600\n",
      "Epoch [17/30], Step [1840/3535], Loss: 0.0648\n",
      "Epoch [17/30], Step [1860/3535], Loss: 0.7332\n",
      "Epoch [17/30], Step [1880/3535], Loss: 0.2790\n",
      "Epoch [17/30], Step [1900/3535], Loss: 0.7212\n",
      "Epoch [17/30], Step [1920/3535], Loss: 0.1293\n",
      "Epoch [17/30], Step [1940/3535], Loss: 0.7859\n",
      "Epoch [17/30], Step [1960/3535], Loss: 0.1060\n",
      "Epoch [17/30], Step [1980/3535], Loss: 0.4182\n",
      "Epoch [17/30], Step [2000/3535], Loss: 0.4586\n",
      "Epoch [17/30], Step [2020/3535], Loss: 0.6274\n",
      "Epoch [17/30], Step [2040/3535], Loss: 0.6783\n",
      "Epoch [17/30], Step [2060/3535], Loss: 0.1635\n",
      "Epoch [17/30], Step [2080/3535], Loss: 0.9555\n",
      "Epoch [17/30], Step [2100/3535], Loss: 0.4156\n",
      "Epoch [17/30], Step [2120/3535], Loss: 0.4120\n",
      "Epoch [17/30], Step [2140/3535], Loss: 0.2174\n",
      "Epoch [17/30], Step [2160/3535], Loss: 0.8575\n",
      "Epoch [17/30], Step [2180/3535], Loss: 0.3256\n",
      "Epoch [17/30], Step [2200/3535], Loss: 0.1889\n",
      "Epoch [17/30], Step [2220/3535], Loss: 0.2814\n",
      "Epoch [17/30], Step [2240/3535], Loss: 0.7590\n",
      "Epoch [17/30], Step [2260/3535], Loss: 0.5161\n",
      "Epoch [17/30], Step [2280/3535], Loss: 0.6566\n",
      "Epoch [17/30], Step [2300/3535], Loss: 0.2674\n",
      "Epoch [17/30], Step [2320/3535], Loss: 0.1310\n",
      "Epoch [17/30], Step [2340/3535], Loss: 0.5789\n",
      "Epoch [17/30], Step [2360/3535], Loss: 0.0437\n",
      "Epoch [17/30], Step [2380/3535], Loss: 0.4510\n",
      "Epoch [17/30], Step [2400/3535], Loss: 0.2629\n",
      "Epoch [17/30], Step [2420/3535], Loss: 0.2661\n",
      "Epoch [17/30], Step [2440/3535], Loss: 0.5260\n",
      "Epoch [17/30], Step [2460/3535], Loss: 0.1326\n",
      "Epoch [17/30], Step [2480/3535], Loss: 0.4080\n",
      "Epoch [17/30], Step [2500/3535], Loss: 1.2344\n",
      "Epoch [17/30], Step [2520/3535], Loss: 0.1676\n",
      "Epoch [17/30], Step [2540/3535], Loss: 0.5963\n",
      "Epoch [17/30], Step [2560/3535], Loss: 0.6395\n",
      "Epoch [17/30], Step [2580/3535], Loss: 0.5130\n",
      "Epoch [17/30], Step [2600/3535], Loss: 0.1065\n",
      "Epoch [17/30], Step [2620/3535], Loss: 0.5541\n",
      "Epoch [17/30], Step [2640/3535], Loss: 1.0613\n",
      "Epoch [17/30], Step [2660/3535], Loss: 0.4519\n",
      "Epoch [17/30], Step [2680/3535], Loss: 1.0307\n",
      "Epoch [17/30], Step [2700/3535], Loss: 0.3005\n",
      "Epoch [17/30], Step [2720/3535], Loss: 0.5393\n",
      "Epoch [17/30], Step [2740/3535], Loss: 0.6933\n",
      "Epoch [17/30], Step [2760/3535], Loss: 0.8433\n",
      "Epoch [17/30], Step [2780/3535], Loss: 0.7033\n",
      "Epoch [17/30], Step [2800/3535], Loss: 0.8140\n",
      "Epoch [17/30], Step [2820/3535], Loss: 0.3729\n",
      "Epoch [17/30], Step [2840/3535], Loss: 0.7624\n",
      "Epoch [17/30], Step [2860/3535], Loss: 0.2803\n",
      "Epoch [17/30], Step [2880/3535], Loss: 0.8653\n",
      "Epoch [17/30], Step [2900/3535], Loss: 0.8755\n",
      "Epoch [17/30], Step [2920/3535], Loss: 0.3322\n",
      "Epoch [17/30], Step [2940/3535], Loss: 0.4982\n",
      "Epoch [17/30], Step [2960/3535], Loss: 0.8409\n",
      "Epoch [17/30], Step [2980/3535], Loss: 0.4363\n",
      "Epoch [17/30], Step [3000/3535], Loss: 0.6302\n",
      "Epoch [17/30], Step [3020/3535], Loss: 0.8222\n",
      "Epoch [17/30], Step [3040/3535], Loss: 0.5136\n",
      "Epoch [17/30], Step [3060/3535], Loss: 0.6388\n",
      "Epoch [17/30], Step [3080/3535], Loss: 0.5194\n",
      "Epoch [17/30], Step [3100/3535], Loss: 0.7934\n",
      "Epoch [17/30], Step [3120/3535], Loss: 0.3910\n",
      "Epoch [17/30], Step [3140/3535], Loss: 0.2534\n",
      "Epoch [17/30], Step [3160/3535], Loss: 0.2798\n",
      "Epoch [17/30], Step [3180/3535], Loss: 0.4684\n",
      "Epoch [17/30], Step [3200/3535], Loss: 0.5784\n",
      "Epoch [17/30], Step [3220/3535], Loss: 0.2513\n",
      "Epoch [17/30], Step [3240/3535], Loss: 0.3490\n",
      "Epoch [17/30], Step [3260/3535], Loss: 0.7030\n",
      "Epoch [17/30], Step [3280/3535], Loss: 0.3042\n",
      "Epoch [17/30], Step [3300/3535], Loss: 0.3577\n",
      "Epoch [17/30], Step [3320/3535], Loss: 0.3324\n",
      "Epoch [17/30], Step [3340/3535], Loss: 0.2280\n",
      "Epoch [17/30], Step [3360/3535], Loss: 0.6979\n",
      "Epoch [17/30], Step [3380/3535], Loss: 0.4013\n",
      "Epoch [17/30], Step [3400/3535], Loss: 0.2006\n",
      "Epoch [17/30], Step [3420/3535], Loss: 0.4086\n",
      "Epoch [17/30], Step [3440/3535], Loss: 0.3633\n",
      "Epoch [17/30], Step [3460/3535], Loss: 0.6835\n",
      "Epoch [17/30], Step [3480/3535], Loss: 0.2324\n",
      "Epoch [17/30], Step [3500/3535], Loss: 1.4906\n",
      "Epoch [17/30], Step [3520/3535], Loss: 0.7098\n",
      "\n",
      "train-loss: 0.9889, train-acc: 84.7239\n",
      "validation loss: 1.1803, validation acc: 60.8745\n",
      "\n",
      "Epoch 18\n",
      "\n",
      "Epoch [18/30], Step [0/3535], Loss: 0.2972\n",
      "Epoch [18/30], Step [20/3535], Loss: 0.3186\n",
      "Epoch [18/30], Step [40/3535], Loss: 0.1683\n",
      "Epoch [18/30], Step [60/3535], Loss: 0.0326\n",
      "Epoch [18/30], Step [80/3535], Loss: 0.1967\n",
      "Epoch [18/30], Step [100/3535], Loss: 0.1652\n",
      "Epoch [18/30], Step [120/3535], Loss: 0.4408\n",
      "Epoch [18/30], Step [140/3535], Loss: 0.2542\n",
      "Epoch [18/30], Step [160/3535], Loss: 0.1958\n",
      "Epoch [18/30], Step [180/3535], Loss: 0.0805\n",
      "Epoch [18/30], Step [200/3535], Loss: 0.2140\n",
      "Epoch [18/30], Step [220/3535], Loss: 0.4593\n",
      "Epoch [18/30], Step [240/3535], Loss: 0.1422\n",
      "Epoch [18/30], Step [260/3535], Loss: 0.2808\n",
      "Epoch [18/30], Step [280/3535], Loss: 0.5615\n",
      "Epoch [18/30], Step [300/3535], Loss: 0.3972\n",
      "Epoch [18/30], Step [320/3535], Loss: 0.6633\n",
      "Epoch [18/30], Step [340/3535], Loss: 0.7115\n",
      "Epoch [18/30], Step [360/3535], Loss: 0.0313\n",
      "Epoch [18/30], Step [380/3535], Loss: 0.2280\n",
      "Epoch [18/30], Step [400/3535], Loss: 0.4643\n",
      "Epoch [18/30], Step [420/3535], Loss: 0.4738\n",
      "Epoch [18/30], Step [440/3535], Loss: 0.0697\n",
      "Epoch [18/30], Step [460/3535], Loss: 0.2767\n",
      "Epoch [18/30], Step [480/3535], Loss: 0.5093\n",
      "Epoch [18/30], Step [500/3535], Loss: 0.4322\n",
      "Epoch [18/30], Step [520/3535], Loss: 0.0784\n",
      "Epoch [18/30], Step [540/3535], Loss: 0.5840\n",
      "Epoch [18/30], Step [560/3535], Loss: 0.1660\n",
      "Epoch [18/30], Step [580/3535], Loss: 0.2161\n",
      "Epoch [18/30], Step [600/3535], Loss: 0.0706\n",
      "Epoch [18/30], Step [620/3535], Loss: 0.0632\n",
      "Epoch [18/30], Step [640/3535], Loss: 0.0968\n",
      "Epoch [18/30], Step [660/3535], Loss: 0.0716\n",
      "Epoch [18/30], Step [680/3535], Loss: 0.0894\n",
      "Epoch [18/30], Step [700/3535], Loss: 0.2981\n",
      "Epoch [18/30], Step [720/3535], Loss: 0.1894\n",
      "Epoch [18/30], Step [740/3535], Loss: 0.0828\n",
      "Epoch [18/30], Step [760/3535], Loss: 0.1495\n",
      "Epoch [18/30], Step [780/3535], Loss: 0.6664\n",
      "Epoch [18/30], Step [800/3535], Loss: 0.0629\n",
      "Epoch [18/30], Step [820/3535], Loss: 0.3857\n",
      "Epoch [18/30], Step [840/3535], Loss: 0.4233\n",
      "Epoch [18/30], Step [860/3535], Loss: 0.0352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/30], Step [880/3535], Loss: 0.2484\n",
      "Epoch [18/30], Step [900/3535], Loss: 0.3591\n",
      "Epoch [18/30], Step [920/3535], Loss: 0.4230\n",
      "Epoch [18/30], Step [940/3535], Loss: 0.1000\n",
      "Epoch [18/30], Step [960/3535], Loss: 0.3366\n",
      "Epoch [18/30], Step [980/3535], Loss: 0.3035\n",
      "Epoch [18/30], Step [1000/3535], Loss: 0.1208\n",
      "Epoch [18/30], Step [1020/3535], Loss: 0.3797\n",
      "Epoch [18/30], Step [1040/3535], Loss: 0.2526\n",
      "Epoch [18/30], Step [1060/3535], Loss: 0.4112\n",
      "Epoch [18/30], Step [1080/3535], Loss: 0.3085\n",
      "Epoch [18/30], Step [1100/3535], Loss: 0.2615\n",
      "Epoch [18/30], Step [1120/3535], Loss: 0.1416\n",
      "Epoch [18/30], Step [1140/3535], Loss: 0.6393\n",
      "Epoch [18/30], Step [1160/3535], Loss: 0.2741\n",
      "Epoch [18/30], Step [1180/3535], Loss: 0.2036\n",
      "Epoch [18/30], Step [1200/3535], Loss: 0.0440\n",
      "Epoch [18/30], Step [1220/3535], Loss: 0.3425\n",
      "Epoch [18/30], Step [1240/3535], Loss: 0.3754\n",
      "Epoch [18/30], Step [1260/3535], Loss: 0.2228\n",
      "Epoch [18/30], Step [1280/3535], Loss: 1.0255\n",
      "Epoch [18/30], Step [1300/3535], Loss: 0.1993\n",
      "Epoch [18/30], Step [1320/3535], Loss: 0.5999\n",
      "Epoch [18/30], Step [1340/3535], Loss: 0.3545\n",
      "Epoch [18/30], Step [1360/3535], Loss: 0.5679\n",
      "Epoch [18/30], Step [1380/3535], Loss: 0.5159\n",
      "Epoch [18/30], Step [1400/3535], Loss: 0.2913\n",
      "Epoch [18/30], Step [1420/3535], Loss: 0.3339\n",
      "Epoch [18/30], Step [1440/3535], Loss: 0.8097\n",
      "Epoch [18/30], Step [1460/3535], Loss: 0.4768\n",
      "Epoch [18/30], Step [1480/3535], Loss: 0.5167\n",
      "Epoch [18/30], Step [1500/3535], Loss: 0.1696\n",
      "Epoch [18/30], Step [1520/3535], Loss: 0.3975\n",
      "Epoch [18/30], Step [1540/3535], Loss: 0.5154\n",
      "Epoch [18/30], Step [1560/3535], Loss: 0.5782\n",
      "Epoch [18/30], Step [1580/3535], Loss: 0.6103\n",
      "Epoch [18/30], Step [1600/3535], Loss: 0.1976\n",
      "Epoch [18/30], Step [1620/3535], Loss: 0.4262\n",
      "Epoch [18/30], Step [1640/3535], Loss: 0.7666\n",
      "Epoch [18/30], Step [1660/3535], Loss: 0.1970\n",
      "Epoch [18/30], Step [1680/3535], Loss: 0.9666\n",
      "Epoch [18/30], Step [1700/3535], Loss: 0.1489\n",
      "Epoch [18/30], Step [1720/3535], Loss: 0.3552\n",
      "Epoch [18/30], Step [1740/3535], Loss: 0.5956\n",
      "Epoch [18/30], Step [1760/3535], Loss: 0.2292\n",
      "Epoch [18/30], Step [1780/3535], Loss: 0.0623\n",
      "Epoch [18/30], Step [1800/3535], Loss: 0.2036\n",
      "Epoch [18/30], Step [1820/3535], Loss: 0.1286\n",
      "Epoch [18/30], Step [1840/3535], Loss: 0.5313\n",
      "Epoch [18/30], Step [1860/3535], Loss: 0.3117\n",
      "Epoch [18/30], Step [1880/3535], Loss: 0.4864\n",
      "Epoch [18/30], Step [1900/3535], Loss: 0.8507\n",
      "Epoch [18/30], Step [1920/3535], Loss: 0.1497\n",
      "Epoch [18/30], Step [1940/3535], Loss: 0.1551\n",
      "Epoch [18/30], Step [1960/3535], Loss: 0.6291\n",
      "Epoch [18/30], Step [1980/3535], Loss: 0.3461\n",
      "Epoch [18/30], Step [2000/3535], Loss: 0.6372\n",
      "Epoch [18/30], Step [2020/3535], Loss: 0.5410\n",
      "Epoch [18/30], Step [2040/3535], Loss: 0.0261\n",
      "Epoch [18/30], Step [2060/3535], Loss: 0.2296\n",
      "Epoch [18/30], Step [2080/3535], Loss: 0.2198\n",
      "Epoch [18/30], Step [2100/3535], Loss: 0.4184\n",
      "Epoch [18/30], Step [2120/3535], Loss: 0.9222\n",
      "Epoch [18/30], Step [2140/3535], Loss: 0.3486\n",
      "Epoch [18/30], Step [2160/3535], Loss: 0.3356\n",
      "Epoch [18/30], Step [2180/3535], Loss: 0.2997\n",
      "Epoch [18/30], Step [2200/3535], Loss: 0.0971\n",
      "Epoch [18/30], Step [2220/3535], Loss: 0.5814\n",
      "Epoch [18/30], Step [2240/3535], Loss: 0.1910\n",
      "Epoch [18/30], Step [2260/3535], Loss: 0.1427\n",
      "Epoch [18/30], Step [2280/3535], Loss: 0.1078\n",
      "Epoch [18/30], Step [2300/3535], Loss: 0.1331\n",
      "Epoch [18/30], Step [2320/3535], Loss: 0.3933\n",
      "Epoch [18/30], Step [2340/3535], Loss: 0.3162\n",
      "Epoch [18/30], Step [2360/3535], Loss: 0.3776\n",
      "Epoch [18/30], Step [2380/3535], Loss: 0.5612\n",
      "Epoch [18/30], Step [2400/3535], Loss: 0.3435\n",
      "Epoch [18/30], Step [2420/3535], Loss: 0.2007\n",
      "Epoch [18/30], Step [2440/3535], Loss: 0.2662\n",
      "Epoch [18/30], Step [2460/3535], Loss: 0.1500\n",
      "Epoch [18/30], Step [2480/3535], Loss: 0.7617\n",
      "Epoch [18/30], Step [2500/3535], Loss: 0.1986\n",
      "Epoch [18/30], Step [2520/3535], Loss: 0.6349\n",
      "Epoch [18/30], Step [2540/3535], Loss: 0.3045\n",
      "Epoch [18/30], Step [2560/3535], Loss: 0.1135\n",
      "Epoch [18/30], Step [2580/3535], Loss: 0.2891\n",
      "Epoch [18/30], Step [2600/3535], Loss: 0.5396\n",
      "Epoch [18/30], Step [2620/3535], Loss: 0.6611\n",
      "Epoch [18/30], Step [2640/3535], Loss: 0.2793\n",
      "Epoch [18/30], Step [2660/3535], Loss: 0.4406\n",
      "Epoch [18/30], Step [2680/3535], Loss: 0.6082\n",
      "Epoch [18/30], Step [2700/3535], Loss: 0.2989\n",
      "Epoch [18/30], Step [2720/3535], Loss: 0.3907\n",
      "Epoch [18/30], Step [2740/3535], Loss: 0.2687\n",
      "Epoch [18/30], Step [2760/3535], Loss: 0.3429\n",
      "Epoch [18/30], Step [2780/3535], Loss: 0.5600\n",
      "Epoch [18/30], Step [2800/3535], Loss: 0.2962\n",
      "Epoch [18/30], Step [2820/3535], Loss: 0.3097\n",
      "Epoch [18/30], Step [2840/3535], Loss: 0.3131\n",
      "Epoch [18/30], Step [2860/3535], Loss: 0.1065\n",
      "Epoch [18/30], Step [2880/3535], Loss: 0.1338\n",
      "Epoch [18/30], Step [2900/3535], Loss: 0.6776\n",
      "Epoch [18/30], Step [2920/3535], Loss: 0.6090\n",
      "Epoch [18/30], Step [2940/3535], Loss: 0.1800\n",
      "Epoch [18/30], Step [2960/3535], Loss: 0.3806\n",
      "Epoch [18/30], Step [2980/3535], Loss: 0.1165\n",
      "Epoch [18/30], Step [3000/3535], Loss: 0.4984\n",
      "Epoch [18/30], Step [3020/3535], Loss: 0.2360\n",
      "Epoch [18/30], Step [3040/3535], Loss: 0.4450\n",
      "Epoch [18/30], Step [3060/3535], Loss: 0.2731\n",
      "Epoch [18/30], Step [3080/3535], Loss: 0.4335\n",
      "Epoch [18/30], Step [3100/3535], Loss: 0.4100\n",
      "Epoch [18/30], Step [3120/3535], Loss: 0.3738\n",
      "Epoch [18/30], Step [3140/3535], Loss: 0.1696\n",
      "Epoch [18/30], Step [3160/3535], Loss: 0.1239\n",
      "Epoch [18/30], Step [3180/3535], Loss: 0.1122\n",
      "Epoch [18/30], Step [3200/3535], Loss: 0.4554\n",
      "Epoch [18/30], Step [3220/3535], Loss: 0.3074\n",
      "Epoch [18/30], Step [3240/3535], Loss: 0.4991\n",
      "Epoch [18/30], Step [3260/3535], Loss: 0.7553\n",
      "Epoch [18/30], Step [3280/3535], Loss: 0.7650\n",
      "Epoch [18/30], Step [3300/3535], Loss: 0.5775\n",
      "Epoch [18/30], Step [3320/3535], Loss: 0.1639\n",
      "Epoch [18/30], Step [3340/3535], Loss: 0.3597\n",
      "Epoch [18/30], Step [3360/3535], Loss: 0.2533\n",
      "Epoch [18/30], Step [3380/3535], Loss: 0.1891\n",
      "Epoch [18/30], Step [3400/3535], Loss: 0.7192\n",
      "Epoch [18/30], Step [3420/3535], Loss: 0.6537\n",
      "Epoch [18/30], Step [3440/3535], Loss: 0.4804\n",
      "Epoch [18/30], Step [3460/3535], Loss: 0.1340\n",
      "Epoch [18/30], Step [3480/3535], Loss: 0.2944\n",
      "Epoch [18/30], Step [3500/3535], Loss: 0.6944\n",
      "Epoch [18/30], Step [3520/3535], Loss: 0.2128\n",
      "\n",
      "train-loss: 0.9539, train-acc: 87.3873\n",
      "validation loss: 1.1897, validation acc: 60.3509\n",
      "\n",
      "Epoch 19\n",
      "\n",
      "Epoch [19/30], Step [0/3535], Loss: 0.2443\n",
      "Epoch [19/30], Step [20/3535], Loss: 0.2369\n",
      "Epoch [19/30], Step [40/3535], Loss: 0.1385\n",
      "Epoch [19/30], Step [60/3535], Loss: 0.3388\n",
      "Epoch [19/30], Step [80/3535], Loss: 0.3167\n",
      "Epoch [19/30], Step [100/3535], Loss: 0.2892\n",
      "Epoch [19/30], Step [120/3535], Loss: 0.3086\n",
      "Epoch [19/30], Step [140/3535], Loss: 0.2017\n",
      "Epoch [19/30], Step [160/3535], Loss: 0.0916\n",
      "Epoch [19/30], Step [180/3535], Loss: 0.1772\n",
      "Epoch [19/30], Step [200/3535], Loss: 0.0893\n",
      "Epoch [19/30], Step [220/3535], Loss: 0.0744\n",
      "Epoch [19/30], Step [240/3535], Loss: 0.1173\n",
      "Epoch [19/30], Step [260/3535], Loss: 0.1591\n",
      "Epoch [19/30], Step [280/3535], Loss: 0.0801\n",
      "Epoch [19/30], Step [300/3535], Loss: 0.4518\n",
      "Epoch [19/30], Step [320/3535], Loss: 0.0762\n",
      "Epoch [19/30], Step [340/3535], Loss: 0.1173\n",
      "Epoch [19/30], Step [360/3535], Loss: 0.0488\n",
      "Epoch [19/30], Step [380/3535], Loss: 1.5746\n",
      "Epoch [19/30], Step [400/3535], Loss: 0.5992\n",
      "Epoch [19/30], Step [420/3535], Loss: 0.7184\n",
      "Epoch [19/30], Step [440/3535], Loss: 0.1693\n",
      "Epoch [19/30], Step [460/3535], Loss: 0.0376\n",
      "Epoch [19/30], Step [480/3535], Loss: 0.0259\n",
      "Epoch [19/30], Step [500/3535], Loss: 0.3783\n",
      "Epoch [19/30], Step [520/3535], Loss: 0.1700\n",
      "Epoch [19/30], Step [540/3535], Loss: 0.2990\n",
      "Epoch [19/30], Step [560/3535], Loss: 0.4038\n",
      "Epoch [19/30], Step [580/3535], Loss: 0.0322\n",
      "Epoch [19/30], Step [600/3535], Loss: 0.0357\n",
      "Epoch [19/30], Step [620/3535], Loss: 0.2289\n",
      "Epoch [19/30], Step [640/3535], Loss: 0.2216\n",
      "Epoch [19/30], Step [660/3535], Loss: 0.1098\n",
      "Epoch [19/30], Step [680/3535], Loss: 0.0998\n",
      "Epoch [19/30], Step [700/3535], Loss: 0.3033\n",
      "Epoch [19/30], Step [720/3535], Loss: 0.0184\n",
      "Epoch [19/30], Step [740/3535], Loss: 0.1846\n",
      "Epoch [19/30], Step [760/3535], Loss: 0.5422\n",
      "Epoch [19/30], Step [780/3535], Loss: 0.3323\n",
      "Epoch [19/30], Step [800/3535], Loss: 0.3421\n",
      "Epoch [19/30], Step [820/3535], Loss: 0.3115\n",
      "Epoch [19/30], Step [840/3535], Loss: 0.5291\n",
      "Epoch [19/30], Step [860/3535], Loss: 0.3388\n",
      "Epoch [19/30], Step [880/3535], Loss: 0.7430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30], Step [900/3535], Loss: 0.2172\n",
      "Epoch [19/30], Step [920/3535], Loss: 0.2335\n",
      "Epoch [19/30], Step [940/3535], Loss: 0.4814\n",
      "Epoch [19/30], Step [960/3535], Loss: 0.0603\n",
      "Epoch [19/30], Step [980/3535], Loss: 0.1726\n",
      "Epoch [19/30], Step [1000/3535], Loss: 0.0469\n",
      "Epoch [19/30], Step [1020/3535], Loss: 0.3126\n",
      "Epoch [19/30], Step [1040/3535], Loss: 0.1733\n",
      "Epoch [19/30], Step [1060/3535], Loss: 0.6196\n",
      "Epoch [19/30], Step [1080/3535], Loss: 0.2254\n",
      "Epoch [19/30], Step [1100/3535], Loss: 0.2801\n",
      "Epoch [19/30], Step [1120/3535], Loss: 0.1344\n",
      "Epoch [19/30], Step [1140/3535], Loss: 0.4198\n",
      "Epoch [19/30], Step [1160/3535], Loss: 0.1106\n",
      "Epoch [19/30], Step [1180/3535], Loss: 0.3661\n",
      "Epoch [19/30], Step [1200/3535], Loss: 0.1667\n",
      "Epoch [19/30], Step [1220/3535], Loss: 0.2012\n",
      "Epoch [19/30], Step [1240/3535], Loss: 0.0530\n",
      "Epoch [19/30], Step [1260/3535], Loss: 0.0350\n",
      "Epoch [19/30], Step [1280/3535], Loss: 0.1089\n",
      "Epoch [19/30], Step [1300/3535], Loss: 0.1284\n",
      "Epoch [19/30], Step [1320/3535], Loss: 0.0285\n",
      "Epoch [19/30], Step [1340/3535], Loss: 0.0697\n",
      "Epoch [19/30], Step [1360/3535], Loss: 0.2326\n",
      "Epoch [19/30], Step [1380/3535], Loss: 0.3547\n",
      "Epoch [19/30], Step [1400/3535], Loss: 0.3783\n",
      "Epoch [19/30], Step [1420/3535], Loss: 0.5443\n",
      "Epoch [19/30], Step [1440/3535], Loss: 0.0081\n",
      "Epoch [19/30], Step [1460/3535], Loss: 0.0323\n",
      "Epoch [19/30], Step [1480/3535], Loss: 0.2067\n",
      "Epoch [19/30], Step [1500/3535], Loss: 0.8411\n",
      "Epoch [19/30], Step [1520/3535], Loss: 0.0698\n",
      "Epoch [19/30], Step [1540/3535], Loss: 0.3266\n",
      "Epoch [19/30], Step [1560/3535], Loss: 0.2176\n",
      "Epoch [19/30], Step [1580/3535], Loss: 0.5060\n",
      "Epoch [19/30], Step [1600/3535], Loss: 0.1685\n",
      "Epoch [19/30], Step [1620/3535], Loss: 0.1641\n",
      "Epoch [19/30], Step [1640/3535], Loss: 0.1172\n",
      "Epoch [19/30], Step [1660/3535], Loss: 0.4632\n",
      "Epoch [19/30], Step [1680/3535], Loss: 0.1338\n",
      "Epoch [19/30], Step [1700/3535], Loss: 0.2109\n",
      "Epoch [19/30], Step [1720/3535], Loss: 0.6290\n",
      "Epoch [19/30], Step [1740/3535], Loss: 0.3936\n",
      "Epoch [19/30], Step [1760/3535], Loss: 0.1108\n",
      "Epoch [19/30], Step [1780/3535], Loss: 0.2263\n",
      "Epoch [19/30], Step [1800/3535], Loss: 0.4336\n",
      "Epoch [19/30], Step [1820/3535], Loss: 0.1146\n",
      "Epoch [19/30], Step [1840/3535], Loss: 0.2322\n",
      "Epoch [19/30], Step [1860/3535], Loss: 0.0494\n",
      "Epoch [19/30], Step [1880/3535], Loss: 0.1786\n",
      "Epoch [19/30], Step [1900/3535], Loss: 0.3342\n",
      "Epoch [19/30], Step [1920/3535], Loss: 0.9779\n",
      "Epoch [19/30], Step [1940/3535], Loss: 0.7523\n",
      "Epoch [19/30], Step [1960/3535], Loss: 0.5057\n",
      "Epoch [19/30], Step [1980/3535], Loss: 0.0619\n",
      "Epoch [19/30], Step [2000/3535], Loss: 0.2236\n",
      "Epoch [19/30], Step [2020/3535], Loss: 0.0777\n",
      "Epoch [19/30], Step [2040/3535], Loss: 0.0717\n",
      "Epoch [19/30], Step [2060/3535], Loss: 0.1761\n",
      "Epoch [19/30], Step [2080/3535], Loss: 0.3519\n",
      "Epoch [19/30], Step [2100/3535], Loss: 0.2817\n",
      "Epoch [19/30], Step [2120/3535], Loss: 0.5954\n",
      "Epoch [19/30], Step [2140/3535], Loss: 0.1157\n",
      "Epoch [19/30], Step [2160/3535], Loss: 0.0723\n",
      "Epoch [19/30], Step [2180/3535], Loss: 0.1801\n",
      "Epoch [19/30], Step [2200/3535], Loss: 0.2569\n",
      "Epoch [19/30], Step [2220/3535], Loss: 0.2420\n",
      "Epoch [19/30], Step [2240/3535], Loss: 0.2285\n",
      "Epoch [19/30], Step [2260/3535], Loss: 0.7082\n",
      "Epoch [19/30], Step [2280/3535], Loss: 0.1526\n",
      "Epoch [19/30], Step [2300/3535], Loss: 0.2036\n",
      "Epoch [19/30], Step [2320/3535], Loss: 0.1723\n",
      "Epoch [19/30], Step [2340/3535], Loss: 0.2297\n",
      "Epoch [19/30], Step [2360/3535], Loss: 0.0384\n",
      "Epoch [19/30], Step [2380/3535], Loss: 0.7624\n",
      "Epoch [19/30], Step [2400/3535], Loss: 0.5840\n",
      "Epoch [19/30], Step [2420/3535], Loss: 0.6824\n",
      "Epoch [19/30], Step [2440/3535], Loss: 0.7537\n",
      "Epoch [19/30], Step [2460/3535], Loss: 0.1677\n",
      "Epoch [19/30], Step [2480/3535], Loss: 0.0669\n",
      "Epoch [19/30], Step [2500/3535], Loss: 0.3088\n",
      "Epoch [19/30], Step [2520/3535], Loss: 0.1089\n",
      "Epoch [19/30], Step [2540/3535], Loss: 0.1419\n",
      "Epoch [19/30], Step [2560/3535], Loss: 0.2709\n",
      "Epoch [19/30], Step [2580/3535], Loss: 0.0925\n",
      "Epoch [19/30], Step [2600/3535], Loss: 0.1648\n",
      "Epoch [19/30], Step [2620/3535], Loss: 0.6245\n",
      "Epoch [19/30], Step [2640/3535], Loss: 0.4008\n",
      "Epoch [19/30], Step [2660/3535], Loss: 0.2760\n",
      "Epoch [19/30], Step [2680/3535], Loss: 0.5220\n",
      "Epoch [19/30], Step [2700/3535], Loss: 0.0615\n",
      "Epoch [19/30], Step [2720/3535], Loss: 0.3281\n",
      "Epoch [19/30], Step [2740/3535], Loss: 1.0044\n",
      "Epoch [19/30], Step [2760/3535], Loss: 0.1508\n",
      "Epoch [19/30], Step [2780/3535], Loss: 0.8203\n",
      "Epoch [19/30], Step [2800/3535], Loss: 0.0817\n",
      "Epoch [19/30], Step [2820/3535], Loss: 0.1845\n",
      "Epoch [19/30], Step [2840/3535], Loss: 0.0806\n",
      "Epoch [19/30], Step [2860/3535], Loss: 0.2796\n",
      "Epoch [19/30], Step [2880/3535], Loss: 0.3176\n",
      "Epoch [19/30], Step [2900/3535], Loss: 0.3221\n",
      "Epoch [19/30], Step [2920/3535], Loss: 0.3573\n",
      "Epoch [19/30], Step [2940/3535], Loss: 0.0848\n",
      "Epoch [19/30], Step [2960/3535], Loss: 0.8127\n",
      "Epoch [19/30], Step [2980/3535], Loss: 0.2276\n",
      "Epoch [19/30], Step [3000/3535], Loss: 0.1736\n",
      "Epoch [19/30], Step [3020/3535], Loss: 0.5976\n",
      "Epoch [19/30], Step [3040/3535], Loss: 0.1605\n",
      "Epoch [19/30], Step [3060/3535], Loss: 0.1867\n",
      "Epoch [19/30], Step [3080/3535], Loss: 0.0420\n",
      "Epoch [19/30], Step [3100/3535], Loss: 0.6029\n",
      "Epoch [19/30], Step [3120/3535], Loss: 0.0340\n",
      "Epoch [19/30], Step [3140/3535], Loss: 0.0633\n",
      "Epoch [19/30], Step [3160/3535], Loss: 0.0542\n",
      "Epoch [19/30], Step [3180/3535], Loss: 0.1487\n",
      "Epoch [19/30], Step [3200/3535], Loss: 0.5857\n",
      "Epoch [19/30], Step [3220/3535], Loss: 0.2595\n",
      "Epoch [19/30], Step [3240/3535], Loss: 0.2541\n",
      "Epoch [19/30], Step [3260/3535], Loss: 0.3770\n",
      "Epoch [19/30], Step [3280/3535], Loss: 0.2204\n",
      "Epoch [19/30], Step [3300/3535], Loss: 0.0877\n",
      "Epoch [19/30], Step [3320/3535], Loss: 0.6276\n",
      "Epoch [19/30], Step [3340/3535], Loss: 0.1358\n",
      "Epoch [19/30], Step [3360/3535], Loss: 0.1453\n",
      "Epoch [19/30], Step [3380/3535], Loss: 0.3030\n",
      "Epoch [19/30], Step [3400/3535], Loss: 0.3000\n",
      "Epoch [19/30], Step [3420/3535], Loss: 0.2582\n",
      "Epoch [19/30], Step [3440/3535], Loss: 0.1398\n",
      "Epoch [19/30], Step [3460/3535], Loss: 0.1816\n",
      "Epoch [19/30], Step [3480/3535], Loss: 0.4135\n",
      "Epoch [19/30], Step [3500/3535], Loss: 0.4222\n",
      "Epoch [19/30], Step [3520/3535], Loss: 0.6267\n",
      "\n",
      "train-loss: 0.9185, train-acc: 90.2982\n",
      "validation loss: 1.2033, validation acc: 60.7754\n",
      "\n",
      "Epoch 20\n",
      "\n",
      "Epoch [20/30], Step [0/3535], Loss: 0.1420\n",
      "Epoch [20/30], Step [20/3535], Loss: 0.0655\n",
      "Epoch [20/30], Step [40/3535], Loss: 0.1296\n",
      "Epoch [20/30], Step [60/3535], Loss: 0.1755\n",
      "Epoch [20/30], Step [80/3535], Loss: 0.0547\n",
      "Epoch [20/30], Step [100/3535], Loss: 0.0225\n",
      "Epoch [20/30], Step [120/3535], Loss: 0.2718\n",
      "Epoch [20/30], Step [140/3535], Loss: 0.0914\n",
      "Epoch [20/30], Step [160/3535], Loss: 0.1859\n",
      "Epoch [20/30], Step [180/3535], Loss: 0.0790\n",
      "Epoch [20/30], Step [200/3535], Loss: 0.7451\n",
      "Epoch [20/30], Step [220/3535], Loss: 0.4504\n",
      "Epoch [20/30], Step [240/3535], Loss: 0.1272\n",
      "Epoch [20/30], Step [260/3535], Loss: 0.0368\n",
      "Epoch [20/30], Step [280/3535], Loss: 0.0595\n",
      "Epoch [20/30], Step [300/3535], Loss: 0.1000\n",
      "Epoch [20/30], Step [320/3535], Loss: 0.0252\n",
      "Epoch [20/30], Step [340/3535], Loss: 0.8775\n",
      "Epoch [20/30], Step [360/3535], Loss: 0.1358\n",
      "Epoch [20/30], Step [380/3535], Loss: 0.0697\n",
      "Epoch [20/30], Step [400/3535], Loss: 0.2539\n",
      "Epoch [20/30], Step [420/3535], Loss: 0.1135\n",
      "Epoch [20/30], Step [440/3535], Loss: 0.1355\n",
      "Epoch [20/30], Step [460/3535], Loss: 0.1249\n",
      "Epoch [20/30], Step [480/3535], Loss: 0.6353\n",
      "Epoch [20/30], Step [500/3535], Loss: 0.0283\n",
      "Epoch [20/30], Step [520/3535], Loss: 0.1430\n",
      "Epoch [20/30], Step [540/3535], Loss: 0.1492\n",
      "Epoch [20/30], Step [560/3535], Loss: 0.1194\n",
      "Epoch [20/30], Step [580/3535], Loss: 0.0249\n",
      "Epoch [20/30], Step [600/3535], Loss: 0.4356\n",
      "Epoch [20/30], Step [620/3535], Loss: 0.1039\n",
      "Epoch [20/30], Step [640/3535], Loss: 0.2664\n",
      "Epoch [20/30], Step [660/3535], Loss: 0.0302\n",
      "Epoch [20/30], Step [680/3535], Loss: 0.6394\n",
      "Epoch [20/30], Step [700/3535], Loss: 0.0527\n",
      "Epoch [20/30], Step [720/3535], Loss: 0.0286\n",
      "Epoch [20/30], Step [740/3535], Loss: 0.0247\n",
      "Epoch [20/30], Step [760/3535], Loss: 0.2900\n",
      "Epoch [20/30], Step [780/3535], Loss: 0.1509\n",
      "Epoch [20/30], Step [800/3535], Loss: 0.1018\n",
      "Epoch [20/30], Step [820/3535], Loss: 0.0597\n",
      "Epoch [20/30], Step [840/3535], Loss: 0.0630\n",
      "Epoch [20/30], Step [860/3535], Loss: 0.1301\n",
      "Epoch [20/30], Step [880/3535], Loss: 0.0722\n",
      "Epoch [20/30], Step [900/3535], Loss: 0.2228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Step [920/3535], Loss: 0.1276\n",
      "Epoch [20/30], Step [940/3535], Loss: 0.0378\n",
      "Epoch [20/30], Step [960/3535], Loss: 0.1694\n",
      "Epoch [20/30], Step [980/3535], Loss: 0.1425\n",
      "Epoch [20/30], Step [1000/3535], Loss: 0.0781\n",
      "Epoch [20/30], Step [1020/3535], Loss: 0.0505\n",
      "Epoch [20/30], Step [1040/3535], Loss: 0.1580\n",
      "Epoch [20/30], Step [1060/3535], Loss: 0.2460\n",
      "Epoch [20/30], Step [1080/3535], Loss: 0.1309\n",
      "Epoch [20/30], Step [1100/3535], Loss: 0.1145\n",
      "Epoch [20/30], Step [1120/3535], Loss: 0.1730\n",
      "Epoch [20/30], Step [1140/3535], Loss: 0.0038\n",
      "Epoch [20/30], Step [1160/3535], Loss: 0.1070\n",
      "Epoch [20/30], Step [1180/3535], Loss: 0.0257\n",
      "Epoch [20/30], Step [1200/3535], Loss: 0.1549\n",
      "Epoch [20/30], Step [1220/3535], Loss: 0.3477\n",
      "Epoch [20/30], Step [1240/3535], Loss: 0.0890\n",
      "Epoch [20/30], Step [1260/3535], Loss: 0.2152\n",
      "Epoch [20/30], Step [1280/3535], Loss: 0.0301\n",
      "Epoch [20/30], Step [1300/3535], Loss: 0.0974\n",
      "Epoch [20/30], Step [1320/3535], Loss: 0.1501\n",
      "Epoch [20/30], Step [1340/3535], Loss: 0.0948\n",
      "Epoch [20/30], Step [1360/3535], Loss: 0.0363\n",
      "Epoch [20/30], Step [1380/3535], Loss: 0.1356\n",
      "Epoch [20/30], Step [1400/3535], Loss: 0.1628\n",
      "Epoch [20/30], Step [1420/3535], Loss: 0.1460\n",
      "Epoch [20/30], Step [1440/3535], Loss: 0.1438\n",
      "Epoch [20/30], Step [1460/3535], Loss: 0.0756\n",
      "Epoch [20/30], Step [1480/3535], Loss: 0.1576\n",
      "Epoch [20/30], Step [1500/3535], Loss: 0.2231\n",
      "Epoch [20/30], Step [1520/3535], Loss: 0.0470\n",
      "Epoch [20/30], Step [1540/3535], Loss: 0.1596\n",
      "Epoch [20/30], Step [1560/3535], Loss: 0.0557\n",
      "Epoch [20/30], Step [1580/3535], Loss: 0.0832\n",
      "Epoch [20/30], Step [1600/3535], Loss: 0.1766\n",
      "Epoch [20/30], Step [1620/3535], Loss: 0.1649\n",
      "Epoch [20/30], Step [1640/3535], Loss: 0.0426\n",
      "Epoch [20/30], Step [1660/3535], Loss: 0.0598\n",
      "Epoch [20/30], Step [1680/3535], Loss: 0.4468\n",
      "Epoch [20/30], Step [1700/3535], Loss: 0.3300\n",
      "Epoch [20/30], Step [1720/3535], Loss: 0.6806\n",
      "Epoch [20/30], Step [1740/3535], Loss: 0.4925\n",
      "Epoch [20/30], Step [1760/3535], Loss: 0.1649\n",
      "Epoch [20/30], Step [1780/3535], Loss: 0.3365\n",
      "Epoch [20/30], Step [1800/3535], Loss: 0.0621\n",
      "Epoch [20/30], Step [1820/3535], Loss: 0.6400\n",
      "Epoch [20/30], Step [1840/3535], Loss: 0.3618\n",
      "Epoch [20/30], Step [1860/3535], Loss: 0.1524\n",
      "Epoch [20/30], Step [1880/3535], Loss: 0.4122\n",
      "Epoch [20/30], Step [1900/3535], Loss: 0.7479\n",
      "Epoch [20/30], Step [1920/3535], Loss: 0.0743\n",
      "Epoch [20/30], Step [1940/3535], Loss: 0.1431\n",
      "Epoch [20/30], Step [1960/3535], Loss: 0.0491\n",
      "Epoch [20/30], Step [1980/3535], Loss: 0.2437\n",
      "Epoch [20/30], Step [2000/3535], Loss: 0.7121\n",
      "Epoch [20/30], Step [2020/3535], Loss: 0.1979\n",
      "Epoch [20/30], Step [2040/3535], Loss: 0.3945\n",
      "Epoch [20/30], Step [2060/3535], Loss: 0.0322\n",
      "Epoch [20/30], Step [2080/3535], Loss: 0.0542\n",
      "Epoch [20/30], Step [2100/3535], Loss: 0.1104\n",
      "Epoch [20/30], Step [2120/3535], Loss: 0.6283\n",
      "Epoch [20/30], Step [2140/3535], Loss: 0.1615\n",
      "Epoch [20/30], Step [2160/3535], Loss: 0.1806\n",
      "Epoch [20/30], Step [2180/3535], Loss: 0.1561\n",
      "Epoch [20/30], Step [2200/3535], Loss: 0.4895\n",
      "Epoch [20/30], Step [2220/3535], Loss: 0.1771\n",
      "Epoch [20/30], Step [2240/3535], Loss: 0.3816\n",
      "Epoch [20/30], Step [2260/3535], Loss: 1.5274\n",
      "Epoch [20/30], Step [2280/3535], Loss: 0.0251\n",
      "Epoch [20/30], Step [2300/3535], Loss: 0.0336\n",
      "Epoch [20/30], Step [2320/3535], Loss: 0.3051\n",
      "Epoch [20/30], Step [2340/3535], Loss: 0.2662\n",
      "Epoch [20/30], Step [2360/3535], Loss: 0.1120\n",
      "Epoch [20/30], Step [2380/3535], Loss: 0.2303\n",
      "Epoch [20/30], Step [2400/3535], Loss: 0.5143\n",
      "Epoch [20/30], Step [2420/3535], Loss: 0.0562\n",
      "Epoch [20/30], Step [2440/3535], Loss: 0.1101\n",
      "Epoch [20/30], Step [2460/3535], Loss: 0.0805\n",
      "Epoch [20/30], Step [2480/3535], Loss: 0.1135\n",
      "Epoch [20/30], Step [2500/3535], Loss: 0.2405\n",
      "Epoch [20/30], Step [2520/3535], Loss: 0.0295\n",
      "Epoch [20/30], Step [2540/3535], Loss: 0.1137\n",
      "Epoch [20/30], Step [2560/3535], Loss: 0.0273\n",
      "Epoch [20/30], Step [2580/3535], Loss: 0.0665\n",
      "Epoch [20/30], Step [2600/3535], Loss: 0.0269\n",
      "Epoch [20/30], Step [2620/3535], Loss: 0.2069\n",
      "Epoch [20/30], Step [2640/3535], Loss: 0.0558\n",
      "Epoch [20/30], Step [2660/3535], Loss: 0.2521\n",
      "Epoch [20/30], Step [2680/3535], Loss: 0.2159\n",
      "Epoch [20/30], Step [2700/3535], Loss: 0.1499\n",
      "Epoch [20/30], Step [2720/3535], Loss: 0.3236\n",
      "Epoch [20/30], Step [2740/3535], Loss: 0.3491\n",
      "Epoch [20/30], Step [2760/3535], Loss: 0.0756\n",
      "Epoch [20/30], Step [2780/3535], Loss: 0.2635\n",
      "Epoch [20/30], Step [2800/3535], Loss: 0.0546\n",
      "Epoch [20/30], Step [2820/3535], Loss: 0.5656\n",
      "Epoch [20/30], Step [2840/3535], Loss: 0.2175\n",
      "Epoch [20/30], Step [2860/3535], Loss: 0.1074\n",
      "Epoch [20/30], Step [2880/3535], Loss: 0.5445\n",
      "Epoch [20/30], Step [2900/3535], Loss: 0.0731\n",
      "Epoch [20/30], Step [2920/3535], Loss: 0.2302\n",
      "Epoch [20/30], Step [2940/3535], Loss: 0.1112\n",
      "Epoch [20/30], Step [2960/3535], Loss: 0.0335\n",
      "Epoch [20/30], Step [2980/3535], Loss: 0.0603\n",
      "Epoch [20/30], Step [3000/3535], Loss: 0.3055\n",
      "Epoch [20/30], Step [3020/3535], Loss: 0.6935\n",
      "Epoch [20/30], Step [3040/3535], Loss: 0.2157\n",
      "Epoch [20/30], Step [3060/3535], Loss: 0.1354\n",
      "Epoch [20/30], Step [3080/3535], Loss: 0.0590\n",
      "Epoch [20/30], Step [3100/3535], Loss: 0.1504\n",
      "Epoch [20/30], Step [3120/3535], Loss: 0.1503\n",
      "Epoch [20/30], Step [3140/3535], Loss: 0.5609\n",
      "Epoch [20/30], Step [3160/3535], Loss: 0.1706\n",
      "Epoch [20/30], Step [3180/3535], Loss: 0.7152\n",
      "Epoch [20/30], Step [3200/3535], Loss: 1.1143\n",
      "Epoch [20/30], Step [3220/3535], Loss: 0.2207\n",
      "Epoch [20/30], Step [3240/3535], Loss: 0.4205\n",
      "Epoch [20/30], Step [3260/3535], Loss: 0.3788\n",
      "Epoch [20/30], Step [3280/3535], Loss: 0.1351\n",
      "Epoch [20/30], Step [3300/3535], Loss: 0.2776\n",
      "Epoch [20/30], Step [3320/3535], Loss: 0.2752\n",
      "Epoch [20/30], Step [3340/3535], Loss: 1.2278\n",
      "Epoch [20/30], Step [3360/3535], Loss: 0.2479\n",
      "Epoch [20/30], Step [3380/3535], Loss: 0.3627\n",
      "Epoch [20/30], Step [3400/3535], Loss: 0.2731\n",
      "Epoch [20/30], Step [3420/3535], Loss: 0.0665\n",
      "Epoch [20/30], Step [3440/3535], Loss: 0.1705\n",
      "Epoch [20/30], Step [3460/3535], Loss: 0.0839\n",
      "Epoch [20/30], Step [3480/3535], Loss: 0.8536\n",
      "Epoch [20/30], Step [3500/3535], Loss: 0.5612\n",
      "Epoch [20/30], Step [3520/3535], Loss: 0.0824\n",
      "\n",
      "train-loss: 0.8840, train-acc: 92.3673\n",
      "validation loss: 1.2200, validation acc: 58.4123\n",
      "\n",
      "Epoch 21\n",
      "\n",
      "Epoch [21/30], Step [0/3535], Loss: 0.0681\n",
      "Epoch [21/30], Step [20/3535], Loss: 0.3891\n",
      "Epoch [21/30], Step [40/3535], Loss: 0.0600\n",
      "Epoch [21/30], Step [60/3535], Loss: 0.4207\n",
      "Epoch [21/30], Step [80/3535], Loss: 0.1442\n",
      "Epoch [21/30], Step [100/3535], Loss: 0.0530\n",
      "Epoch [21/30], Step [120/3535], Loss: 0.0192\n",
      "Epoch [21/30], Step [140/3535], Loss: 0.4895\n",
      "Epoch [21/30], Step [160/3535], Loss: 0.1832\n",
      "Epoch [21/30], Step [180/3535], Loss: 0.1720\n",
      "Epoch [21/30], Step [200/3535], Loss: 0.3060\n",
      "Epoch [21/30], Step [220/3535], Loss: 0.0961\n",
      "Epoch [21/30], Step [240/3535], Loss: 0.1801\n",
      "Epoch [21/30], Step [260/3535], Loss: 0.0362\n",
      "Epoch [21/30], Step [280/3535], Loss: 0.0011\n",
      "Epoch [21/30], Step [300/3535], Loss: 0.1027\n",
      "Epoch [21/30], Step [320/3535], Loss: 0.0804\n",
      "Epoch [21/30], Step [340/3535], Loss: 0.2690\n",
      "Epoch [21/30], Step [360/3535], Loss: 0.1413\n",
      "Epoch [21/30], Step [380/3535], Loss: 0.1181\n",
      "Epoch [21/30], Step [400/3535], Loss: 0.3137\n",
      "Epoch [21/30], Step [420/3535], Loss: 0.2051\n",
      "Epoch [21/30], Step [440/3535], Loss: 0.0964\n",
      "Epoch [21/30], Step [460/3535], Loss: 0.4813\n",
      "Epoch [21/30], Step [480/3535], Loss: 0.1031\n",
      "Epoch [21/30], Step [500/3535], Loss: 0.1075\n",
      "Epoch [21/30], Step [520/3535], Loss: 0.2960\n",
      "Epoch [21/30], Step [540/3535], Loss: 0.0984\n",
      "Epoch [21/30], Step [560/3535], Loss: 0.3431\n",
      "Epoch [21/30], Step [580/3535], Loss: 0.0071\n",
      "Epoch [21/30], Step [600/3535], Loss: 0.1548\n",
      "Epoch [21/30], Step [620/3535], Loss: 0.0482\n",
      "Epoch [21/30], Step [640/3535], Loss: 1.2213\n",
      "Epoch [21/30], Step [660/3535], Loss: 0.0447\n",
      "Epoch [21/30], Step [680/3535], Loss: 0.0875\n",
      "Epoch [21/30], Step [700/3535], Loss: 0.0243\n",
      "Epoch [21/30], Step [720/3535], Loss: 0.0973\n",
      "Epoch [21/30], Step [740/3535], Loss: 0.2559\n",
      "Epoch [21/30], Step [760/3535], Loss: 0.1076\n",
      "Epoch [21/30], Step [780/3535], Loss: 0.1035\n",
      "Epoch [21/30], Step [800/3535], Loss: 0.4290\n",
      "Epoch [21/30], Step [820/3535], Loss: 0.3698\n",
      "Epoch [21/30], Step [840/3535], Loss: 0.0218\n",
      "Epoch [21/30], Step [860/3535], Loss: 0.4182\n",
      "Epoch [21/30], Step [880/3535], Loss: 0.0682\n",
      "Epoch [21/30], Step [900/3535], Loss: 0.3626\n",
      "Epoch [21/30], Step [920/3535], Loss: 0.2064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], Step [940/3535], Loss: 0.1036\n",
      "Epoch [21/30], Step [960/3535], Loss: 0.4626\n",
      "Epoch [21/30], Step [980/3535], Loss: 0.2956\n",
      "Epoch [21/30], Step [1000/3535], Loss: 0.0363\n",
      "Epoch [21/30], Step [1020/3535], Loss: 0.0566\n",
      "Epoch [21/30], Step [1040/3535], Loss: 0.1114\n",
      "Epoch [21/30], Step [1060/3535], Loss: 0.0520\n",
      "Epoch [21/30], Step [1080/3535], Loss: 0.2782\n",
      "Epoch [21/30], Step [1100/3535], Loss: 0.5628\n",
      "Epoch [21/30], Step [1120/3535], Loss: 0.2451\n",
      "Epoch [21/30], Step [1140/3535], Loss: 0.1792\n",
      "Epoch [21/30], Step [1160/3535], Loss: 0.2846\n",
      "Epoch [21/30], Step [1180/3535], Loss: 0.0202\n",
      "Epoch [21/30], Step [1200/3535], Loss: 0.0315\n",
      "Epoch [21/30], Step [1220/3535], Loss: 0.0874\n",
      "Epoch [21/30], Step [1240/3535], Loss: 0.1613\n",
      "Epoch [21/30], Step [1260/3535], Loss: 0.0258\n",
      "Epoch [21/30], Step [1280/3535], Loss: 0.0984\n",
      "Epoch [21/30], Step [1300/3535], Loss: 0.1096\n",
      "Epoch [21/30], Step [1320/3535], Loss: 0.1291\n",
      "Epoch [21/30], Step [1340/3535], Loss: 0.0464\n",
      "Epoch [21/30], Step [1360/3535], Loss: 0.0182\n",
      "Epoch [21/30], Step [1380/3535], Loss: 0.4554\n",
      "Epoch [21/30], Step [1400/3535], Loss: 0.1839\n",
      "Epoch [21/30], Step [1420/3535], Loss: 0.0301\n",
      "Epoch [21/30], Step [1440/3535], Loss: 0.0448\n",
      "Epoch [21/30], Step [1460/3535], Loss: 0.0744\n",
      "Epoch [21/30], Step [1480/3535], Loss: 0.4758\n",
      "Epoch [21/30], Step [1500/3535], Loss: 0.0958\n",
      "Epoch [21/30], Step [1520/3535], Loss: 0.4439\n",
      "Epoch [21/30], Step [1540/3535], Loss: 0.0736\n",
      "Epoch [21/30], Step [1560/3535], Loss: 0.0554\n",
      "Epoch [21/30], Step [1580/3535], Loss: 0.0554\n",
      "Epoch [21/30], Step [1600/3535], Loss: 0.3029\n",
      "Epoch [21/30], Step [1620/3535], Loss: 0.0925\n",
      "Epoch [21/30], Step [1640/3535], Loss: 0.0553\n",
      "Epoch [21/30], Step [1660/3535], Loss: 0.0322\n",
      "Epoch [21/30], Step [1680/3535], Loss: 0.0335\n",
      "Epoch [21/30], Step [1700/3535], Loss: 0.1216\n",
      "Epoch [21/30], Step [1720/3535], Loss: 0.0433\n",
      "Epoch [21/30], Step [1740/3535], Loss: 0.0915\n",
      "Epoch [21/30], Step [1760/3535], Loss: 0.5938\n",
      "Epoch [21/30], Step [1780/3535], Loss: 0.0616\n",
      "Epoch [21/30], Step [1800/3535], Loss: 0.2103\n",
      "Epoch [21/30], Step [1820/3535], Loss: 0.0254\n",
      "Epoch [21/30], Step [1840/3535], Loss: 0.0344\n",
      "Epoch [21/30], Step [1860/3535], Loss: 0.3585\n",
      "Epoch [21/30], Step [1880/3535], Loss: 0.0452\n",
      "Epoch [21/30], Step [1900/3535], Loss: 0.0964\n",
      "Epoch [21/30], Step [1920/3535], Loss: 0.0085\n",
      "Epoch [21/30], Step [1940/3535], Loss: 0.4312\n",
      "Epoch [21/30], Step [1960/3535], Loss: 0.0518\n",
      "Epoch [21/30], Step [1980/3535], Loss: 0.2152\n",
      "Epoch [21/30], Step [2000/3535], Loss: 0.1829\n",
      "Epoch [21/30], Step [2020/3535], Loss: 0.5895\n",
      "Epoch [21/30], Step [2040/3535], Loss: 0.2369\n",
      "Epoch [21/30], Step [2060/3535], Loss: 0.1674\n",
      "Epoch [21/30], Step [2080/3535], Loss: 0.1134\n",
      "Epoch [21/30], Step [2100/3535], Loss: 0.2917\n",
      "Epoch [21/30], Step [2120/3535], Loss: 0.0566\n",
      "Epoch [21/30], Step [2140/3535], Loss: 0.3879\n",
      "Epoch [21/30], Step [2160/3535], Loss: 0.0690\n",
      "Epoch [21/30], Step [2180/3535], Loss: 0.2129\n",
      "Epoch [21/30], Step [2200/3535], Loss: 0.1885\n",
      "Epoch [21/30], Step [2220/3535], Loss: 0.2751\n",
      "Epoch [21/30], Step [2240/3535], Loss: 0.1164\n",
      "Epoch [21/30], Step [2260/3535], Loss: 0.2115\n",
      "Epoch [21/30], Step [2280/3535], Loss: 0.1247\n",
      "Epoch [21/30], Step [2300/3535], Loss: 0.0400\n",
      "Epoch [21/30], Step [2320/3535], Loss: 0.6228\n",
      "Epoch [21/30], Step [2340/3535], Loss: 0.3502\n",
      "Epoch [21/30], Step [2360/3535], Loss: 0.0164\n",
      "Epoch [21/30], Step [2380/3535], Loss: 0.3216\n",
      "Epoch [21/30], Step [2400/3535], Loss: 0.1647\n",
      "Epoch [21/30], Step [2420/3535], Loss: 0.2713\n",
      "Epoch [21/30], Step [2440/3535], Loss: 0.0740\n",
      "Epoch [21/30], Step [2460/3535], Loss: 0.8905\n",
      "Epoch [21/30], Step [2480/3535], Loss: 0.3393\n",
      "Epoch [21/30], Step [2500/3535], Loss: 0.0672\n",
      "Epoch [21/30], Step [2520/3535], Loss: 0.0892\n",
      "Epoch [21/30], Step [2540/3535], Loss: 0.0258\n",
      "Epoch [21/30], Step [2560/3535], Loss: 0.3892\n",
      "Epoch [21/30], Step [2580/3535], Loss: 0.1301\n",
      "Epoch [21/30], Step [2600/3535], Loss: 0.1546\n",
      "Epoch [21/30], Step [2620/3535], Loss: 0.5562\n",
      "Epoch [21/30], Step [2640/3535], Loss: 0.0606\n",
      "Epoch [21/30], Step [2660/3535], Loss: 0.0869\n",
      "Epoch [21/30], Step [2680/3535], Loss: 0.0073\n",
      "Epoch [21/30], Step [2700/3535], Loss: 0.1240\n",
      "Epoch [21/30], Step [2720/3535], Loss: 0.2790\n",
      "Epoch [21/30], Step [2740/3535], Loss: 0.1704\n",
      "Epoch [21/30], Step [2760/3535], Loss: 0.0183\n",
      "Epoch [21/30], Step [2780/3535], Loss: 0.2753\n",
      "Epoch [21/30], Step [2800/3535], Loss: 0.4265\n",
      "Epoch [21/30], Step [2820/3535], Loss: 0.0650\n",
      "Epoch [21/30], Step [2840/3535], Loss: 0.6551\n",
      "Epoch [21/30], Step [2860/3535], Loss: 0.0172\n",
      "Epoch [21/30], Step [2880/3535], Loss: 1.3455\n",
      "Epoch [21/30], Step [2900/3535], Loss: 0.6065\n",
      "Epoch [21/30], Step [2920/3535], Loss: 0.2780\n",
      "Epoch [21/30], Step [2940/3535], Loss: 0.0699\n",
      "Epoch [21/30], Step [2960/3535], Loss: 0.1202\n",
      "Epoch [21/30], Step [2980/3535], Loss: 0.2942\n",
      "Epoch [21/30], Step [3000/3535], Loss: 0.2470\n",
      "Epoch [21/30], Step [3020/3535], Loss: 0.0841\n",
      "Epoch [21/30], Step [3040/3535], Loss: 0.0509\n",
      "Epoch [21/30], Step [3060/3535], Loss: 0.2040\n",
      "Epoch [21/30], Step [3080/3535], Loss: 0.0857\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "print_every = 10\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_dataloader)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    for batch_idx, (data_, target_) in enumerate(train_dataloader):\n",
    "        data_, target_ = data_.to(device), target_.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(data_)\n",
    "        loss = criterion(outputs, target_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==target_).item()\n",
    "        total += target_.size(0)\n",
    "        if (batch_idx) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data_t, target_t in (test_dataloader):\n",
    "            data_t, target_t = data_t.to(device), target_t.to(device)\n",
    "            outputs_t = net(data_t)\n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            _,pred_t = torch.max(outputs_t, dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "        val_acc.append(100 * correct_t/total_t)\n",
    "        val_loss.append(batch_loss/len(test_dataloader))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n",
    "\n",
    "        \n",
    "        if network_learned:\n",
    "            valid_loss_min = batch_loss\n",
    "            torch.save(net.state_dict(), 'WideResnet101_adam_batch8_learninglr0.001_nnlinear48_epoch30.pt')\n",
    "            print('Improvement-Detected, save-model')\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e32765",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.title(\"Train-Validation Accuracy\")\n",
    "plt.plot(train_acc, label='train')\n",
    "plt.plot(val_acc, label='validation')\n",
    "plt.xlabel('num_epochs', fontsize=12)\n",
    "plt.ylabel('accuracy', fontsize=12)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(net, num_images=4):\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        if device:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        preds = preds.cpu().numpy() if device else preds.numpy()\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.subplot(2, num_images//2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('predictes: {}'.format(test_dataset.classes[preds[j]]))\n",
    "            imshow(inputs[j])\n",
    "            \n",
    "            if images_so_far == num_images:\n",
    "                return \n",
    "\n",
    "plt.ion()\n",
    "visualize_model(net)\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b415f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
