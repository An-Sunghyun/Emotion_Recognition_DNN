{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756bf2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10cdbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size=1\n",
    "learning_rate = 0.001\n",
    "layers = 100\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(48),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(root='emotion_FixedVer_TrainTest/train/', transform=transforms)\n",
    "test_dataset = datasets.ImageFolder(root='emotion_FixedVer_TrainTest/test/', transform=transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,in_planes,out_planes,dropRate = 0.0):\n",
    "        #input dimsnsion을 정하고, output dimension을 정하고(growh_rate임), dropRate를 정함.\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace = True) # inplace 하면 input으로 들어온 것 자체를 수정하겠다는 뜻. 메모리 usage가 좀 좋아짐. 하지만 input을 없앰.\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride = 1, padding = 1, bias = False)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate>0:\n",
    "            out = F.dropout (out,p=self.droprate,training = self.training)\n",
    "        return torch.cat([x,out],1)\n",
    "        \n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self,in_planes,out_planes,dropRate=0.0):\n",
    "        #out_planes => growh_rate를 입력으로 받게 된다.\n",
    "        super(BottleneckBlock,self).__init__()\n",
    "        inter_planes = out_planes * 4 # bottleneck layer의 conv 1x1 filter chennel 수는 4*growh_rate이다.\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.conv1 = nn.Conv2d(in_planes,inter_planes,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(inter_planes)\n",
    "        self.conv2 = nn.Conv2d(inter_planes,out_planes,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate>0:\n",
    "            out = F.dropout(out,p=self.droprate,inplace=False,training = self.training)\n",
    "        out = self.conv2(self.relu(self.bn2(out)))\n",
    "        if self.droprate>0:\n",
    "            out = F.dropout(out,p=self.droprate,inplace=False,training = self.training)\n",
    "        return torch.cat([x,out],1) # 입력으로 받은 x와 새로 만든 output을 합쳐서 내보낸다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self,nb_layers,in_planes,growh_rate,block,dropRate=0.0):\n",
    "        super(DenseBlock,self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, growh_rate, nb_layers, dropRate)\n",
    "    \n",
    "    def _make_layer(self,block,in_planes,growh_rate,nb_layers,dropRate):\n",
    "        layers=[]\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(in_planes + i*growh_rate ,growh_rate,dropRate))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6869af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionBlock(nn.Module):\n",
    "    def __init__(self,in_planes,out_planes,dropRate=0.0):\n",
    "        super(TransitionBlock,self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes,out_planes,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate>0:\n",
    "            out = F.dropout(out,p=self.droprate,inplace=False,training=self.training)\n",
    "        return F.avg_pool2d(out,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe947225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,depth,num_classes,growh_rate=12,reduction=0.5,bottleneck=True,dropRate=0.0):\n",
    "        super(DenseNet,self).__init__()\n",
    "        num_of_blocks = 3\n",
    "        in_planes = 16 # 2 * growh_rate\n",
    "        n = (depth - num_of_blocks - 1)/num_of_blocks # 총 depth에서 첫 conv , 2개의 transit , 마지막 linear 빼고 / num_of_blocks\n",
    "        if reduction != 1 :\n",
    "            in_planes = 2 * growh_rate\n",
    "        if bottleneck == True:\n",
    "            in_planes = 2 * growh_rate #논문에서 Bottleneck + Compression 할 경우 first layer은 2*growh_rate라고 했다.\n",
    "            n = n/2 # conv 1x1 레이어가 추가되니까 !\n",
    "            block = BottleneckBlock \n",
    "        else :\n",
    "            block = BasicBlock\n",
    "        \n",
    "        n = int(n) #n = DenseBlock에서 block layer 개수를 의미한다.\n",
    "        self.conv1 = nn.Conv2d(3,in_planes,kernel_size=3,stride=1,padding=1,bias=False) # input:RGB -> output:growhR*2\n",
    "        \n",
    "        \n",
    "        #1st block\n",
    "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
    "        self.block1 = DenseBlock(n,in_planes,growh_rate,block,dropRate)\n",
    "        in_planes = int(in_planes+n*growh_rate) # 입력 + 레이어 만큼의 growh_rate\n",
    "        \n",
    "        # in_planes,out_planes,dropRate\n",
    "        self.trans1 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)),dropRate=dropRate)\n",
    "        in_planes = int(math.floor(in_planes*reduction))\n",
    "        \n",
    "        \n",
    "        #2nd block\n",
    "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
    "        self.block2 = DenseBlock(n,in_planes,growh_rate,block,dropRate)\n",
    "        in_planes = int(in_planes+n*growh_rate) # 입력 + 레이어 만큼의 growh_rate\n",
    "        \n",
    "        # in_planes,out_planes,dropRate\n",
    "        self.trans2 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)),dropRate=dropRate)\n",
    "        in_planes = int(math.floor(in_planes*reduction))\n",
    "        \n",
    "        \n",
    "        #3rd block\n",
    "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
    "        self.block3 = DenseBlock(n,in_planes,growh_rate,block,dropRate)\n",
    "        in_planes = int(in_planes+n*growh_rate) # 입력 + 레이어 만큼의 growh_rate\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        \n",
    "        self.fc = nn.Linear(in_planes,num_classes) # 마지막에 ave_pool 후에 1x1 size의 결과만 남음.\n",
    "        \n",
    "        self.in_planes = in_planes\n",
    "        \n",
    "        # module 초기화\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Conv layer들은 필터에서 나오는 분산 root(2/n)로 normalize 함\n",
    "                # mean = 0 , 분산 = sqrt(2/n) // 이게 무슨 초기화 방법이었는지 기억이 안난다.\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d): # shifting param이랑 scaling param 초기화(?)\n",
    "                m.weight.data.fill_(1) # \n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):# linear layer 초기화.\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x : 32*32\n",
    "        out = self.conv1(x) # 32*32\n",
    "        out = self.block1(out) # 32*32\n",
    "        out = self.trans1(out) # 16*16\n",
    "        out = self.block2(out) # 16*16\n",
    "        out = self.trans2(out) # 8*8\n",
    "        out = self.block3(out) # 8*8\n",
    "        out = self.relu(self.bn1(out)) #8*8\n",
    "        out = F.avg_pool2d(out,8) #1*1\n",
    "        out = out.view(-1, self.in_planes) #channel수만 남기 때문에 Linear -> in_planes\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee58627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('DenseNetModelSave.pt')\n",
    "model = DenseNet(layers,10,growh_rate=12,dropRate = 0.0)\n",
    "\n",
    "\n",
    "# get the number of model parameters\n",
    "# 재미있는 코드라서 들고와봄.\n",
    "print('Number of model parameters: {}'.format(\n",
    "    sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)#해보자 한번\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate,\n",
    "                            momentum=0.9,nesterov=True,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57585211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader,model,criterion,optimizer,epoch):\n",
    "    model.train()\n",
    "    for i, (input,target) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "        \n",
    "        output = model(input)\n",
    "        loss = criterion(output,target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(i%20 == 0):\n",
    "            print(\"loss in epoch %d , step %d : %f\" % (epoch, i,loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d097b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader,model,criterion,epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    \n",
    "    for i, (input,target) in enumerate(test_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "        \n",
    "        output = model(input)\n",
    "        loss = criterion(output,target)\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().float().sum()\n",
    "    \n",
    "    print(\"Accuracy in epoch %d : %f\" % (epoch,100.0*correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce1b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, epoch, learning_rate):\n",
    "    if epoch==15 :\n",
    "        learning_rate*=0.1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0af231",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0,20):\n",
    "    adjust_lr(optimizer,epoch,learning_rate)\n",
    "    train(train_loader,model,criterion,optimizer,epoch)\n",
    "    test(test_loader,model,criterion,epoch)\n",
    "    # ... after training, save your model \n",
    "    torch.save(model, 'DenseNetModelSave.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7314d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. to load your previously training model:\n",
    "model = torch.load('DenseNetModelSave.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b844c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.title(\"Train-Validation Accuracy\")\n",
    "plt.plot(train_acc, label='train')\n",
    "plt.plot(val_acc, label='validation')\n",
    "plt.xlabel('num_epochs', fontsize=12)\n",
    "plt.ylabel('accuracy', fontsize=12)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f877e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
