{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756bf2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201aa963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Otakuking/.cache\\torch\\hub\\pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (features): Sequential(\n",
       "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer25): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer26): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer27): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1088, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1088, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer28): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1120, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer29): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer30): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer31): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1216, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer32): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer33): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer34): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1312, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer35): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1344, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer36): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1376, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer37): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1408, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer38): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1440, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer39): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1472, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1472, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer40): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1504, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer41): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer42): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1568, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1568, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer43): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1600, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer44): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1632, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer45): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1664, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1664, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer46): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1696, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1696, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer47): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1728, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer48): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1760, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1760, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(1792, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1088, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1088, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1120, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1216, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1312, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1344, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1376, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1408, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1440, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1472, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1472, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1504, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1568, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1568, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1600, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1632, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer25): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1664, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1664, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer26): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1696, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1696, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer27): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1728, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer28): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1760, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1760, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer29): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1792, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer30): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1824, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer31): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1856, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer32): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1888, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1888, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1920, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.9.0', 'densenet201', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8801bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 0.0001\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='emotion_FixedVer_TrainTest/train/', transform=transforms)\n",
    "test_dataset = datasets.ImageFolder(root='emotion_FixedVer_TrainTest/test/', transform=transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0e2b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \n",
    "    inp = inp.cpu() if device else inp\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d010d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images-size: torch.Size([16, 3, 224, 224])\n",
      "out-size: torch.Size([3, 454, 1810])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(train_dataloader)) \n",
    "print(\"images-size:\", images.shape)\n",
    "\n",
    "out = torchvision.utils.make_grid(images)\n",
    "print(\"out-size:\", out.shape)\n",
    "\n",
    "imshow(out, title=[train_dataset.classes[x] for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe975ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (features): Sequential(\n",
       "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer25): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer26): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer27): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1088, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1088, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer28): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1120, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer29): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer30): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer31): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1216, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer32): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer33): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer34): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1312, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer35): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1344, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer36): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1376, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer37): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1408, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer38): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1440, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer39): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1472, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1472, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer40): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1504, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer41): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer42): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1568, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1568, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer43): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1600, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer44): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1632, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer45): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1664, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1664, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer46): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1696, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1696, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer47): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1728, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer48): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1760, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1760, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(1792, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1088, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1088, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1120, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1216, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1312, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1344, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1376, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1408, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1440, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1472, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1472, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1504, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1568, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1568, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1600, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1632, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer25): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1664, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1664, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer26): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1696, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1696, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer27): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1728, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer28): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1760, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1760, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer29): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1792, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer30): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1824, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer31): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1856, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer32): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(1888, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(1888, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1920, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = model\n",
    "net = net.cuda() if device else net\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c622f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr= 0.0001, \n",
    "                 betas=(0.9, 0.999), \n",
    "                 eps=1e-08, \n",
    "                 weight_decay=0, \n",
    "                 amsgrad=False)\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    _,pred = torch.max(out, dim=1)\n",
    "    return torch.sum(pred==labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b24daf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "Epoch [1/100], Step [0/1248], Loss: 12.5665\n",
      "Epoch [1/100], Step [20/1248], Loss: 1.7986\n",
      "Epoch [1/100], Step [40/1248], Loss: 1.7564\n",
      "Epoch [1/100], Step [60/1248], Loss: 1.9362\n",
      "Epoch [1/100], Step [80/1248], Loss: 1.7727\n",
      "Epoch [1/100], Step [100/1248], Loss: 1.7726\n",
      "Epoch [1/100], Step [120/1248], Loss: 1.7963\n",
      "Epoch [1/100], Step [140/1248], Loss: 1.5297\n",
      "Epoch [1/100], Step [160/1248], Loss: 1.8040\n",
      "Epoch [1/100], Step [180/1248], Loss: 1.3248\n",
      "Epoch [1/100], Step [200/1248], Loss: 1.5236\n",
      "Epoch [1/100], Step [220/1248], Loss: 1.6529\n",
      "Epoch [1/100], Step [240/1248], Loss: 1.7456\n",
      "Epoch [1/100], Step [260/1248], Loss: 1.4531\n",
      "Epoch [1/100], Step [280/1248], Loss: 1.9066\n",
      "Epoch [1/100], Step [300/1248], Loss: 1.3632\n",
      "Epoch [1/100], Step [320/1248], Loss: 1.3861\n",
      "Epoch [1/100], Step [340/1248], Loss: 0.9668\n",
      "Epoch [1/100], Step [360/1248], Loss: 1.1055\n",
      "Epoch [1/100], Step [380/1248], Loss: 1.3828\n",
      "Epoch [1/100], Step [400/1248], Loss: 1.3804\n",
      "Epoch [1/100], Step [420/1248], Loss: 1.5017\n",
      "Epoch [1/100], Step [440/1248], Loss: 1.0815\n",
      "Epoch [1/100], Step [460/1248], Loss: 1.4852\n",
      "Epoch [1/100], Step [480/1248], Loss: 0.9545\n",
      "Epoch [1/100], Step [500/1248], Loss: 1.3429\n",
      "Epoch [1/100], Step [520/1248], Loss: 1.2846\n",
      "Epoch [1/100], Step [540/1248], Loss: 1.5542\n",
      "Epoch [1/100], Step [560/1248], Loss: 1.2122\n",
      "Epoch [1/100], Step [580/1248], Loss: 1.1119\n",
      "Epoch [1/100], Step [600/1248], Loss: 1.2479\n",
      "Epoch [1/100], Step [620/1248], Loss: 1.0314\n",
      "Epoch [1/100], Step [640/1248], Loss: 1.2265\n",
      "Epoch [1/100], Step [660/1248], Loss: 1.0245\n",
      "Epoch [1/100], Step [680/1248], Loss: 1.1707\n",
      "Epoch [1/100], Step [700/1248], Loss: 1.3055\n",
      "Epoch [1/100], Step [720/1248], Loss: 1.1995\n",
      "Epoch [1/100], Step [740/1248], Loss: 1.1235\n",
      "Epoch [1/100], Step [760/1248], Loss: 1.2597\n",
      "Epoch [1/100], Step [780/1248], Loss: 1.5893\n",
      "Epoch [1/100], Step [800/1248], Loss: 1.5440\n",
      "Epoch [1/100], Step [820/1248], Loss: 0.7550\n",
      "Epoch [1/100], Step [840/1248], Loss: 1.2687\n",
      "Epoch [1/100], Step [860/1248], Loss: 1.3454\n",
      "Epoch [1/100], Step [880/1248], Loss: 1.2891\n",
      "Epoch [1/100], Step [900/1248], Loss: 1.2101\n",
      "Epoch [1/100], Step [920/1248], Loss: 1.7107\n",
      "Epoch [1/100], Step [940/1248], Loss: 1.7896\n",
      "Epoch [1/100], Step [960/1248], Loss: 1.1921\n",
      "Epoch [1/100], Step [980/1248], Loss: 1.1266\n",
      "Epoch [1/100], Step [1000/1248], Loss: 1.2823\n",
      "Epoch [1/100], Step [1020/1248], Loss: 1.0568\n",
      "Epoch [1/100], Step [1040/1248], Loss: 0.9663\n",
      "Epoch [1/100], Step [1060/1248], Loss: 1.1185\n",
      "Epoch [1/100], Step [1080/1248], Loss: 1.0359\n",
      "Epoch [1/100], Step [1100/1248], Loss: 0.8601\n",
      "Epoch [1/100], Step [1120/1248], Loss: 1.3774\n",
      "Epoch [1/100], Step [1140/1248], Loss: 1.5442\n",
      "Epoch [1/100], Step [1160/1248], Loss: 0.8441\n",
      "Epoch [1/100], Step [1180/1248], Loss: 1.0226\n",
      "Epoch [1/100], Step [1200/1248], Loss: 1.3675\n",
      "Epoch [1/100], Step [1220/1248], Loss: 1.1939\n",
      "Epoch [1/100], Step [1240/1248], Loss: 1.6301\n",
      "\n",
      "train-loss: 1.3532, train-acc: 47.0691\n",
      "validation loss: 1.1938, validation acc: 54.2615\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 2\n",
      "\n",
      "Epoch [2/100], Step [0/1248], Loss: 4.3048\n",
      "Epoch [2/100], Step [20/1248], Loss: 1.4829\n",
      "Epoch [2/100], Step [40/1248], Loss: 1.5137\n",
      "Epoch [2/100], Step [60/1248], Loss: 2.0253\n",
      "Epoch [2/100], Step [80/1248], Loss: 1.1913\n",
      "Epoch [2/100], Step [100/1248], Loss: 1.0965\n",
      "Epoch [2/100], Step [120/1248], Loss: 1.0364\n",
      "Epoch [2/100], Step [140/1248], Loss: 1.1083\n",
      "Epoch [2/100], Step [160/1248], Loss: 0.8621\n",
      "Epoch [2/100], Step [180/1248], Loss: 1.5631\n",
      "Epoch [2/100], Step [200/1248], Loss: 1.4372\n",
      "Epoch [2/100], Step [220/1248], Loss: 0.8867\n",
      "Epoch [2/100], Step [240/1248], Loss: 0.9285\n",
      "Epoch [2/100], Step [260/1248], Loss: 1.2794\n",
      "Epoch [2/100], Step [280/1248], Loss: 0.7811\n",
      "Epoch [2/100], Step [300/1248], Loss: 1.3785\n",
      "Epoch [2/100], Step [320/1248], Loss: 0.8208\n",
      "Epoch [2/100], Step [340/1248], Loss: 1.1586\n",
      "Epoch [2/100], Step [360/1248], Loss: 1.0207\n",
      "Epoch [2/100], Step [380/1248], Loss: 1.3913\n",
      "Epoch [2/100], Step [400/1248], Loss: 1.1990\n",
      "Epoch [2/100], Step [420/1248], Loss: 1.0056\n",
      "Epoch [2/100], Step [440/1248], Loss: 1.3236\n",
      "Epoch [2/100], Step [460/1248], Loss: 0.5847\n",
      "Epoch [2/100], Step [480/1248], Loss: 0.9191\n",
      "Epoch [2/100], Step [500/1248], Loss: 0.7972\n",
      "Epoch [2/100], Step [520/1248], Loss: 0.7201\n",
      "Epoch [2/100], Step [540/1248], Loss: 1.1580\n",
      "Epoch [2/100], Step [560/1248], Loss: 1.0663\n",
      "Epoch [2/100], Step [580/1248], Loss: 0.9428\n",
      "Epoch [2/100], Step [600/1248], Loss: 0.8573\n",
      "Epoch [2/100], Step [620/1248], Loss: 0.9233\n",
      "Epoch [2/100], Step [640/1248], Loss: 1.3504\n",
      "Epoch [2/100], Step [660/1248], Loss: 1.0162\n",
      "Epoch [2/100], Step [680/1248], Loss: 1.5861\n",
      "Epoch [2/100], Step [700/1248], Loss: 1.0407\n",
      "Epoch [2/100], Step [720/1248], Loss: 1.0484\n",
      "Epoch [2/100], Step [740/1248], Loss: 1.2715\n",
      "Epoch [2/100], Step [760/1248], Loss: 1.2795\n",
      "Epoch [2/100], Step [780/1248], Loss: 0.9512\n",
      "Epoch [2/100], Step [800/1248], Loss: 0.9639\n",
      "Epoch [2/100], Step [820/1248], Loss: 1.0018\n",
      "Epoch [2/100], Step [840/1248], Loss: 0.8855\n",
      "Epoch [2/100], Step [860/1248], Loss: 0.7677\n",
      "Epoch [2/100], Step [880/1248], Loss: 1.0292\n",
      "Epoch [2/100], Step [900/1248], Loss: 1.0650\n",
      "Epoch [2/100], Step [920/1248], Loss: 0.9305\n",
      "Epoch [2/100], Step [940/1248], Loss: 1.0210\n",
      "Epoch [2/100], Step [960/1248], Loss: 0.8316\n",
      "Epoch [2/100], Step [980/1248], Loss: 0.8825\n",
      "Epoch [2/100], Step [1000/1248], Loss: 0.8807\n",
      "Epoch [2/100], Step [1020/1248], Loss: 0.7982\n",
      "Epoch [2/100], Step [1040/1248], Loss: 1.3007\n",
      "Epoch [2/100], Step [1060/1248], Loss: 0.9082\n",
      "Epoch [2/100], Step [1080/1248], Loss: 0.9720\n",
      "Epoch [2/100], Step [1100/1248], Loss: 0.6536\n",
      "Epoch [2/100], Step [1120/1248], Loss: 0.8521\n",
      "Epoch [2/100], Step [1140/1248], Loss: 0.6687\n",
      "Epoch [2/100], Step [1160/1248], Loss: 1.2720\n",
      "Epoch [2/100], Step [1180/1248], Loss: 1.1871\n",
      "Epoch [2/100], Step [1200/1248], Loss: 1.0766\n",
      "Epoch [2/100], Step [1220/1248], Loss: 1.0208\n",
      "Epoch [2/100], Step [1240/1248], Loss: 0.9724\n",
      "\n",
      "train-loss: 1.2249, train-acc: 58.8677\n",
      "validation loss: 1.0422, validation acc: 66.2114\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 3\n",
      "\n",
      "Epoch [3/100], Step [0/1248], Loss: 0.8556\n",
      "Epoch [3/100], Step [20/1248], Loss: 1.4311\n",
      "Epoch [3/100], Step [40/1248], Loss: 0.5807\n",
      "Epoch [3/100], Step [60/1248], Loss: 0.8963\n",
      "Epoch [3/100], Step [80/1248], Loss: 0.6201\n",
      "Epoch [3/100], Step [100/1248], Loss: 0.8729\n",
      "Epoch [3/100], Step [120/1248], Loss: 0.4382\n",
      "Epoch [3/100], Step [140/1248], Loss: 0.6859\n",
      "Epoch [3/100], Step [160/1248], Loss: 0.3228\n",
      "Epoch [3/100], Step [180/1248], Loss: 1.0240\n",
      "Epoch [3/100], Step [200/1248], Loss: 1.2480\n",
      "Epoch [3/100], Step [220/1248], Loss: 1.2247\n",
      "Epoch [3/100], Step [240/1248], Loss: 0.7375\n",
      "Epoch [3/100], Step [260/1248], Loss: 0.6447\n",
      "Epoch [3/100], Step [280/1248], Loss: 1.0564\n",
      "Epoch [3/100], Step [300/1248], Loss: 0.7250\n",
      "Epoch [3/100], Step [320/1248], Loss: 1.2134\n",
      "Epoch [3/100], Step [340/1248], Loss: 1.2585\n",
      "Epoch [3/100], Step [360/1248], Loss: 0.6895\n",
      "Epoch [3/100], Step [380/1248], Loss: 0.6687\n",
      "Epoch [3/100], Step [400/1248], Loss: 1.0204\n",
      "Epoch [3/100], Step [420/1248], Loss: 0.8173\n",
      "Epoch [3/100], Step [440/1248], Loss: 0.6639\n",
      "Epoch [3/100], Step [460/1248], Loss: 0.8930\n",
      "Epoch [3/100], Step [480/1248], Loss: 0.9271\n",
      "Epoch [3/100], Step [500/1248], Loss: 0.7452\n",
      "Epoch [3/100], Step [520/1248], Loss: 0.7486\n",
      "Epoch [3/100], Step [540/1248], Loss: 0.9422\n",
      "Epoch [3/100], Step [560/1248], Loss: 0.6739\n",
      "Epoch [3/100], Step [580/1248], Loss: 0.7524\n",
      "Epoch [3/100], Step [600/1248], Loss: 1.5819\n",
      "Epoch [3/100], Step [620/1248], Loss: 0.6024\n",
      "Epoch [3/100], Step [640/1248], Loss: 0.6025\n",
      "Epoch [3/100], Step [660/1248], Loss: 0.9060\n",
      "Epoch [3/100], Step [680/1248], Loss: 1.3617\n",
      "Epoch [3/100], Step [700/1248], Loss: 0.8739\n",
      "Epoch [3/100], Step [720/1248], Loss: 0.8082\n",
      "Epoch [3/100], Step [740/1248], Loss: 0.7693\n",
      "Epoch [3/100], Step [760/1248], Loss: 0.7429\n",
      "Epoch [3/100], Step [780/1248], Loss: 1.0716\n",
      "Epoch [3/100], Step [800/1248], Loss: 0.9176\n",
      "Epoch [3/100], Step [820/1248], Loss: 1.0261\n",
      "Epoch [3/100], Step [840/1248], Loss: 0.8508\n",
      "Epoch [3/100], Step [860/1248], Loss: 0.5296\n",
      "Epoch [3/100], Step [880/1248], Loss: 1.1125\n",
      "Epoch [3/100], Step [900/1248], Loss: 0.3523\n",
      "Epoch [3/100], Step [920/1248], Loss: 0.5664\n",
      "Epoch [3/100], Step [940/1248], Loss: 1.4120\n",
      "Epoch [3/100], Step [960/1248], Loss: 0.9837\n",
      "Epoch [3/100], Step [980/1248], Loss: 0.6896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Step [1000/1248], Loss: 1.3809\n",
      "Epoch [3/100], Step [1020/1248], Loss: 1.0284\n",
      "Epoch [3/100], Step [1040/1248], Loss: 1.2691\n",
      "Epoch [3/100], Step [1060/1248], Loss: 0.9566\n",
      "Epoch [3/100], Step [1080/1248], Loss: 0.7799\n",
      "Epoch [3/100], Step [1100/1248], Loss: 0.9394\n",
      "Epoch [3/100], Step [1120/1248], Loss: 0.7179\n",
      "Epoch [3/100], Step [1140/1248], Loss: 0.8776\n",
      "Epoch [3/100], Step [1160/1248], Loss: 0.6698\n",
      "Epoch [3/100], Step [1180/1248], Loss: 0.9261\n",
      "Epoch [3/100], Step [1200/1248], Loss: 0.7168\n",
      "Epoch [3/100], Step [1220/1248], Loss: 0.9944\n",
      "Epoch [3/100], Step [1240/1248], Loss: 1.1245\n",
      "\n",
      "train-loss: 1.1078, train-acc: 67.0341\n",
      "validation loss: 0.9912, validation acc: 66.9196\n",
      "\n",
      "Improvement-Detected, save-model\n",
      "Epoch 4\n",
      "\n",
      "Epoch [4/100], Step [0/1248], Loss: 0.7259\n",
      "Epoch [4/100], Step [20/1248], Loss: 0.6908\n",
      "Epoch [4/100], Step [40/1248], Loss: 0.7023\n",
      "Epoch [4/100], Step [60/1248], Loss: 0.5233\n",
      "Epoch [4/100], Step [80/1248], Loss: 0.6413\n",
      "Epoch [4/100], Step [100/1248], Loss: 0.8169\n",
      "Epoch [4/100], Step [120/1248], Loss: 0.5535\n",
      "Epoch [4/100], Step [140/1248], Loss: 0.3467\n",
      "Epoch [4/100], Step [160/1248], Loss: 0.6508\n",
      "Epoch [4/100], Step [180/1248], Loss: 0.4864\n",
      "Epoch [4/100], Step [200/1248], Loss: 0.5107\n",
      "Epoch [4/100], Step [220/1248], Loss: 0.8659\n",
      "Epoch [4/100], Step [240/1248], Loss: 0.9680\n",
      "Epoch [4/100], Step [260/1248], Loss: 0.5492\n",
      "Epoch [4/100], Step [280/1248], Loss: 0.5452\n",
      "Epoch [4/100], Step [300/1248], Loss: 0.7109\n",
      "Epoch [4/100], Step [320/1248], Loss: 0.7974\n",
      "Epoch [4/100], Step [340/1248], Loss: 0.3777\n",
      "Epoch [4/100], Step [360/1248], Loss: 1.0156\n",
      "Epoch [4/100], Step [380/1248], Loss: 0.4929\n",
      "Epoch [4/100], Step [400/1248], Loss: 0.7060\n",
      "Epoch [4/100], Step [420/1248], Loss: 0.8548\n",
      "Epoch [4/100], Step [440/1248], Loss: 0.8947\n",
      "Epoch [4/100], Step [460/1248], Loss: 0.5806\n",
      "Epoch [4/100], Step [480/1248], Loss: 0.5834\n",
      "Epoch [4/100], Step [500/1248], Loss: 0.3413\n",
      "Epoch [4/100], Step [520/1248], Loss: 1.1674\n",
      "Epoch [4/100], Step [540/1248], Loss: 0.7019\n",
      "Epoch [4/100], Step [560/1248], Loss: 0.8583\n",
      "Epoch [4/100], Step [580/1248], Loss: 1.1897\n",
      "Epoch [4/100], Step [600/1248], Loss: 0.5592\n",
      "Epoch [4/100], Step [620/1248], Loss: 0.9255\n",
      "Epoch [4/100], Step [640/1248], Loss: 0.8171\n",
      "Epoch [4/100], Step [660/1248], Loss: 0.8781\n",
      "Epoch [4/100], Step [680/1248], Loss: 0.5706\n",
      "Epoch [4/100], Step [700/1248], Loss: 1.2033\n",
      "Epoch [4/100], Step [720/1248], Loss: 0.3281\n",
      "Epoch [4/100], Step [740/1248], Loss: 0.7705\n",
      "Epoch [4/100], Step [760/1248], Loss: 0.8000\n",
      "Epoch [4/100], Step [780/1248], Loss: 0.9148\n",
      "Epoch [4/100], Step [800/1248], Loss: 1.3219\n",
      "Epoch [4/100], Step [820/1248], Loss: 0.9637\n",
      "Epoch [4/100], Step [840/1248], Loss: 0.5067\n",
      "Epoch [4/100], Step [860/1248], Loss: 0.6171\n",
      "Epoch [4/100], Step [880/1248], Loss: 0.7354\n",
      "Epoch [4/100], Step [900/1248], Loss: 0.6149\n",
      "Epoch [4/100], Step [920/1248], Loss: 0.7707\n",
      "Epoch [4/100], Step [940/1248], Loss: 0.9173\n",
      "Epoch [4/100], Step [960/1248], Loss: 0.2540\n",
      "Epoch [4/100], Step [980/1248], Loss: 0.9170\n",
      "Epoch [4/100], Step [1000/1248], Loss: 1.3928\n",
      "Epoch [4/100], Step [1020/1248], Loss: 0.8524\n",
      "Epoch [4/100], Step [1040/1248], Loss: 0.8119\n",
      "Epoch [4/100], Step [1060/1248], Loss: 0.7437\n",
      "Epoch [4/100], Step [1080/1248], Loss: 0.4652\n",
      "Epoch [4/100], Step [1100/1248], Loss: 0.6583\n",
      "Epoch [4/100], Step [1120/1248], Loss: 1.0687\n",
      "Epoch [4/100], Step [1140/1248], Loss: 0.8959\n",
      "Epoch [4/100], Step [1160/1248], Loss: 0.6065\n",
      "Epoch [4/100], Step [1180/1248], Loss: 1.0041\n",
      "Epoch [4/100], Step [1200/1248], Loss: 0.5710\n",
      "Epoch [4/100], Step [1220/1248], Loss: 0.3903\n",
      "Epoch [4/100], Step [1240/1248], Loss: 0.7184\n",
      "\n",
      "train-loss: 1.0137, train-acc: 72.8808\n",
      "validation loss: 0.9692, validation acc: 67.5266\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "Epoch [5/100], Step [0/1248], Loss: 0.5815\n",
      "Epoch [5/100], Step [20/1248], Loss: 0.3541\n",
      "Epoch [5/100], Step [40/1248], Loss: 0.5156\n",
      "Epoch [5/100], Step [60/1248], Loss: 0.5171\n",
      "Epoch [5/100], Step [80/1248], Loss: 0.5904\n",
      "Epoch [5/100], Step [100/1248], Loss: 0.4754\n",
      "Epoch [5/100], Step [120/1248], Loss: 0.4699\n",
      "Epoch [5/100], Step [140/1248], Loss: 0.8158\n",
      "Epoch [5/100], Step [160/1248], Loss: 0.3687\n",
      "Epoch [5/100], Step [180/1248], Loss: 0.2410\n",
      "Epoch [5/100], Step [200/1248], Loss: 0.6285\n",
      "Epoch [5/100], Step [220/1248], Loss: 0.2146\n",
      "Epoch [5/100], Step [240/1248], Loss: 0.6027\n",
      "Epoch [5/100], Step [260/1248], Loss: 0.7049\n",
      "Epoch [5/100], Step [280/1248], Loss: 0.4751\n",
      "Epoch [5/100], Step [300/1248], Loss: 0.5855\n",
      "Epoch [5/100], Step [320/1248], Loss: 0.5763\n",
      "Epoch [5/100], Step [340/1248], Loss: 0.4655\n",
      "Epoch [5/100], Step [360/1248], Loss: 0.6513\n",
      "Epoch [5/100], Step [380/1248], Loss: 0.3445\n",
      "Epoch [5/100], Step [400/1248], Loss: 0.5310\n",
      "Epoch [5/100], Step [420/1248], Loss: 0.6949\n",
      "Epoch [5/100], Step [440/1248], Loss: 0.7567\n",
      "Epoch [5/100], Step [460/1248], Loss: 0.2684\n",
      "Epoch [5/100], Step [480/1248], Loss: 0.3604\n",
      "Epoch [5/100], Step [500/1248], Loss: 0.7804\n",
      "Epoch [5/100], Step [520/1248], Loss: 0.2942\n",
      "Epoch [5/100], Step [540/1248], Loss: 0.4690\n",
      "Epoch [5/100], Step [560/1248], Loss: 0.5896\n",
      "Epoch [5/100], Step [580/1248], Loss: 0.5909\n",
      "Epoch [5/100], Step [600/1248], Loss: 0.4273\n",
      "Epoch [5/100], Step [620/1248], Loss: 0.4209\n",
      "Epoch [5/100], Step [640/1248], Loss: 0.5525\n",
      "Epoch [5/100], Step [660/1248], Loss: 0.9672\n",
      "Epoch [5/100], Step [680/1248], Loss: 0.9423\n",
      "Epoch [5/100], Step [700/1248], Loss: 0.7887\n",
      "Epoch [5/100], Step [720/1248], Loss: 0.5345\n",
      "Epoch [5/100], Step [740/1248], Loss: 0.4004\n",
      "Epoch [5/100], Step [760/1248], Loss: 0.4926\n",
      "Epoch [5/100], Step [780/1248], Loss: 1.1394\n",
      "Epoch [5/100], Step [800/1248], Loss: 0.5887\n",
      "Epoch [5/100], Step [820/1248], Loss: 0.9396\n",
      "Epoch [5/100], Step [840/1248], Loss: 0.6675\n",
      "Epoch [5/100], Step [860/1248], Loss: 0.4375\n",
      "Epoch [5/100], Step [880/1248], Loss: 0.9859\n",
      "Epoch [5/100], Step [900/1248], Loss: 0.4058\n",
      "Epoch [5/100], Step [920/1248], Loss: 0.3829\n",
      "Epoch [5/100], Step [940/1248], Loss: 0.6003\n",
      "Epoch [5/100], Step [960/1248], Loss: 1.2403\n",
      "Epoch [5/100], Step [980/1248], Loss: 0.3109\n",
      "Epoch [5/100], Step [1000/1248], Loss: 0.4841\n",
      "Epoch [5/100], Step [1020/1248], Loss: 0.4552\n",
      "Epoch [5/100], Step [1040/1248], Loss: 0.7231\n",
      "Epoch [5/100], Step [1060/1248], Loss: 1.1384\n",
      "Epoch [5/100], Step [1080/1248], Loss: 0.2929\n",
      "Epoch [5/100], Step [1100/1248], Loss: 0.4294\n",
      "Epoch [5/100], Step [1120/1248], Loss: 0.4510\n",
      "Epoch [5/100], Step [1140/1248], Loss: 0.8840\n",
      "Epoch [5/100], Step [1160/1248], Loss: 0.6677\n",
      "Epoch [5/100], Step [1180/1248], Loss: 0.7890\n",
      "Epoch [5/100], Step [1200/1248], Loss: 0.4264\n",
      "Epoch [5/100], Step [1220/1248], Loss: 0.7742\n",
      "Epoch [5/100], Step [1240/1248], Loss: 0.3290\n",
      "\n",
      "train-loss: 0.9254, train-acc: 79.0381\n",
      "validation loss: 0.9735, validation acc: 67.6657\n",
      "\n",
      "Epoch 6\n",
      "\n",
      "Epoch [6/100], Step [0/1248], Loss: 0.1673\n",
      "Epoch [6/100], Step [20/1248], Loss: 0.1932\n",
      "Epoch [6/100], Step [40/1248], Loss: 0.4315\n",
      "Epoch [6/100], Step [60/1248], Loss: 0.1591\n",
      "Epoch [6/100], Step [80/1248], Loss: 0.3929\n",
      "Epoch [6/100], Step [100/1248], Loss: 0.1969\n",
      "Epoch [6/100], Step [120/1248], Loss: 0.1704\n",
      "Epoch [6/100], Step [140/1248], Loss: 0.4373\n",
      "Epoch [6/100], Step [160/1248], Loss: 0.3546\n",
      "Epoch [6/100], Step [180/1248], Loss: 0.5590\n",
      "Epoch [6/100], Step [200/1248], Loss: 0.3392\n",
      "Epoch [6/100], Step [220/1248], Loss: 0.1897\n",
      "Epoch [6/100], Step [240/1248], Loss: 0.7371\n",
      "Epoch [6/100], Step [260/1248], Loss: 0.3987\n",
      "Epoch [6/100], Step [280/1248], Loss: 0.4657\n",
      "Epoch [6/100], Step [300/1248], Loss: 0.4746\n",
      "Epoch [6/100], Step [320/1248], Loss: 0.3002\n",
      "Epoch [6/100], Step [340/1248], Loss: 0.1890\n",
      "Epoch [6/100], Step [360/1248], Loss: 0.2518\n",
      "Epoch [6/100], Step [380/1248], Loss: 0.7994\n",
      "Epoch [6/100], Step [400/1248], Loss: 0.5134\n",
      "Epoch [6/100], Step [420/1248], Loss: 0.5059\n",
      "Epoch [6/100], Step [440/1248], Loss: 0.2327\n",
      "Epoch [6/100], Step [460/1248], Loss: 0.2995\n",
      "Epoch [6/100], Step [480/1248], Loss: 0.0599\n",
      "Epoch [6/100], Step [500/1248], Loss: 0.5464\n",
      "Epoch [6/100], Step [520/1248], Loss: 0.2250\n",
      "Epoch [6/100], Step [540/1248], Loss: 0.9787\n",
      "Epoch [6/100], Step [560/1248], Loss: 0.2937\n",
      "Epoch [6/100], Step [580/1248], Loss: 0.7750\n",
      "Epoch [6/100], Step [600/1248], Loss: 0.3420\n",
      "Epoch [6/100], Step [620/1248], Loss: 0.3567\n",
      "Epoch [6/100], Step [640/1248], Loss: 0.7337\n",
      "Epoch [6/100], Step [660/1248], Loss: 0.2706\n",
      "Epoch [6/100], Step [680/1248], Loss: 0.4648\n",
      "Epoch [6/100], Step [700/1248], Loss: 0.3535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Step [720/1248], Loss: 0.4292\n",
      "Epoch [6/100], Step [740/1248], Loss: 0.5107\n",
      "Epoch [6/100], Step [760/1248], Loss: 0.6449\n",
      "Epoch [6/100], Step [780/1248], Loss: 0.2379\n",
      "Epoch [6/100], Step [800/1248], Loss: 0.2521\n",
      "Epoch [6/100], Step [820/1248], Loss: 0.9065\n",
      "Epoch [6/100], Step [840/1248], Loss: 0.2746\n",
      "Epoch [6/100], Step [860/1248], Loss: 0.6751\n",
      "Epoch [6/100], Step [880/1248], Loss: 0.5896\n",
      "Epoch [6/100], Step [900/1248], Loss: 0.0862\n",
      "Epoch [6/100], Step [920/1248], Loss: 0.7438\n",
      "Epoch [6/100], Step [940/1248], Loss: 0.7523\n",
      "Epoch [6/100], Step [960/1248], Loss: 0.1982\n",
      "Epoch [6/100], Step [980/1248], Loss: 0.4683\n",
      "Epoch [6/100], Step [1000/1248], Loss: 0.5617\n",
      "Epoch [6/100], Step [1020/1248], Loss: 0.7711\n",
      "Epoch [6/100], Step [1040/1248], Loss: 0.1277\n",
      "Epoch [6/100], Step [1060/1248], Loss: 0.3682\n",
      "Epoch [6/100], Step [1080/1248], Loss: 0.1855\n",
      "Epoch [6/100], Step [1100/1248], Loss: 0.4275\n",
      "Epoch [6/100], Step [1120/1248], Loss: 0.3695\n",
      "Epoch [6/100], Step [1140/1248], Loss: 0.3604\n",
      "Epoch [6/100], Step [1160/1248], Loss: 0.4420\n",
      "Epoch [6/100], Step [1180/1248], Loss: 0.3342\n",
      "Epoch [6/100], Step [1200/1248], Loss: 0.5682\n",
      "Epoch [6/100], Step [1220/1248], Loss: 0.3948\n",
      "Epoch [6/100], Step [1240/1248], Loss: 0.5867\n",
      "\n",
      "train-loss: 0.8378, train-acc: 85.7715\n",
      "validation loss: 0.9816, validation acc: 67.8048\n",
      "\n",
      "Epoch 7\n",
      "\n",
      "Epoch [7/100], Step [0/1248], Loss: 0.2474\n",
      "Epoch [7/100], Step [20/1248], Loss: 0.0405\n",
      "Epoch [7/100], Step [40/1248], Loss: 0.1921\n",
      "Epoch [7/100], Step [60/1248], Loss: 0.5221\n",
      "Epoch [7/100], Step [80/1248], Loss: 0.0520\n",
      "Epoch [7/100], Step [100/1248], Loss: 0.4470\n",
      "Epoch [7/100], Step [120/1248], Loss: 0.1335\n",
      "Epoch [7/100], Step [140/1248], Loss: 0.0787\n",
      "Epoch [7/100], Step [160/1248], Loss: 0.1896\n",
      "Epoch [7/100], Step [180/1248], Loss: 0.0730\n",
      "Epoch [7/100], Step [200/1248], Loss: 0.7645\n",
      "Epoch [7/100], Step [220/1248], Loss: 0.1839\n",
      "Epoch [7/100], Step [240/1248], Loss: 0.2921\n",
      "Epoch [7/100], Step [260/1248], Loss: 0.0863\n",
      "Epoch [7/100], Step [280/1248], Loss: 0.3886\n",
      "Epoch [7/100], Step [300/1248], Loss: 0.1122\n",
      "Epoch [7/100], Step [320/1248], Loss: 0.2879\n",
      "Epoch [7/100], Step [340/1248], Loss: 0.0442\n",
      "Epoch [7/100], Step [360/1248], Loss: 0.3509\n",
      "Epoch [7/100], Step [380/1248], Loss: 0.4158\n",
      "Epoch [7/100], Step [400/1248], Loss: 0.3068\n",
      "Epoch [7/100], Step [420/1248], Loss: 0.2593\n",
      "Epoch [7/100], Step [440/1248], Loss: 0.2821\n",
      "Epoch [7/100], Step [460/1248], Loss: 0.0867\n",
      "Epoch [7/100], Step [480/1248], Loss: 0.1097\n",
      "Epoch [7/100], Step [500/1248], Loss: 0.3095\n",
      "Epoch [7/100], Step [520/1248], Loss: 0.0373\n",
      "Epoch [7/100], Step [540/1248], Loss: 0.1567\n",
      "Epoch [7/100], Step [560/1248], Loss: 0.4762\n",
      "Epoch [7/100], Step [580/1248], Loss: 0.3099\n",
      "Epoch [7/100], Step [600/1248], Loss: 0.1348\n",
      "Epoch [7/100], Step [620/1248], Loss: 0.3335\n",
      "Epoch [7/100], Step [640/1248], Loss: 0.2278\n",
      "Epoch [7/100], Step [660/1248], Loss: 0.3035\n",
      "Epoch [7/100], Step [680/1248], Loss: 0.4379\n",
      "Epoch [7/100], Step [700/1248], Loss: 0.7888\n",
      "Epoch [7/100], Step [720/1248], Loss: 0.1610\n",
      "Epoch [7/100], Step [740/1248], Loss: 0.1312\n",
      "Epoch [7/100], Step [760/1248], Loss: 0.1941\n",
      "Epoch [7/100], Step [780/1248], Loss: 0.4464\n",
      "Epoch [7/100], Step [800/1248], Loss: 0.0994\n",
      "Epoch [7/100], Step [820/1248], Loss: 0.1181\n",
      "Epoch [7/100], Step [840/1248], Loss: 0.2180\n",
      "Epoch [7/100], Step [860/1248], Loss: 0.1315\n",
      "Epoch [7/100], Step [880/1248], Loss: 0.2048\n",
      "Epoch [7/100], Step [900/1248], Loss: 0.2152\n",
      "Epoch [7/100], Step [920/1248], Loss: 0.3455\n",
      "Epoch [7/100], Step [940/1248], Loss: 0.4829\n",
      "Epoch [7/100], Step [960/1248], Loss: 0.1766\n",
      "Epoch [7/100], Step [980/1248], Loss: 0.1534\n",
      "Epoch [7/100], Step [1000/1248], Loss: 0.1829\n",
      "Epoch [7/100], Step [1020/1248], Loss: 0.2058\n",
      "Epoch [7/100], Step [1040/1248], Loss: 0.4674\n",
      "Epoch [7/100], Step [1060/1248], Loss: 0.0842\n",
      "Epoch [7/100], Step [1080/1248], Loss: 0.0823\n",
      "Epoch [7/100], Step [1100/1248], Loss: 0.4347\n",
      "Epoch [7/100], Step [1120/1248], Loss: 0.2231\n",
      "Epoch [7/100], Step [1140/1248], Loss: 0.2946\n",
      "Epoch [7/100], Step [1160/1248], Loss: 0.3274\n",
      "Epoch [7/100], Step [1180/1248], Loss: 0.1602\n",
      "Epoch [7/100], Step [1200/1248], Loss: 0.0833\n",
      "Epoch [7/100], Step [1220/1248], Loss: 0.0634\n",
      "Epoch [7/100], Step [1240/1248], Loss: 0.6043\n",
      "\n",
      "train-loss: 0.7592, train-acc: 90.0401\n",
      "validation loss: 1.0070, validation acc: 68.3485\n",
      "\n",
      "Epoch 8\n",
      "\n",
      "Epoch [8/100], Step [0/1248], Loss: 0.5242\n",
      "Epoch [8/100], Step [20/1248], Loss: 0.1836\n",
      "Epoch [8/100], Step [40/1248], Loss: 0.0309\n",
      "Epoch [8/100], Step [60/1248], Loss: 0.0790\n",
      "Epoch [8/100], Step [80/1248], Loss: 0.1625\n",
      "Epoch [8/100], Step [100/1248], Loss: 0.2937\n",
      "Epoch [8/100], Step [120/1248], Loss: 0.1813\n",
      "Epoch [8/100], Step [140/1248], Loss: 0.3166\n",
      "Epoch [8/100], Step [160/1248], Loss: 0.0328\n",
      "Epoch [8/100], Step [180/1248], Loss: 0.1621\n",
      "Epoch [8/100], Step [200/1248], Loss: 0.1430\n",
      "Epoch [8/100], Step [220/1248], Loss: 0.1452\n",
      "Epoch [8/100], Step [240/1248], Loss: 0.1460\n",
      "Epoch [8/100], Step [260/1248], Loss: 0.0516\n",
      "Epoch [8/100], Step [280/1248], Loss: 1.0235\n",
      "Epoch [8/100], Step [300/1248], Loss: 0.0885\n",
      "Epoch [8/100], Step [320/1248], Loss: 0.1214\n",
      "Epoch [8/100], Step [340/1248], Loss: 0.2209\n",
      "Epoch [8/100], Step [360/1248], Loss: 0.3080\n",
      "Epoch [8/100], Step [380/1248], Loss: 0.5372\n",
      "Epoch [8/100], Step [400/1248], Loss: 0.2290\n",
      "Epoch [8/100], Step [420/1248], Loss: 0.1391\n",
      "Epoch [8/100], Step [440/1248], Loss: 0.0553\n",
      "Epoch [8/100], Step [460/1248], Loss: 0.0394\n",
      "Epoch [8/100], Step [480/1248], Loss: 0.2896\n",
      "Epoch [8/100], Step [500/1248], Loss: 0.1010\n",
      "Epoch [8/100], Step [520/1248], Loss: 0.0854\n",
      "Epoch [8/100], Step [540/1248], Loss: 0.0584\n",
      "Epoch [8/100], Step [560/1248], Loss: 0.0383\n",
      "Epoch [8/100], Step [580/1248], Loss: 0.3813\n",
      "Epoch [8/100], Step [600/1248], Loss: 0.0911\n",
      "Epoch [8/100], Step [620/1248], Loss: 0.1059\n",
      "Epoch [8/100], Step [640/1248], Loss: 0.0477\n",
      "Epoch [8/100], Step [660/1248], Loss: 0.0191\n",
      "Epoch [8/100], Step [680/1248], Loss: 0.1386\n",
      "Epoch [8/100], Step [700/1248], Loss: 0.4258\n",
      "Epoch [8/100], Step [720/1248], Loss: 0.0644\n",
      "Epoch [8/100], Step [740/1248], Loss: 0.0920\n",
      "Epoch [8/100], Step [760/1248], Loss: 0.0953\n",
      "Epoch [8/100], Step [780/1248], Loss: 0.0730\n",
      "Epoch [8/100], Step [800/1248], Loss: 0.0624\n",
      "Epoch [8/100], Step [820/1248], Loss: 0.1596\n",
      "Epoch [8/100], Step [840/1248], Loss: 0.0335\n",
      "Epoch [8/100], Step [860/1248], Loss: 0.4015\n",
      "Epoch [8/100], Step [880/1248], Loss: 0.0556\n",
      "Epoch [8/100], Step [900/1248], Loss: 0.1630\n",
      "Epoch [8/100], Step [920/1248], Loss: 0.0216\n",
      "Epoch [8/100], Step [940/1248], Loss: 0.5685\n",
      "Epoch [8/100], Step [960/1248], Loss: 0.2806\n",
      "Epoch [8/100], Step [980/1248], Loss: 0.0477\n",
      "Epoch [8/100], Step [1000/1248], Loss: 0.3139\n",
      "Epoch [8/100], Step [1020/1248], Loss: 0.5465\n",
      "Epoch [8/100], Step [1040/1248], Loss: 0.1684\n",
      "Epoch [8/100], Step [1060/1248], Loss: 0.2384\n",
      "Epoch [8/100], Step [1080/1248], Loss: 0.1314\n",
      "Epoch [8/100], Step [1100/1248], Loss: 0.4294\n",
      "Epoch [8/100], Step [1120/1248], Loss: 0.1514\n",
      "Epoch [8/100], Step [1140/1248], Loss: 0.4153\n",
      "Epoch [8/100], Step [1160/1248], Loss: 0.3744\n",
      "Epoch [8/100], Step [1180/1248], Loss: 0.2588\n",
      "Epoch [8/100], Step [1200/1248], Loss: 0.2417\n",
      "Epoch [8/100], Step [1220/1248], Loss: 0.1067\n",
      "Epoch [8/100], Step [1240/1248], Loss: 0.0358\n",
      "\n",
      "train-loss: 0.6899, train-acc: 92.9259\n",
      "validation loss: 1.0433, validation acc: 66.6793\n",
      "\n",
      "Epoch 9\n",
      "\n",
      "Epoch [9/100], Step [0/1248], Loss: 0.0473\n",
      "Epoch [9/100], Step [20/1248], Loss: 0.1142\n",
      "Epoch [9/100], Step [40/1248], Loss: 0.1116\n",
      "Epoch [9/100], Step [60/1248], Loss: 0.1375\n",
      "Epoch [9/100], Step [80/1248], Loss: 0.2431\n",
      "Epoch [9/100], Step [100/1248], Loss: 0.0216\n",
      "Epoch [9/100], Step [120/1248], Loss: 0.1580\n",
      "Epoch [9/100], Step [140/1248], Loss: 0.2850\n",
      "Epoch [9/100], Step [160/1248], Loss: 0.0881\n",
      "Epoch [9/100], Step [180/1248], Loss: 0.1373\n",
      "Epoch [9/100], Step [200/1248], Loss: 0.0467\n",
      "Epoch [9/100], Step [220/1248], Loss: 0.1866\n",
      "Epoch [9/100], Step [240/1248], Loss: 0.0461\n",
      "Epoch [9/100], Step [260/1248], Loss: 0.3249\n",
      "Epoch [9/100], Step [280/1248], Loss: 0.1982\n",
      "Epoch [9/100], Step [300/1248], Loss: 0.0575\n",
      "Epoch [9/100], Step [320/1248], Loss: 0.7477\n",
      "Epoch [9/100], Step [340/1248], Loss: 0.2434\n",
      "Epoch [9/100], Step [360/1248], Loss: 0.4563\n",
      "Epoch [9/100], Step [380/1248], Loss: 0.4270\n",
      "Epoch [9/100], Step [400/1248], Loss: 0.6118\n",
      "Epoch [9/100], Step [420/1248], Loss: 0.0792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Step [440/1248], Loss: 0.2247\n",
      "Epoch [9/100], Step [460/1248], Loss: 0.0933\n",
      "Epoch [9/100], Step [480/1248], Loss: 0.3226\n",
      "Epoch [9/100], Step [500/1248], Loss: 0.2297\n",
      "Epoch [9/100], Step [520/1248], Loss: 0.1900\n",
      "Epoch [9/100], Step [540/1248], Loss: 0.0657\n",
      "Epoch [9/100], Step [560/1248], Loss: 0.9133\n",
      "Epoch [9/100], Step [580/1248], Loss: 0.0313\n",
      "Epoch [9/100], Step [600/1248], Loss: 0.4733\n",
      "Epoch [9/100], Step [620/1248], Loss: 0.0916\n",
      "Epoch [9/100], Step [640/1248], Loss: 0.2402\n",
      "Epoch [9/100], Step [660/1248], Loss: 0.0826\n",
      "Epoch [9/100], Step [680/1248], Loss: 0.1211\n",
      "Epoch [9/100], Step [700/1248], Loss: 0.1297\n",
      "Epoch [9/100], Step [720/1248], Loss: 0.1556\n",
      "Epoch [9/100], Step [740/1248], Loss: 0.2881\n",
      "Epoch [9/100], Step [760/1248], Loss: 0.1480\n",
      "Epoch [9/100], Step [780/1248], Loss: 0.0563\n",
      "Epoch [9/100], Step [800/1248], Loss: 0.0504\n",
      "Epoch [9/100], Step [820/1248], Loss: 0.1201\n",
      "Epoch [9/100], Step [840/1248], Loss: 0.0637\n",
      "Epoch [9/100], Step [860/1248], Loss: 0.1459\n",
      "Epoch [9/100], Step [880/1248], Loss: 0.1192\n",
      "Epoch [9/100], Step [900/1248], Loss: 0.2284\n",
      "Epoch [9/100], Step [920/1248], Loss: 0.3008\n",
      "Epoch [9/100], Step [940/1248], Loss: 0.6786\n",
      "Epoch [9/100], Step [960/1248], Loss: 0.1818\n",
      "Epoch [9/100], Step [980/1248], Loss: 0.2810\n",
      "Epoch [9/100], Step [1000/1248], Loss: 0.6980\n",
      "Epoch [9/100], Step [1020/1248], Loss: 0.3802\n",
      "Epoch [9/100], Step [1040/1248], Loss: 0.7958\n",
      "Epoch [9/100], Step [1060/1248], Loss: 0.3694\n",
      "Epoch [9/100], Step [1080/1248], Loss: 0.2434\n",
      "Epoch [9/100], Step [1100/1248], Loss: 0.2209\n",
      "Epoch [9/100], Step [1120/1248], Loss: 0.0718\n",
      "Epoch [9/100], Step [1140/1248], Loss: 0.0327\n",
      "Epoch [9/100], Step [1160/1248], Loss: 0.2475\n",
      "Epoch [9/100], Step [1180/1248], Loss: 0.2729\n",
      "Epoch [9/100], Step [1200/1248], Loss: 0.1941\n",
      "Epoch [9/100], Step [1220/1248], Loss: 0.2696\n",
      "Epoch [9/100], Step [1240/1248], Loss: 0.3502\n",
      "\n",
      "train-loss: 0.6327, train-acc: 94.1032\n",
      "validation loss: 1.0771, validation acc: 67.3875\n",
      "\n",
      "Epoch 10\n",
      "\n",
      "Epoch [10/100], Step [0/1248], Loss: 0.0375\n",
      "Epoch [10/100], Step [20/1248], Loss: 0.0988\n",
      "Epoch [10/100], Step [40/1248], Loss: 0.3228\n",
      "Epoch [10/100], Step [60/1248], Loss: 0.0872\n",
      "Epoch [10/100], Step [80/1248], Loss: 0.0610\n",
      "Epoch [10/100], Step [100/1248], Loss: 0.3454\n",
      "Epoch [10/100], Step [120/1248], Loss: 0.1944\n",
      "Epoch [10/100], Step [140/1248], Loss: 0.0432\n",
      "Epoch [10/100], Step [160/1248], Loss: 0.0586\n",
      "Epoch [10/100], Step [180/1248], Loss: 0.0217\n",
      "Epoch [10/100], Step [200/1248], Loss: 0.0080\n",
      "Epoch [10/100], Step [220/1248], Loss: 0.0109\n",
      "Epoch [10/100], Step [240/1248], Loss: 0.2339\n",
      "Epoch [10/100], Step [260/1248], Loss: 0.1862\n",
      "Epoch [10/100], Step [280/1248], Loss: 0.1915\n",
      "Epoch [10/100], Step [300/1248], Loss: 0.0654\n",
      "Epoch [10/100], Step [320/1248], Loss: 0.1270\n",
      "Epoch [10/100], Step [340/1248], Loss: 0.1084\n",
      "Epoch [10/100], Step [360/1248], Loss: 0.0360\n",
      "Epoch [10/100], Step [380/1248], Loss: 0.6028\n",
      "Epoch [10/100], Step [400/1248], Loss: 0.1383\n",
      "Epoch [10/100], Step [420/1248], Loss: 0.1498\n",
      "Epoch [10/100], Step [440/1248], Loss: 0.2697\n",
      "Epoch [10/100], Step [460/1248], Loss: 0.1098\n",
      "Epoch [10/100], Step [480/1248], Loss: 0.1015\n",
      "Epoch [10/100], Step [500/1248], Loss: 0.0791\n",
      "Epoch [10/100], Step [520/1248], Loss: 0.1293\n",
      "Epoch [10/100], Step [540/1248], Loss: 0.0712\n",
      "Epoch [10/100], Step [560/1248], Loss: 0.1209\n",
      "Epoch [10/100], Step [580/1248], Loss: 0.1001\n",
      "Epoch [10/100], Step [600/1248], Loss: 0.2080\n",
      "Epoch [10/100], Step [620/1248], Loss: 0.1441\n",
      "Epoch [10/100], Step [640/1248], Loss: 0.0887\n",
      "Epoch [10/100], Step [660/1248], Loss: 0.2408\n",
      "Epoch [10/100], Step [680/1248], Loss: 0.1136\n",
      "Epoch [10/100], Step [700/1248], Loss: 0.0543\n",
      "Epoch [10/100], Step [720/1248], Loss: 0.0203\n",
      "Epoch [10/100], Step [740/1248], Loss: 0.0589\n",
      "Epoch [10/100], Step [760/1248], Loss: 0.1384\n",
      "Epoch [10/100], Step [780/1248], Loss: 0.3383\n",
      "Epoch [10/100], Step [800/1248], Loss: 0.0804\n",
      "Epoch [10/100], Step [820/1248], Loss: 0.2042\n",
      "Epoch [10/100], Step [840/1248], Loss: 0.0828\n",
      "Epoch [10/100], Step [860/1248], Loss: 0.1536\n",
      "Epoch [10/100], Step [880/1248], Loss: 0.0535\n",
      "Epoch [10/100], Step [900/1248], Loss: 0.1040\n",
      "Epoch [10/100], Step [920/1248], Loss: 0.2614\n",
      "Epoch [10/100], Step [940/1248], Loss: 0.0798\n",
      "Epoch [10/100], Step [960/1248], Loss: 0.0336\n",
      "Epoch [10/100], Step [980/1248], Loss: 0.1260\n",
      "Epoch [10/100], Step [1000/1248], Loss: 0.0694\n",
      "Epoch [10/100], Step [1020/1248], Loss: 0.0733\n",
      "Epoch [10/100], Step [1040/1248], Loss: 0.0889\n",
      "Epoch [10/100], Step [1060/1248], Loss: 0.1620\n",
      "Epoch [10/100], Step [1080/1248], Loss: 0.1256\n",
      "Epoch [10/100], Step [1100/1248], Loss: 0.0368\n",
      "Epoch [10/100], Step [1120/1248], Loss: 0.2312\n",
      "Epoch [10/100], Step [1140/1248], Loss: 0.1222\n",
      "Epoch [10/100], Step [1160/1248], Loss: 0.0357\n",
      "Epoch [10/100], Step [1180/1248], Loss: 0.1984\n",
      "Epoch [10/100], Step [1200/1248], Loss: 0.1884\n",
      "Epoch [10/100], Step [1220/1248], Loss: 0.3890\n",
      "Epoch [10/100], Step [1240/1248], Loss: 0.0778\n",
      "\n",
      "train-loss: 0.5850, train-acc: 94.7345\n",
      "validation loss: 1.1037, validation acc: 67.2104\n",
      "\n",
      "Epoch 11\n",
      "\n",
      "Epoch [11/100], Step [0/1248], Loss: 0.0395\n",
      "Epoch [11/100], Step [20/1248], Loss: 0.2017\n",
      "Epoch [11/100], Step [40/1248], Loss: 0.1390\n",
      "Epoch [11/100], Step [60/1248], Loss: 0.0940\n",
      "Epoch [11/100], Step [80/1248], Loss: 0.1761\n",
      "Epoch [11/100], Step [100/1248], Loss: 0.3602\n",
      "Epoch [11/100], Step [120/1248], Loss: 0.0322\n",
      "Epoch [11/100], Step [140/1248], Loss: 0.0445\n",
      "Epoch [11/100], Step [160/1248], Loss: 0.0904\n",
      "Epoch [11/100], Step [180/1248], Loss: 0.0201\n",
      "Epoch [11/100], Step [200/1248], Loss: 0.0320\n",
      "Epoch [11/100], Step [220/1248], Loss: 0.2622\n",
      "Epoch [11/100], Step [240/1248], Loss: 0.2452\n",
      "Epoch [11/100], Step [260/1248], Loss: 0.0893\n",
      "Epoch [11/100], Step [280/1248], Loss: 0.2523\n",
      "Epoch [11/100], Step [300/1248], Loss: 0.0847\n",
      "Epoch [11/100], Step [320/1248], Loss: 0.0570\n",
      "Epoch [11/100], Step [340/1248], Loss: 0.1217\n",
      "Epoch [11/100], Step [360/1248], Loss: 0.0619\n",
      "Epoch [11/100], Step [380/1248], Loss: 0.2735\n",
      "Epoch [11/100], Step [400/1248], Loss: 0.1675\n",
      "Epoch [11/100], Step [420/1248], Loss: 0.0053\n",
      "Epoch [11/100], Step [440/1248], Loss: 0.1027\n",
      "Epoch [11/100], Step [460/1248], Loss: 0.0453\n",
      "Epoch [11/100], Step [480/1248], Loss: 0.1605\n",
      "Epoch [11/100], Step [500/1248], Loss: 0.2124\n",
      "Epoch [11/100], Step [520/1248], Loss: 0.2129\n",
      "Epoch [11/100], Step [540/1248], Loss: 0.0736\n",
      "Epoch [11/100], Step [560/1248], Loss: 0.1831\n",
      "Epoch [11/100], Step [580/1248], Loss: 0.0602\n",
      "Epoch [11/100], Step [600/1248], Loss: 0.0756\n",
      "Epoch [11/100], Step [620/1248], Loss: 0.0656\n",
      "Epoch [11/100], Step [640/1248], Loss: 0.1084\n",
      "Epoch [11/100], Step [660/1248], Loss: 0.3530\n",
      "Epoch [11/100], Step [680/1248], Loss: 0.0279\n",
      "Epoch [11/100], Step [700/1248], Loss: 0.0342\n",
      "Epoch [11/100], Step [720/1248], Loss: 0.0792\n",
      "Epoch [11/100], Step [740/1248], Loss: 0.1838\n",
      "Epoch [11/100], Step [760/1248], Loss: 0.2515\n",
      "Epoch [11/100], Step [780/1248], Loss: 0.1000\n",
      "Epoch [11/100], Step [800/1248], Loss: 0.1044\n",
      "Epoch [11/100], Step [820/1248], Loss: 0.0117\n",
      "Epoch [11/100], Step [840/1248], Loss: 0.0549\n",
      "Epoch [11/100], Step [860/1248], Loss: 0.1428\n",
      "Epoch [11/100], Step [880/1248], Loss: 0.0914\n",
      "Epoch [11/100], Step [900/1248], Loss: 0.1158\n",
      "Epoch [11/100], Step [920/1248], Loss: 0.2472\n",
      "Epoch [11/100], Step [940/1248], Loss: 0.1124\n",
      "Epoch [11/100], Step [960/1248], Loss: 0.3013\n",
      "Epoch [11/100], Step [980/1248], Loss: 0.1131\n",
      "Epoch [11/100], Step [1000/1248], Loss: 0.2660\n",
      "Epoch [11/100], Step [1020/1248], Loss: 0.0277\n",
      "Epoch [11/100], Step [1040/1248], Loss: 0.2087\n",
      "Epoch [11/100], Step [1060/1248], Loss: 0.1169\n",
      "Epoch [11/100], Step [1080/1248], Loss: 0.2049\n",
      "Epoch [11/100], Step [1100/1248], Loss: 0.0707\n",
      "Epoch [11/100], Step [1120/1248], Loss: 0.1856\n",
      "Epoch [11/100], Step [1140/1248], Loss: 0.1775\n",
      "Epoch [11/100], Step [1160/1248], Loss: 0.0688\n",
      "Epoch [11/100], Step [1180/1248], Loss: 0.1798\n",
      "Epoch [11/100], Step [1200/1248], Loss: 0.1394\n",
      "Epoch [11/100], Step [1220/1248], Loss: 0.0703\n",
      "Epoch [11/100], Step [1240/1248], Loss: 0.0127\n",
      "\n",
      "train-loss: 0.5445, train-acc: 95.3557\n",
      "validation loss: 1.1277, validation acc: 67.1851\n",
      "\n",
      "Epoch 12\n",
      "\n",
      "Epoch [12/100], Step [0/1248], Loss: 0.0352\n",
      "Epoch [12/100], Step [20/1248], Loss: 0.4312\n",
      "Epoch [12/100], Step [40/1248], Loss: 0.0096\n",
      "Epoch [12/100], Step [60/1248], Loss: 0.0556\n",
      "Epoch [12/100], Step [80/1248], Loss: 0.5625\n",
      "Epoch [12/100], Step [100/1248], Loss: 0.0904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Step [120/1248], Loss: 0.0607\n",
      "Epoch [12/100], Step [140/1248], Loss: 0.0556\n",
      "Epoch [12/100], Step [160/1248], Loss: 0.0096\n",
      "Epoch [12/100], Step [180/1248], Loss: 0.0070\n",
      "Epoch [12/100], Step [200/1248], Loss: 0.0612\n",
      "Epoch [12/100], Step [220/1248], Loss: 0.0812\n",
      "Epoch [12/100], Step [240/1248], Loss: 0.1096\n",
      "Epoch [12/100], Step [260/1248], Loss: 0.1302\n",
      "Epoch [12/100], Step [280/1248], Loss: 0.0394\n",
      "Epoch [12/100], Step [300/1248], Loss: 0.0245\n",
      "Epoch [12/100], Step [320/1248], Loss: 0.0235\n",
      "Epoch [12/100], Step [340/1248], Loss: 0.0174\n",
      "Epoch [12/100], Step [360/1248], Loss: 0.0405\n",
      "Epoch [12/100], Step [380/1248], Loss: 0.0969\n",
      "Epoch [12/100], Step [400/1248], Loss: 0.0407\n",
      "Epoch [12/100], Step [420/1248], Loss: 0.4040\n",
      "Epoch [12/100], Step [440/1248], Loss: 0.0191\n",
      "Epoch [12/100], Step [460/1248], Loss: 0.0480\n",
      "Epoch [12/100], Step [480/1248], Loss: 0.0925\n",
      "Epoch [12/100], Step [500/1248], Loss: 0.1190\n",
      "Epoch [12/100], Step [520/1248], Loss: 0.0606\n",
      "Epoch [12/100], Step [540/1248], Loss: 0.0147\n",
      "Epoch [12/100], Step [560/1248], Loss: 0.0372\n",
      "Epoch [12/100], Step [580/1248], Loss: 0.0954\n",
      "Epoch [12/100], Step [600/1248], Loss: 0.3581\n",
      "Epoch [12/100], Step [620/1248], Loss: 0.4868\n",
      "Epoch [12/100], Step [640/1248], Loss: 0.0705\n",
      "Epoch [12/100], Step [660/1248], Loss: 0.0751\n",
      "Epoch [12/100], Step [680/1248], Loss: 0.1329\n",
      "Epoch [12/100], Step [700/1248], Loss: 0.3244\n",
      "Epoch [12/100], Step [720/1248], Loss: 0.0638\n",
      "Epoch [12/100], Step [740/1248], Loss: 0.0180\n",
      "Epoch [12/100], Step [760/1248], Loss: 0.0553\n",
      "Epoch [12/100], Step [780/1248], Loss: 0.0895\n",
      "Epoch [12/100], Step [800/1248], Loss: 0.1388\n",
      "Epoch [12/100], Step [820/1248], Loss: 0.0370\n",
      "Epoch [12/100], Step [840/1248], Loss: 0.2175\n",
      "Epoch [12/100], Step [860/1248], Loss: 0.2207\n",
      "Epoch [12/100], Step [880/1248], Loss: 0.2758\n",
      "Epoch [12/100], Step [900/1248], Loss: 0.1111\n",
      "Epoch [12/100], Step [920/1248], Loss: 0.2757\n",
      "Epoch [12/100], Step [940/1248], Loss: 0.0452\n",
      "Epoch [12/100], Step [960/1248], Loss: 0.0189\n",
      "Epoch [12/100], Step [980/1248], Loss: 0.6610\n",
      "Epoch [12/100], Step [1000/1248], Loss: 0.1032\n",
      "Epoch [12/100], Step [1020/1248], Loss: 0.1470\n",
      "Epoch [12/100], Step [1040/1248], Loss: 0.0364\n",
      "Epoch [12/100], Step [1060/1248], Loss: 0.2255\n",
      "Epoch [12/100], Step [1080/1248], Loss: 0.2297\n",
      "Epoch [12/100], Step [1100/1248], Loss: 0.1986\n",
      "Epoch [12/100], Step [1120/1248], Loss: 0.2008\n",
      "Epoch [12/100], Step [1140/1248], Loss: 0.2156\n",
      "Epoch [12/100], Step [1160/1248], Loss: 0.3260\n",
      "Epoch [12/100], Step [1180/1248], Loss: 0.1844\n",
      "Epoch [12/100], Step [1200/1248], Loss: 0.0780\n",
      "Epoch [12/100], Step [1220/1248], Loss: 0.0492\n",
      "Epoch [12/100], Step [1240/1248], Loss: 0.3243\n",
      "\n",
      "train-loss: 0.5096, train-acc: 95.8166\n",
      "validation loss: 1.1508, validation acc: 67.1725\n",
      "\n",
      "Epoch 13\n",
      "\n",
      "Epoch [13/100], Step [0/1248], Loss: 0.0146\n",
      "Epoch [13/100], Step [20/1248], Loss: 0.0388\n",
      "Epoch [13/100], Step [40/1248], Loss: 0.0276\n",
      "Epoch [13/100], Step [60/1248], Loss: 0.1451\n",
      "Epoch [13/100], Step [80/1248], Loss: 0.0258\n",
      "Epoch [13/100], Step [100/1248], Loss: 0.2862\n",
      "Epoch [13/100], Step [120/1248], Loss: 0.0705\n",
      "Epoch [13/100], Step [140/1248], Loss: 0.0085\n",
      "Epoch [13/100], Step [160/1248], Loss: 0.0629\n",
      "Epoch [13/100], Step [180/1248], Loss: 0.0725\n",
      "Epoch [13/100], Step [200/1248], Loss: 0.1853\n",
      "Epoch [13/100], Step [220/1248], Loss: 0.0188\n",
      "Epoch [13/100], Step [240/1248], Loss: 0.2215\n",
      "Epoch [13/100], Step [260/1248], Loss: 0.1162\n",
      "Epoch [13/100], Step [280/1248], Loss: 0.0789\n",
      "Epoch [13/100], Step [300/1248], Loss: 0.0574\n",
      "Epoch [13/100], Step [320/1248], Loss: 0.2315\n",
      "Epoch [13/100], Step [340/1248], Loss: 0.0458\n",
      "Epoch [13/100], Step [360/1248], Loss: 0.0118\n",
      "Epoch [13/100], Step [380/1248], Loss: 0.0685\n",
      "Epoch [13/100], Step [400/1248], Loss: 0.4519\n",
      "Epoch [13/100], Step [420/1248], Loss: 0.1195\n",
      "Epoch [13/100], Step [440/1248], Loss: 0.0337\n",
      "Epoch [13/100], Step [460/1248], Loss: 0.0055\n",
      "Epoch [13/100], Step [480/1248], Loss: 0.2846\n",
      "Epoch [13/100], Step [500/1248], Loss: 0.0044\n",
      "Epoch [13/100], Step [520/1248], Loss: 0.0294\n",
      "Epoch [13/100], Step [540/1248], Loss: 0.0851\n",
      "Epoch [13/100], Step [560/1248], Loss: 0.2083\n",
      "Epoch [13/100], Step [580/1248], Loss: 0.0369\n",
      "Epoch [13/100], Step [600/1248], Loss: 0.1094\n",
      "Epoch [13/100], Step [620/1248], Loss: 0.1006\n",
      "Epoch [13/100], Step [640/1248], Loss: 0.1660\n",
      "Epoch [13/100], Step [660/1248], Loss: 0.1214\n",
      "Epoch [13/100], Step [680/1248], Loss: 0.0178\n",
      "Epoch [13/100], Step [700/1248], Loss: 0.1060\n",
      "Epoch [13/100], Step [720/1248], Loss: 0.0180\n",
      "Epoch [13/100], Step [740/1248], Loss: 0.1395\n",
      "Epoch [13/100], Step [760/1248], Loss: 0.2019\n",
      "Epoch [13/100], Step [780/1248], Loss: 0.1727\n",
      "Epoch [13/100], Step [800/1248], Loss: 0.2765\n",
      "Epoch [13/100], Step [820/1248], Loss: 0.2039\n",
      "Epoch [13/100], Step [840/1248], Loss: 0.0682\n",
      "Epoch [13/100], Step [860/1248], Loss: 0.1220\n",
      "Epoch [13/100], Step [880/1248], Loss: 0.0267\n",
      "Epoch [13/100], Step [900/1248], Loss: 0.0084\n",
      "Epoch [13/100], Step [920/1248], Loss: 0.1070\n",
      "Epoch [13/100], Step [940/1248], Loss: 0.3691\n",
      "Epoch [13/100], Step [960/1248], Loss: 0.0159\n",
      "Epoch [13/100], Step [980/1248], Loss: 0.1989\n",
      "Epoch [13/100], Step [1000/1248], Loss: 0.0241\n",
      "Epoch [13/100], Step [1020/1248], Loss: 0.1398\n",
      "Epoch [13/100], Step [1040/1248], Loss: 0.0959\n",
      "Epoch [13/100], Step [1060/1248], Loss: 0.1038\n",
      "Epoch [13/100], Step [1080/1248], Loss: 0.0767\n",
      "Epoch [13/100], Step [1100/1248], Loss: 0.0509\n",
      "Epoch [13/100], Step [1120/1248], Loss: 0.0409\n",
      "Epoch [13/100], Step [1140/1248], Loss: 0.0602\n",
      "Epoch [13/100], Step [1160/1248], Loss: 0.1279\n",
      "Epoch [13/100], Step [1180/1248], Loss: 0.4004\n",
      "Epoch [13/100], Step [1200/1248], Loss: 0.4495\n",
      "Epoch [13/100], Step [1220/1248], Loss: 0.4017\n",
      "Epoch [13/100], Step [1240/1248], Loss: 0.0202\n",
      "\n",
      "train-loss: 0.4795, train-acc: 96.0972\n",
      "validation loss: 1.1746, validation acc: 68.3359\n",
      "\n",
      "Epoch 14\n",
      "\n",
      "Epoch [14/100], Step [0/1248], Loss: 0.0044\n",
      "Epoch [14/100], Step [20/1248], Loss: 0.1037\n",
      "Epoch [14/100], Step [40/1248], Loss: 0.0333\n",
      "Epoch [14/100], Step [60/1248], Loss: 0.1666\n",
      "Epoch [14/100], Step [80/1248], Loss: 0.0270\n",
      "Epoch [14/100], Step [100/1248], Loss: 0.0503\n",
      "Epoch [14/100], Step [120/1248], Loss: 0.0146\n",
      "Epoch [14/100], Step [140/1248], Loss: 0.0128\n",
      "Epoch [14/100], Step [160/1248], Loss: 0.0437\n",
      "Epoch [14/100], Step [180/1248], Loss: 0.0208\n",
      "Epoch [14/100], Step [200/1248], Loss: 0.0245\n",
      "Epoch [14/100], Step [220/1248], Loss: 0.0359\n",
      "Epoch [14/100], Step [240/1248], Loss: 0.0677\n",
      "Epoch [14/100], Step [260/1248], Loss: 0.0166\n",
      "Epoch [14/100], Step [280/1248], Loss: 0.0177\n",
      "Epoch [14/100], Step [300/1248], Loss: 0.0645\n",
      "Epoch [14/100], Step [320/1248], Loss: 0.1759\n",
      "Epoch [14/100], Step [340/1248], Loss: 0.0363\n",
      "Epoch [14/100], Step [360/1248], Loss: 0.0223\n",
      "Epoch [14/100], Step [380/1248], Loss: 0.0022\n",
      "Epoch [14/100], Step [400/1248], Loss: 0.0603\n",
      "Epoch [14/100], Step [420/1248], Loss: 0.1246\n",
      "Epoch [14/100], Step [440/1248], Loss: 0.1906\n",
      "Epoch [14/100], Step [460/1248], Loss: 0.0517\n",
      "Epoch [14/100], Step [480/1248], Loss: 0.0878\n",
      "Epoch [14/100], Step [500/1248], Loss: 0.1081\n",
      "Epoch [14/100], Step [520/1248], Loss: 0.2104\n",
      "Epoch [14/100], Step [540/1248], Loss: 0.0541\n",
      "Epoch [14/100], Step [560/1248], Loss: 0.0543\n",
      "Epoch [14/100], Step [580/1248], Loss: 0.0180\n",
      "Epoch [14/100], Step [600/1248], Loss: 0.0715\n",
      "Epoch [14/100], Step [620/1248], Loss: 0.0306\n",
      "Epoch [14/100], Step [640/1248], Loss: 0.0250\n",
      "Epoch [14/100], Step [660/1248], Loss: 0.0096\n",
      "Epoch [14/100], Step [680/1248], Loss: 0.0170\n",
      "Epoch [14/100], Step [700/1248], Loss: 0.3173\n",
      "Epoch [14/100], Step [720/1248], Loss: 0.0341\n",
      "Epoch [14/100], Step [740/1248], Loss: 0.0112\n",
      "Epoch [14/100], Step [760/1248], Loss: 0.1488\n",
      "Epoch [14/100], Step [780/1248], Loss: 0.0318\n",
      "Epoch [14/100], Step [800/1248], Loss: 0.0412\n",
      "Epoch [14/100], Step [820/1248], Loss: 0.0105\n",
      "Epoch [14/100], Step [840/1248], Loss: 0.0685\n",
      "Epoch [14/100], Step [860/1248], Loss: 0.0150\n",
      "Epoch [14/100], Step [880/1248], Loss: 0.3418\n",
      "Epoch [14/100], Step [900/1248], Loss: 0.0160\n",
      "Epoch [14/100], Step [920/1248], Loss: 0.0557\n",
      "Epoch [14/100], Step [940/1248], Loss: 0.0798\n",
      "Epoch [14/100], Step [960/1248], Loss: 0.0348\n",
      "Epoch [14/100], Step [980/1248], Loss: 0.0285\n",
      "Epoch [14/100], Step [1000/1248], Loss: 0.0260\n",
      "Epoch [14/100], Step [1020/1248], Loss: 0.1459\n",
      "Epoch [14/100], Step [1040/1248], Loss: 0.0329\n",
      "Epoch [14/100], Step [1060/1248], Loss: 0.1422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Step [1080/1248], Loss: 0.0805\n",
      "Epoch [14/100], Step [1100/1248], Loss: 0.0857\n",
      "Epoch [14/100], Step [1120/1248], Loss: 0.1750\n",
      "Epoch [14/100], Step [1140/1248], Loss: 0.1419\n",
      "Epoch [14/100], Step [1160/1248], Loss: 0.0876\n",
      "Epoch [14/100], Step [1180/1248], Loss: 0.2396\n",
      "Epoch [14/100], Step [1200/1248], Loss: 0.1950\n",
      "Epoch [14/100], Step [1220/1248], Loss: 0.2052\n",
      "Epoch [14/100], Step [1240/1248], Loss: 0.1665\n",
      "\n",
      "train-loss: 0.4528, train-acc: 96.3327\n",
      "validation loss: 1.1912, validation acc: 67.5771\n",
      "\n",
      "Epoch 15\n",
      "\n",
      "Epoch [15/100], Step [0/1248], Loss: 0.0667\n",
      "Epoch [15/100], Step [20/1248], Loss: 0.0165\n",
      "Epoch [15/100], Step [40/1248], Loss: 0.0374\n",
      "Epoch [15/100], Step [60/1248], Loss: 0.0945\n",
      "Epoch [15/100], Step [80/1248], Loss: 0.3581\n",
      "Epoch [15/100], Step [100/1248], Loss: 0.0040\n",
      "Epoch [15/100], Step [120/1248], Loss: 0.0636\n",
      "Epoch [15/100], Step [140/1248], Loss: 0.3682\n",
      "Epoch [15/100], Step [160/1248], Loss: 0.3243\n",
      "Epoch [15/100], Step [180/1248], Loss: 0.2183\n",
      "Epoch [15/100], Step [200/1248], Loss: 0.0337\n",
      "Epoch [15/100], Step [220/1248], Loss: 0.0355\n",
      "Epoch [15/100], Step [240/1248], Loss: 0.0156\n",
      "Epoch [15/100], Step [260/1248], Loss: 0.0272\n",
      "Epoch [15/100], Step [280/1248], Loss: 0.0075\n",
      "Epoch [15/100], Step [300/1248], Loss: 0.0144\n",
      "Epoch [15/100], Step [320/1248], Loss: 0.0301\n",
      "Epoch [15/100], Step [340/1248], Loss: 0.1020\n",
      "Epoch [15/100], Step [360/1248], Loss: 0.0371\n",
      "Epoch [15/100], Step [380/1248], Loss: 0.1163\n",
      "Epoch [15/100], Step [400/1248], Loss: 0.0094\n",
      "Epoch [15/100], Step [420/1248], Loss: 0.0573\n",
      "Epoch [15/100], Step [440/1248], Loss: 0.0468\n",
      "Epoch [15/100], Step [460/1248], Loss: 0.1499\n",
      "Epoch [15/100], Step [480/1248], Loss: 0.0686\n",
      "Epoch [15/100], Step [500/1248], Loss: 0.0190\n",
      "Epoch [15/100], Step [520/1248], Loss: 0.0973\n",
      "Epoch [15/100], Step [540/1248], Loss: 0.0642\n",
      "Epoch [15/100], Step [560/1248], Loss: 0.4631\n",
      "Epoch [15/100], Step [580/1248], Loss: 0.0730\n",
      "Epoch [15/100], Step [600/1248], Loss: 0.1482\n",
      "Epoch [15/100], Step [620/1248], Loss: 0.0095\n",
      "Epoch [15/100], Step [640/1248], Loss: 0.2971\n",
      "Epoch [15/100], Step [660/1248], Loss: 0.1703\n",
      "Epoch [15/100], Step [680/1248], Loss: 0.0167\n",
      "Epoch [15/100], Step [700/1248], Loss: 0.2007\n",
      "Epoch [15/100], Step [720/1248], Loss: 0.0466\n",
      "Epoch [15/100], Step [740/1248], Loss: 0.1062\n",
      "Epoch [15/100], Step [760/1248], Loss: 0.0168\n",
      "Epoch [15/100], Step [780/1248], Loss: 0.0375\n",
      "Epoch [15/100], Step [800/1248], Loss: 0.4763\n",
      "Epoch [15/100], Step [820/1248], Loss: 0.1050\n",
      "Epoch [15/100], Step [840/1248], Loss: 0.1097\n",
      "Epoch [15/100], Step [860/1248], Loss: 0.0350\n",
      "Epoch [15/100], Step [880/1248], Loss: 0.0872\n",
      "Epoch [15/100], Step [900/1248], Loss: 0.0642\n",
      "Epoch [15/100], Step [920/1248], Loss: 0.1082\n",
      "Epoch [15/100], Step [940/1248], Loss: 0.1195\n",
      "Epoch [15/100], Step [960/1248], Loss: 0.0160\n",
      "Epoch [15/100], Step [980/1248], Loss: 0.0217\n",
      "Epoch [15/100], Step [1000/1248], Loss: 0.0858\n",
      "Epoch [15/100], Step [1020/1248], Loss: 0.7209\n",
      "Epoch [15/100], Step [1040/1248], Loss: 0.0228\n",
      "Epoch [15/100], Step [1060/1248], Loss: 0.0040\n",
      "Epoch [15/100], Step [1080/1248], Loss: 0.0265\n",
      "Epoch [15/100], Step [1100/1248], Loss: 0.3633\n",
      "Epoch [15/100], Step [1120/1248], Loss: 0.0032\n",
      "Epoch [15/100], Step [1140/1248], Loss: 0.0173\n",
      "Epoch [15/100], Step [1160/1248], Loss: 0.0533\n",
      "Epoch [15/100], Step [1180/1248], Loss: 0.0561\n",
      "Epoch [15/100], Step [1200/1248], Loss: 0.0422\n",
      "Epoch [15/100], Step [1220/1248], Loss: 0.0512\n",
      "Epoch [15/100], Step [1240/1248], Loss: 0.0319\n",
      "\n",
      "train-loss: 0.4293, train-acc: 96.6683\n",
      "validation loss: 1.2166, validation acc: 67.7921\n",
      "\n",
      "Epoch 16\n",
      "\n",
      "Epoch [16/100], Step [0/1248], Loss: 0.1774\n",
      "Epoch [16/100], Step [20/1248], Loss: 0.0208\n",
      "Epoch [16/100], Step [40/1248], Loss: 0.0055\n",
      "Epoch [16/100], Step [60/1248], Loss: 0.0479\n",
      "Epoch [16/100], Step [80/1248], Loss: 0.0129\n",
      "Epoch [16/100], Step [100/1248], Loss: 0.1409\n",
      "Epoch [16/100], Step [120/1248], Loss: 0.0062\n",
      "Epoch [16/100], Step [140/1248], Loss: 0.0806\n",
      "Epoch [16/100], Step [160/1248], Loss: 0.0312\n",
      "Epoch [16/100], Step [180/1248], Loss: 0.0549\n",
      "Epoch [16/100], Step [200/1248], Loss: 0.0308\n",
      "Epoch [16/100], Step [220/1248], Loss: 0.0039\n",
      "Epoch [16/100], Step [240/1248], Loss: 0.0046\n",
      "Epoch [16/100], Step [260/1248], Loss: 0.0307\n",
      "Epoch [16/100], Step [280/1248], Loss: 0.0570\n",
      "Epoch [16/100], Step [300/1248], Loss: 0.0090\n",
      "Epoch [16/100], Step [320/1248], Loss: 0.0929\n",
      "Epoch [16/100], Step [340/1248], Loss: 0.0738\n",
      "Epoch [16/100], Step [360/1248], Loss: 0.0537\n",
      "Epoch [16/100], Step [380/1248], Loss: 0.0894\n",
      "Epoch [16/100], Step [400/1248], Loss: 0.0351\n",
      "Epoch [16/100], Step [420/1248], Loss: 0.2311\n",
      "Epoch [16/100], Step [440/1248], Loss: 0.0095\n",
      "Epoch [16/100], Step [460/1248], Loss: 0.3686\n",
      "Epoch [16/100], Step [480/1248], Loss: 0.0036\n",
      "Epoch [16/100], Step [500/1248], Loss: 0.0514\n",
      "Epoch [16/100], Step [520/1248], Loss: 0.3217\n",
      "Epoch [16/100], Step [540/1248], Loss: 0.0204\n",
      "Epoch [16/100], Step [560/1248], Loss: 0.2631\n",
      "Epoch [16/100], Step [580/1248], Loss: 0.1343\n",
      "Epoch [16/100], Step [600/1248], Loss: 0.3429\n",
      "Epoch [16/100], Step [620/1248], Loss: 0.0445\n",
      "Epoch [16/100], Step [640/1248], Loss: 0.0241\n",
      "Epoch [16/100], Step [660/1248], Loss: 0.0216\n",
      "Epoch [16/100], Step [680/1248], Loss: 0.0681\n",
      "Epoch [16/100], Step [700/1248], Loss: 0.0934\n",
      "Epoch [16/100], Step [720/1248], Loss: 0.1842\n",
      "Epoch [16/100], Step [740/1248], Loss: 0.0071\n",
      "Epoch [16/100], Step [760/1248], Loss: 0.3364\n",
      "Epoch [16/100], Step [780/1248], Loss: 0.2059\n",
      "Epoch [16/100], Step [800/1248], Loss: 0.1466\n",
      "Epoch [16/100], Step [820/1248], Loss: 0.0200\n",
      "Epoch [16/100], Step [840/1248], Loss: 0.0208\n",
      "Epoch [16/100], Step [860/1248], Loss: 0.0100\n",
      "Epoch [16/100], Step [880/1248], Loss: 0.0039\n",
      "Epoch [16/100], Step [900/1248], Loss: 0.0132\n",
      "Epoch [16/100], Step [920/1248], Loss: 0.1915\n",
      "Epoch [16/100], Step [940/1248], Loss: 0.0265\n",
      "Epoch [16/100], Step [960/1248], Loss: 0.0646\n",
      "Epoch [16/100], Step [980/1248], Loss: 0.1014\n",
      "Epoch [16/100], Step [1000/1248], Loss: 0.1145\n",
      "Epoch [16/100], Step [1020/1248], Loss: 0.0338\n",
      "Epoch [16/100], Step [1040/1248], Loss: 0.0761\n",
      "Epoch [16/100], Step [1060/1248], Loss: 0.0134\n",
      "Epoch [16/100], Step [1080/1248], Loss: 0.0322\n",
      "Epoch [16/100], Step [1100/1248], Loss: 0.0700\n",
      "Epoch [16/100], Step [1120/1248], Loss: 0.1666\n",
      "Epoch [16/100], Step [1140/1248], Loss: 0.0132\n",
      "Epoch [16/100], Step [1160/1248], Loss: 0.0142\n",
      "Epoch [16/100], Step [1180/1248], Loss: 0.0066\n",
      "Epoch [16/100], Step [1200/1248], Loss: 0.0068\n",
      "Epoch [16/100], Step [1220/1248], Loss: 0.0110\n",
      "Epoch [16/100], Step [1240/1248], Loss: 0.1490\n",
      "\n",
      "train-loss: 0.4083, train-acc: 97.0641\n",
      "validation loss: 1.2373, validation acc: 67.5013\n",
      "\n",
      "Epoch 17\n",
      "\n",
      "Epoch [17/100], Step [0/1248], Loss: 0.1373\n",
      "Epoch [17/100], Step [20/1248], Loss: 0.1854\n",
      "Epoch [17/100], Step [40/1248], Loss: 0.0083\n",
      "Epoch [17/100], Step [60/1248], Loss: 0.1224\n",
      "Epoch [17/100], Step [80/1248], Loss: 0.0261\n",
      "Epoch [17/100], Step [100/1248], Loss: 0.0377\n",
      "Epoch [17/100], Step [120/1248], Loss: 0.0023\n",
      "Epoch [17/100], Step [140/1248], Loss: 0.0096\n",
      "Epoch [17/100], Step [160/1248], Loss: 0.1587\n",
      "Epoch [17/100], Step [180/1248], Loss: 0.0022\n",
      "Epoch [17/100], Step [200/1248], Loss: 0.0386\n",
      "Epoch [17/100], Step [220/1248], Loss: 0.0053\n",
      "Epoch [17/100], Step [240/1248], Loss: 0.2162\n",
      "Epoch [17/100], Step [260/1248], Loss: 0.0587\n",
      "Epoch [17/100], Step [280/1248], Loss: 0.0168\n",
      "Epoch [17/100], Step [300/1248], Loss: 0.0331\n",
      "Epoch [17/100], Step [320/1248], Loss: 0.0073\n",
      "Epoch [17/100], Step [340/1248], Loss: 0.0185\n",
      "Epoch [17/100], Step [360/1248], Loss: 0.0022\n",
      "Epoch [17/100], Step [380/1248], Loss: 0.0566\n",
      "Epoch [17/100], Step [400/1248], Loss: 0.0038\n",
      "Epoch [17/100], Step [420/1248], Loss: 0.0062\n",
      "Epoch [17/100], Step [440/1248], Loss: 0.0313\n",
      "Epoch [17/100], Step [460/1248], Loss: 0.0081\n",
      "Epoch [17/100], Step [480/1248], Loss: 0.0724\n",
      "Epoch [17/100], Step [500/1248], Loss: 0.0904\n",
      "Epoch [17/100], Step [520/1248], Loss: 0.0055\n",
      "Epoch [17/100], Step [540/1248], Loss: 0.0103\n",
      "Epoch [17/100], Step [560/1248], Loss: 0.0073\n",
      "Epoch [17/100], Step [580/1248], Loss: 0.0504\n",
      "Epoch [17/100], Step [600/1248], Loss: 0.1204\n",
      "Epoch [17/100], Step [620/1248], Loss: 0.3999\n",
      "Epoch [17/100], Step [640/1248], Loss: 0.0436\n",
      "Epoch [17/100], Step [660/1248], Loss: 0.0196\n",
      "Epoch [17/100], Step [680/1248], Loss: 0.1131\n",
      "Epoch [17/100], Step [700/1248], Loss: 0.1133\n",
      "Epoch [17/100], Step [720/1248], Loss: 0.0822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Step [740/1248], Loss: 0.0403\n",
      "Epoch [17/100], Step [760/1248], Loss: 0.1605\n",
      "Epoch [17/100], Step [780/1248], Loss: 0.0155\n",
      "Epoch [17/100], Step [800/1248], Loss: 0.4041\n",
      "Epoch [17/100], Step [820/1248], Loss: 0.0212\n",
      "Epoch [17/100], Step [840/1248], Loss: 0.0165\n",
      "Epoch [17/100], Step [860/1248], Loss: 0.0267\n",
      "Epoch [17/100], Step [880/1248], Loss: 0.1954\n",
      "Epoch [17/100], Step [900/1248], Loss: 0.0075\n",
      "Epoch [17/100], Step [920/1248], Loss: 0.0844\n",
      "Epoch [17/100], Step [940/1248], Loss: 0.0582\n",
      "Epoch [17/100], Step [960/1248], Loss: 0.0115\n",
      "Epoch [17/100], Step [980/1248], Loss: 0.0278\n",
      "Epoch [17/100], Step [1000/1248], Loss: 0.0193\n",
      "Epoch [17/100], Step [1020/1248], Loss: 0.0107\n",
      "Epoch [17/100], Step [1040/1248], Loss: 0.0753\n",
      "Epoch [17/100], Step [1060/1248], Loss: 0.0310\n",
      "Epoch [17/100], Step [1080/1248], Loss: 0.1366\n",
      "Epoch [17/100], Step [1100/1248], Loss: 0.0333\n",
      "Epoch [17/100], Step [1120/1248], Loss: 0.1633\n",
      "Epoch [17/100], Step [1140/1248], Loss: 0.0930\n",
      "Epoch [17/100], Step [1160/1248], Loss: 0.0186\n",
      "Epoch [17/100], Step [1180/1248], Loss: 0.0473\n",
      "Epoch [17/100], Step [1200/1248], Loss: 0.0062\n",
      "Epoch [17/100], Step [1220/1248], Loss: 0.0281\n",
      "Epoch [17/100], Step [1240/1248], Loss: 0.0132\n",
      "\n",
      "train-loss: 0.3895, train-acc: 97.0691\n",
      "validation loss: 1.2610, validation acc: 67.1598\n",
      "\n",
      "Epoch 18\n",
      "\n",
      "Epoch [18/100], Step [0/1248], Loss: 0.0179\n",
      "Epoch [18/100], Step [20/1248], Loss: 0.0812\n",
      "Epoch [18/100], Step [40/1248], Loss: 0.0373\n",
      "Epoch [18/100], Step [60/1248], Loss: 0.1187\n",
      "Epoch [18/100], Step [80/1248], Loss: 0.2032\n",
      "Epoch [18/100], Step [100/1248], Loss: 0.0212\n",
      "Epoch [18/100], Step [120/1248], Loss: 0.0946\n",
      "Epoch [18/100], Step [140/1248], Loss: 0.0634\n",
      "Epoch [18/100], Step [160/1248], Loss: 0.0096\n",
      "Epoch [18/100], Step [180/1248], Loss: 0.0733\n",
      "Epoch [18/100], Step [200/1248], Loss: 0.0651\n",
      "Epoch [18/100], Step [220/1248], Loss: 0.0341\n",
      "Epoch [18/100], Step [240/1248], Loss: 0.0110\n",
      "Epoch [18/100], Step [260/1248], Loss: 0.1476\n",
      "Epoch [18/100], Step [280/1248], Loss: 0.0077\n",
      "Epoch [18/100], Step [300/1248], Loss: 0.0167\n",
      "Epoch [18/100], Step [320/1248], Loss: 0.0318\n",
      "Epoch [18/100], Step [340/1248], Loss: 0.0430\n",
      "Epoch [18/100], Step [360/1248], Loss: 0.0522\n",
      "Epoch [18/100], Step [380/1248], Loss: 0.0174\n",
      "Epoch [18/100], Step [400/1248], Loss: 0.0535\n",
      "Epoch [18/100], Step [420/1248], Loss: 0.0925\n",
      "Epoch [18/100], Step [440/1248], Loss: 0.0306\n",
      "Epoch [18/100], Step [460/1248], Loss: 0.1850\n",
      "Epoch [18/100], Step [480/1248], Loss: 0.0624\n",
      "Epoch [18/100], Step [500/1248], Loss: 0.0372\n",
      "Epoch [18/100], Step [520/1248], Loss: 0.0541\n",
      "Epoch [18/100], Step [540/1248], Loss: 0.0747\n",
      "Epoch [18/100], Step [560/1248], Loss: 0.2291\n",
      "Epoch [18/100], Step [580/1248], Loss: 0.0736\n",
      "Epoch [18/100], Step [600/1248], Loss: 0.0109\n",
      "Epoch [18/100], Step [620/1248], Loss: 0.0890\n",
      "Epoch [18/100], Step [640/1248], Loss: 0.1348\n",
      "Epoch [18/100], Step [660/1248], Loss: 0.0116\n",
      "Epoch [18/100], Step [680/1248], Loss: 0.0034\n",
      "Epoch [18/100], Step [700/1248], Loss: 0.0470\n",
      "Epoch [18/100], Step [720/1248], Loss: 0.0177\n",
      "Epoch [18/100], Step [740/1248], Loss: 0.0641\n",
      "Epoch [18/100], Step [760/1248], Loss: 0.0379\n",
      "Epoch [18/100], Step [780/1248], Loss: 0.1017\n",
      "Epoch [18/100], Step [800/1248], Loss: 0.3006\n",
      "Epoch [18/100], Step [820/1248], Loss: 0.0173\n",
      "Epoch [18/100], Step [840/1248], Loss: 0.1495\n",
      "Epoch [18/100], Step [860/1248], Loss: 0.3956\n",
      "Epoch [18/100], Step [880/1248], Loss: 0.0713\n",
      "Epoch [18/100], Step [900/1248], Loss: 0.0834\n",
      "Epoch [18/100], Step [920/1248], Loss: 0.1171\n",
      "Epoch [18/100], Step [940/1248], Loss: 0.2193\n",
      "Epoch [18/100], Step [960/1248], Loss: 0.0474\n",
      "Epoch [18/100], Step [980/1248], Loss: 0.0208\n",
      "Epoch [18/100], Step [1000/1248], Loss: 0.0219\n",
      "Epoch [18/100], Step [1020/1248], Loss: 0.0535\n",
      "Epoch [18/100], Step [1040/1248], Loss: 0.1743\n",
      "Epoch [18/100], Step [1060/1248], Loss: 0.1443\n",
      "Epoch [18/100], Step [1080/1248], Loss: 0.1134\n",
      "Epoch [18/100], Step [1100/1248], Loss: 0.2389\n",
      "Epoch [18/100], Step [1120/1248], Loss: 0.0169\n",
      "Epoch [18/100], Step [1140/1248], Loss: 0.0064\n",
      "Epoch [18/100], Step [1160/1248], Loss: 0.0724\n",
      "Epoch [18/100], Step [1180/1248], Loss: 0.0905\n",
      "Epoch [18/100], Step [1200/1248], Loss: 0.0545\n",
      "Epoch [18/100], Step [1220/1248], Loss: 0.0391\n",
      "Epoch [18/100], Step [1240/1248], Loss: 0.0108\n",
      "\n",
      "train-loss: 0.3731, train-acc: 96.8487\n",
      "validation loss: 1.2797, validation acc: 68.2853\n",
      "\n",
      "Epoch 19\n",
      "\n",
      "Epoch [19/100], Step [0/1248], Loss: 0.2227\n",
      "Epoch [19/100], Step [20/1248], Loss: 0.0401\n",
      "Epoch [19/100], Step [40/1248], Loss: 0.0146\n",
      "Epoch [19/100], Step [60/1248], Loss: 0.1756\n",
      "Epoch [19/100], Step [80/1248], Loss: 0.0249\n",
      "Epoch [19/100], Step [100/1248], Loss: 0.0064\n",
      "Epoch [19/100], Step [120/1248], Loss: 0.1123\n",
      "Epoch [19/100], Step [140/1248], Loss: 0.0145\n",
      "Epoch [19/100], Step [160/1248], Loss: 0.0726\n",
      "Epoch [19/100], Step [180/1248], Loss: 0.1084\n",
      "Epoch [19/100], Step [200/1248], Loss: 0.0068\n",
      "Epoch [19/100], Step [220/1248], Loss: 0.0165\n",
      "Epoch [19/100], Step [240/1248], Loss: 0.0086\n",
      "Epoch [19/100], Step [260/1248], Loss: 0.0321\n",
      "Epoch [19/100], Step [280/1248], Loss: 0.0362\n",
      "Epoch [19/100], Step [300/1248], Loss: 0.0805\n",
      "Epoch [19/100], Step [320/1248], Loss: 0.0320\n",
      "Epoch [19/100], Step [340/1248], Loss: 0.0750\n",
      "Epoch [19/100], Step [360/1248], Loss: 0.0775\n",
      "Epoch [19/100], Step [380/1248], Loss: 0.0851\n",
      "Epoch [19/100], Step [400/1248], Loss: 0.2414\n",
      "Epoch [19/100], Step [420/1248], Loss: 0.0210\n",
      "Epoch [19/100], Step [440/1248], Loss: 0.0122\n",
      "Epoch [19/100], Step [460/1248], Loss: 0.0053\n",
      "Epoch [19/100], Step [480/1248], Loss: 0.0064\n",
      "Epoch [19/100], Step [500/1248], Loss: 0.0042\n",
      "Epoch [19/100], Step [520/1248], Loss: 0.0145\n",
      "Epoch [19/100], Step [540/1248], Loss: 0.0101\n",
      "Epoch [19/100], Step [560/1248], Loss: 0.0213\n",
      "Epoch [19/100], Step [580/1248], Loss: 0.0410\n",
      "Epoch [19/100], Step [600/1248], Loss: 0.1860\n",
      "Epoch [19/100], Step [620/1248], Loss: 0.2851\n",
      "Epoch [19/100], Step [640/1248], Loss: 0.0164\n",
      "Epoch [19/100], Step [660/1248], Loss: 0.0388\n",
      "Epoch [19/100], Step [680/1248], Loss: 0.0204\n",
      "Epoch [19/100], Step [700/1248], Loss: 0.0069\n",
      "Epoch [19/100], Step [720/1248], Loss: 0.0134\n",
      "Epoch [19/100], Step [740/1248], Loss: 0.0601\n",
      "Epoch [19/100], Step [760/1248], Loss: 0.1659\n",
      "Epoch [19/100], Step [780/1248], Loss: 0.0078\n",
      "Epoch [19/100], Step [800/1248], Loss: 0.0174\n",
      "Epoch [19/100], Step [820/1248], Loss: 0.0380\n",
      "Epoch [19/100], Step [840/1248], Loss: 0.0546\n",
      "Epoch [19/100], Step [860/1248], Loss: 0.1705\n",
      "Epoch [19/100], Step [880/1248], Loss: 0.1153\n",
      "Epoch [19/100], Step [900/1248], Loss: 0.3211\n",
      "Epoch [19/100], Step [920/1248], Loss: 0.0072\n",
      "Epoch [19/100], Step [940/1248], Loss: 0.0065\n",
      "Epoch [19/100], Step [960/1248], Loss: 0.0268\n",
      "Epoch [19/100], Step [980/1248], Loss: 0.0803\n",
      "Epoch [19/100], Step [1000/1248], Loss: 0.0172\n",
      "Epoch [19/100], Step [1020/1248], Loss: 0.3160\n",
      "Epoch [19/100], Step [1040/1248], Loss: 0.2220\n",
      "Epoch [19/100], Step [1060/1248], Loss: 0.0174\n",
      "Epoch [19/100], Step [1080/1248], Loss: 0.0361\n",
      "Epoch [19/100], Step [1100/1248], Loss: 0.1507\n",
      "Epoch [19/100], Step [1120/1248], Loss: 0.1564\n",
      "Epoch [19/100], Step [1140/1248], Loss: 0.0905\n",
      "Epoch [19/100], Step [1160/1248], Loss: 0.0640\n",
      "Epoch [19/100], Step [1180/1248], Loss: 0.3135\n",
      "Epoch [19/100], Step [1200/1248], Loss: 0.1336\n",
      "Epoch [19/100], Step [1220/1248], Loss: 0.1735\n",
      "Epoch [19/100], Step [1240/1248], Loss: 0.0408\n",
      "\n",
      "train-loss: 0.3574, train-acc: 97.4599\n",
      "validation loss: 1.3007, validation acc: 67.5645\n",
      "\n",
      "Epoch 20\n",
      "\n",
      "Epoch [20/100], Step [0/1248], Loss: 0.0223\n",
      "Epoch [20/100], Step [20/1248], Loss: 0.0099\n",
      "Epoch [20/100], Step [40/1248], Loss: 0.1588\n",
      "Epoch [20/100], Step [60/1248], Loss: 0.1010\n",
      "Epoch [20/100], Step [80/1248], Loss: 0.3653\n",
      "Epoch [20/100], Step [100/1248], Loss: 0.0339\n",
      "Epoch [20/100], Step [120/1248], Loss: 0.2895\n",
      "Epoch [20/100], Step [140/1248], Loss: 0.0021\n",
      "Epoch [20/100], Step [160/1248], Loss: 0.1688\n",
      "Epoch [20/100], Step [180/1248], Loss: 0.0276\n",
      "Epoch [20/100], Step [200/1248], Loss: 0.0053\n",
      "Epoch [20/100], Step [220/1248], Loss: 0.0041\n",
      "Epoch [20/100], Step [240/1248], Loss: 0.0062\n",
      "Epoch [20/100], Step [260/1248], Loss: 0.0571\n",
      "Epoch [20/100], Step [280/1248], Loss: 0.3606\n",
      "Epoch [20/100], Step [300/1248], Loss: 0.0043\n",
      "Epoch [20/100], Step [320/1248], Loss: 0.0671\n",
      "Epoch [20/100], Step [340/1248], Loss: 0.0351\n",
      "Epoch [20/100], Step [360/1248], Loss: 0.0289\n",
      "Epoch [20/100], Step [380/1248], Loss: 0.0249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Step [400/1248], Loss: 0.2018\n",
      "Epoch [20/100], Step [420/1248], Loss: 0.0947\n",
      "Epoch [20/100], Step [440/1248], Loss: 0.0217\n",
      "Epoch [20/100], Step [460/1248], Loss: 0.0817\n",
      "Epoch [20/100], Step [480/1248], Loss: 0.0545\n",
      "Epoch [20/100], Step [500/1248], Loss: 0.0911\n",
      "Epoch [20/100], Step [520/1248], Loss: 0.0220\n",
      "Epoch [20/100], Step [540/1248], Loss: 0.0121\n",
      "Epoch [20/100], Step [560/1248], Loss: 0.1391\n",
      "Epoch [20/100], Step [580/1248], Loss: 0.0169\n",
      "Epoch [20/100], Step [600/1248], Loss: 0.5820\n",
      "Epoch [20/100], Step [620/1248], Loss: 0.0153\n",
      "Epoch [20/100], Step [640/1248], Loss: 0.3029\n",
      "Epoch [20/100], Step [660/1248], Loss: 0.0080\n",
      "Epoch [20/100], Step [680/1248], Loss: 0.2506\n",
      "Epoch [20/100], Step [700/1248], Loss: 0.0187\n",
      "Epoch [20/100], Step [720/1248], Loss: 0.0899\n",
      "Epoch [20/100], Step [740/1248], Loss: 0.0181\n",
      "Epoch [20/100], Step [760/1248], Loss: 0.0198\n",
      "Epoch [20/100], Step [780/1248], Loss: 0.0162\n",
      "Epoch [20/100], Step [800/1248], Loss: 0.0297\n",
      "Epoch [20/100], Step [820/1248], Loss: 0.0042\n",
      "Epoch [20/100], Step [840/1248], Loss: 0.3521\n",
      "Epoch [20/100], Step [860/1248], Loss: 0.0165\n",
      "Epoch [20/100], Step [880/1248], Loss: 0.0155\n",
      "Epoch [20/100], Step [900/1248], Loss: 0.0312\n",
      "Epoch [20/100], Step [920/1248], Loss: 0.0665\n",
      "Epoch [20/100], Step [940/1248], Loss: 0.2087\n",
      "Epoch [20/100], Step [960/1248], Loss: 0.0610\n",
      "Epoch [20/100], Step [980/1248], Loss: 0.0020\n",
      "Epoch [20/100], Step [1000/1248], Loss: 0.0174\n",
      "Epoch [20/100], Step [1020/1248], Loss: 0.0983\n",
      "Epoch [20/100], Step [1040/1248], Loss: 0.0103\n",
      "Epoch [20/100], Step [1060/1248], Loss: 0.0268\n",
      "Epoch [20/100], Step [1080/1248], Loss: 0.0172\n",
      "Epoch [20/100], Step [1100/1248], Loss: 0.0193\n",
      "Epoch [20/100], Step [1120/1248], Loss: 0.0657\n",
      "Epoch [20/100], Step [1140/1248], Loss: 0.0112\n",
      "Epoch [20/100], Step [1160/1248], Loss: 0.0383\n",
      "Epoch [20/100], Step [1180/1248], Loss: 0.0050\n",
      "Epoch [20/100], Step [1200/1248], Loss: 0.0351\n",
      "Epoch [20/100], Step [1220/1248], Loss: 0.0218\n",
      "Epoch [20/100], Step [1240/1248], Loss: 0.0109\n",
      "\n",
      "train-loss: 0.3435, train-acc: 97.4449\n",
      "validation loss: 1.3126, validation acc: 67.3242\n",
      "\n",
      "Epoch 21\n",
      "\n",
      "Epoch [21/100], Step [0/1248], Loss: 0.0452\n",
      "Epoch [21/100], Step [20/1248], Loss: 0.0226\n",
      "Epoch [21/100], Step [40/1248], Loss: 0.0399\n",
      "Epoch [21/100], Step [60/1248], Loss: 0.0305\n",
      "Epoch [21/100], Step [80/1248], Loss: 0.1963\n",
      "Epoch [21/100], Step [100/1248], Loss: 0.0065\n",
      "Epoch [21/100], Step [120/1248], Loss: 0.0274\n",
      "Epoch [21/100], Step [140/1248], Loss: 0.0252\n",
      "Epoch [21/100], Step [160/1248], Loss: 0.0039\n",
      "Epoch [21/100], Step [180/1248], Loss: 0.4767\n",
      "Epoch [21/100], Step [200/1248], Loss: 0.0043\n",
      "Epoch [21/100], Step [220/1248], Loss: 0.0145\n",
      "Epoch [21/100], Step [240/1248], Loss: 0.0093\n",
      "Epoch [21/100], Step [260/1248], Loss: 0.0164\n",
      "Epoch [21/100], Step [280/1248], Loss: 0.0122\n",
      "Epoch [21/100], Step [300/1248], Loss: 0.0587\n",
      "Epoch [21/100], Step [320/1248], Loss: 0.0117\n",
      "Epoch [21/100], Step [340/1248], Loss: 0.0595\n",
      "Epoch [21/100], Step [360/1248], Loss: 0.0668\n",
      "Epoch [21/100], Step [380/1248], Loss: 0.0413\n",
      "Epoch [21/100], Step [400/1248], Loss: 0.0115\n",
      "Epoch [21/100], Step [420/1248], Loss: 0.0013\n",
      "Epoch [21/100], Step [440/1248], Loss: 0.0802\n",
      "Epoch [21/100], Step [460/1248], Loss: 0.1501\n",
      "Epoch [21/100], Step [480/1248], Loss: 0.1473\n",
      "Epoch [21/100], Step [500/1248], Loss: 0.0427\n",
      "Epoch [21/100], Step [520/1248], Loss: 0.1574\n",
      "Epoch [21/100], Step [540/1248], Loss: 0.0243\n",
      "Epoch [21/100], Step [560/1248], Loss: 0.0191\n",
      "Epoch [21/100], Step [580/1248], Loss: 0.0980\n",
      "Epoch [21/100], Step [600/1248], Loss: 0.0361\n",
      "Epoch [21/100], Step [620/1248], Loss: 0.0045\n",
      "Epoch [21/100], Step [640/1248], Loss: 0.0159\n",
      "Epoch [21/100], Step [660/1248], Loss: 0.1955\n",
      "Epoch [21/100], Step [680/1248], Loss: 0.0116\n",
      "Epoch [21/100], Step [700/1248], Loss: 0.1193\n",
      "Epoch [21/100], Step [720/1248], Loss: 0.2091\n",
      "Epoch [21/100], Step [740/1248], Loss: 0.2193\n",
      "Epoch [21/100], Step [760/1248], Loss: 0.3779\n",
      "Epoch [21/100], Step [780/1248], Loss: 0.0260\n",
      "Epoch [21/100], Step [800/1248], Loss: 0.0039\n",
      "Epoch [21/100], Step [820/1248], Loss: 0.0493\n",
      "Epoch [21/100], Step [840/1248], Loss: 0.0326\n",
      "Epoch [21/100], Step [860/1248], Loss: 0.0691\n",
      "Epoch [21/100], Step [880/1248], Loss: 0.0941\n",
      "Epoch [21/100], Step [900/1248], Loss: 0.1044\n",
      "Epoch [21/100], Step [920/1248], Loss: 0.0451\n",
      "Epoch [21/100], Step [940/1248], Loss: 0.0035\n",
      "Epoch [21/100], Step [960/1248], Loss: 0.0681\n",
      "Epoch [21/100], Step [980/1248], Loss: 0.0337\n",
      "Epoch [21/100], Step [1000/1248], Loss: 0.0599\n",
      "Epoch [21/100], Step [1020/1248], Loss: 0.0074\n",
      "Epoch [21/100], Step [1040/1248], Loss: 0.0230\n",
      "Epoch [21/100], Step [1060/1248], Loss: 0.0292\n",
      "Epoch [21/100], Step [1080/1248], Loss: 0.0052\n",
      "Epoch [21/100], Step [1100/1248], Loss: 0.2613\n",
      "Epoch [21/100], Step [1120/1248], Loss: 0.0699\n",
      "Epoch [21/100], Step [1140/1248], Loss: 0.0651\n",
      "Epoch [21/100], Step [1160/1248], Loss: 0.2923\n",
      "Epoch [21/100], Step [1180/1248], Loss: 0.1461\n",
      "Epoch [21/100], Step [1200/1248], Loss: 0.1850\n",
      "Epoch [21/100], Step [1220/1248], Loss: 0.0328\n",
      "Epoch [21/100], Step [1240/1248], Loss: 0.0683\n",
      "\n",
      "train-loss: 0.3308, train-acc: 97.4549\n",
      "validation loss: 1.3237, validation acc: 69.1452\n",
      "\n",
      "Epoch 22\n",
      "\n",
      "Epoch [22/100], Step [0/1248], Loss: 0.2886\n",
      "Epoch [22/100], Step [20/1248], Loss: 0.0016\n",
      "Epoch [22/100], Step [40/1248], Loss: 0.2545\n",
      "Epoch [22/100], Step [60/1248], Loss: 0.0565\n",
      "Epoch [22/100], Step [80/1248], Loss: 0.1068\n",
      "Epoch [22/100], Step [100/1248], Loss: 0.0973\n",
      "Epoch [22/100], Step [120/1248], Loss: 0.0252\n",
      "Epoch [22/100], Step [140/1248], Loss: 0.0228\n",
      "Epoch [22/100], Step [160/1248], Loss: 0.0251\n",
      "Epoch [22/100], Step [180/1248], Loss: 0.0023\n",
      "Epoch [22/100], Step [200/1248], Loss: 0.0024\n",
      "Epoch [22/100], Step [220/1248], Loss: 0.0333\n",
      "Epoch [22/100], Step [240/1248], Loss: 0.0024\n",
      "Epoch [22/100], Step [260/1248], Loss: 0.0200\n",
      "Epoch [22/100], Step [280/1248], Loss: 0.0052\n",
      "Epoch [22/100], Step [300/1248], Loss: 0.0069\n",
      "Epoch [22/100], Step [320/1248], Loss: 0.0538\n",
      "Epoch [22/100], Step [340/1248], Loss: 0.0922\n",
      "Epoch [22/100], Step [360/1248], Loss: 0.0467\n",
      "Epoch [22/100], Step [380/1248], Loss: 0.1044\n",
      "Epoch [22/100], Step [400/1248], Loss: 0.0248\n",
      "Epoch [22/100], Step [420/1248], Loss: 0.0229\n",
      "Epoch [22/100], Step [440/1248], Loss: 0.0087\n",
      "Epoch [22/100], Step [460/1248], Loss: 0.0782\n",
      "Epoch [22/100], Step [480/1248], Loss: 0.2403\n",
      "Epoch [22/100], Step [500/1248], Loss: 0.0450\n",
      "Epoch [22/100], Step [520/1248], Loss: 0.0042\n",
      "Epoch [22/100], Step [540/1248], Loss: 0.1696\n",
      "Epoch [22/100], Step [560/1248], Loss: 0.1574\n",
      "Epoch [22/100], Step [580/1248], Loss: 0.0451\n",
      "Epoch [22/100], Step [600/1248], Loss: 0.1662\n",
      "Epoch [22/100], Step [620/1248], Loss: 0.0703\n",
      "Epoch [22/100], Step [640/1248], Loss: 0.0221\n",
      "Epoch [22/100], Step [660/1248], Loss: 0.0100\n",
      "Epoch [22/100], Step [680/1248], Loss: 0.0284\n",
      "Epoch [22/100], Step [700/1248], Loss: 0.1031\n",
      "Epoch [22/100], Step [720/1248], Loss: 0.2517\n",
      "Epoch [22/100], Step [740/1248], Loss: 0.0106\n",
      "Epoch [22/100], Step [760/1248], Loss: 0.1435\n",
      "Epoch [22/100], Step [780/1248], Loss: 0.0836\n",
      "Epoch [22/100], Step [800/1248], Loss: 0.1315\n",
      "Epoch [22/100], Step [820/1248], Loss: 0.1115\n",
      "Epoch [22/100], Step [840/1248], Loss: 0.0186\n",
      "Epoch [22/100], Step [860/1248], Loss: 0.0072\n",
      "Epoch [22/100], Step [880/1248], Loss: 0.0443\n",
      "Epoch [22/100], Step [900/1248], Loss: 0.0958\n",
      "Epoch [22/100], Step [920/1248], Loss: 0.0122\n",
      "Epoch [22/100], Step [940/1248], Loss: 0.0076\n",
      "Epoch [22/100], Step [960/1248], Loss: 0.0585\n",
      "Epoch [22/100], Step [980/1248], Loss: 0.0361\n",
      "Epoch [22/100], Step [1000/1248], Loss: 0.0087\n",
      "Epoch [22/100], Step [1020/1248], Loss: 0.1099\n",
      "Epoch [22/100], Step [1040/1248], Loss: 0.3031\n",
      "Epoch [22/100], Step [1060/1248], Loss: 0.0560\n",
      "Epoch [22/100], Step [1080/1248], Loss: 0.0029\n",
      "Epoch [22/100], Step [1100/1248], Loss: 0.0761\n",
      "Epoch [22/100], Step [1120/1248], Loss: 0.1924\n",
      "Epoch [22/100], Step [1140/1248], Loss: 0.3143\n",
      "Epoch [22/100], Step [1160/1248], Loss: 0.0155\n",
      "Epoch [22/100], Step [1180/1248], Loss: 0.0592\n",
      "Epoch [22/100], Step [1200/1248], Loss: 0.0083\n",
      "Epoch [22/100], Step [1220/1248], Loss: 0.0181\n",
      "Epoch [22/100], Step [1240/1248], Loss: 0.0149\n",
      "\n",
      "train-loss: 0.3190, train-acc: 97.6102\n",
      "validation loss: 1.3446, validation acc: 67.2357\n",
      "\n",
      "Epoch 23\n",
      "\n",
      "Epoch [23/100], Step [0/1248], Loss: 0.0423\n",
      "Epoch [23/100], Step [20/1248], Loss: 0.1540\n",
      "Epoch [23/100], Step [40/1248], Loss: 0.0668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Step [60/1248], Loss: 0.0286\n",
      "Epoch [23/100], Step [80/1248], Loss: 0.0256\n",
      "Epoch [23/100], Step [100/1248], Loss: 0.0322\n",
      "Epoch [23/100], Step [120/1248], Loss: 0.0146\n",
      "Epoch [23/100], Step [140/1248], Loss: 0.0109\n",
      "Epoch [23/100], Step [160/1248], Loss: 0.0515\n",
      "Epoch [23/100], Step [180/1248], Loss: 0.0009\n",
      "Epoch [23/100], Step [200/1248], Loss: 0.0462\n",
      "Epoch [23/100], Step [220/1248], Loss: 0.0469\n",
      "Epoch [23/100], Step [240/1248], Loss: 0.0072\n",
      "Epoch [23/100], Step [260/1248], Loss: 0.0513\n",
      "Epoch [23/100], Step [280/1248], Loss: 0.0292\n",
      "Epoch [23/100], Step [300/1248], Loss: 0.0155\n",
      "Epoch [23/100], Step [320/1248], Loss: 0.0167\n",
      "Epoch [23/100], Step [340/1248], Loss: 0.0122\n",
      "Epoch [23/100], Step [360/1248], Loss: 0.0079\n",
      "Epoch [23/100], Step [380/1248], Loss: 0.0068\n",
      "Epoch [23/100], Step [400/1248], Loss: 0.0091\n",
      "Epoch [23/100], Step [420/1248], Loss: 0.0017\n",
      "Epoch [23/100], Step [440/1248], Loss: 0.0044\n",
      "Epoch [23/100], Step [460/1248], Loss: 0.0035\n",
      "Epoch [23/100], Step [480/1248], Loss: 0.1343\n",
      "Epoch [23/100], Step [500/1248], Loss: 0.0604\n",
      "Epoch [23/100], Step [520/1248], Loss: 0.1136\n",
      "Epoch [23/100], Step [540/1248], Loss: 0.0124\n",
      "Epoch [23/100], Step [560/1248], Loss: 0.0034\n",
      "Epoch [23/100], Step [580/1248], Loss: 0.0039\n",
      "Epoch [23/100], Step [600/1248], Loss: 0.0042\n",
      "Epoch [23/100], Step [620/1248], Loss: 0.0810\n",
      "Epoch [23/100], Step [640/1248], Loss: 0.0065\n",
      "Epoch [23/100], Step [660/1248], Loss: 0.3959\n",
      "Epoch [23/100], Step [680/1248], Loss: 0.0197\n",
      "Epoch [23/100], Step [700/1248], Loss: 0.0105\n",
      "Epoch [23/100], Step [720/1248], Loss: 0.0408\n",
      "Epoch [23/100], Step [740/1248], Loss: 0.0216\n",
      "Epoch [23/100], Step [760/1248], Loss: 0.0213\n",
      "Epoch [23/100], Step [780/1248], Loss: 0.0271\n",
      "Epoch [23/100], Step [800/1248], Loss: 0.0042\n",
      "Epoch [23/100], Step [820/1248], Loss: 0.0046\n",
      "Epoch [23/100], Step [840/1248], Loss: 0.0040\n",
      "Epoch [23/100], Step [860/1248], Loss: 0.0089\n",
      "Epoch [23/100], Step [880/1248], Loss: 0.2411\n",
      "Epoch [23/100], Step [900/1248], Loss: 0.0334\n",
      "Epoch [23/100], Step [920/1248], Loss: 0.1629\n",
      "Epoch [23/100], Step [940/1248], Loss: 0.0416\n",
      "Epoch [23/100], Step [960/1248], Loss: 0.0207\n",
      "Epoch [23/100], Step [980/1248], Loss: 0.0896\n",
      "Epoch [23/100], Step [1000/1248], Loss: 0.0222\n",
      "Epoch [23/100], Step [1020/1248], Loss: 0.0743\n",
      "Epoch [23/100], Step [1040/1248], Loss: 0.0051\n",
      "Epoch [23/100], Step [1060/1248], Loss: 0.2578\n",
      "Epoch [23/100], Step [1080/1248], Loss: 0.1231\n",
      "Epoch [23/100], Step [1100/1248], Loss: 0.0805\n",
      "Epoch [23/100], Step [1120/1248], Loss: 0.1141\n",
      "Epoch [23/100], Step [1140/1248], Loss: 0.0026\n",
      "Epoch [23/100], Step [1160/1248], Loss: 0.0155\n",
      "Epoch [23/100], Step [1180/1248], Loss: 0.0240\n",
      "Epoch [23/100], Step [1200/1248], Loss: 0.2872\n",
      "Epoch [23/100], Step [1220/1248], Loss: 0.1091\n",
      "Epoch [23/100], Step [1240/1248], Loss: 0.1183\n",
      "\n",
      "train-loss: 0.3082, train-acc: 97.9058\n",
      "validation loss: 1.3566, validation acc: 67.7668\n",
      "\n",
      "Epoch 24\n",
      "\n",
      "Epoch [24/100], Step [0/1248], Loss: 0.0221\n",
      "Epoch [24/100], Step [20/1248], Loss: 0.0030\n",
      "Epoch [24/100], Step [40/1248], Loss: 0.0022\n",
      "Epoch [24/100], Step [60/1248], Loss: 0.0219\n",
      "Epoch [24/100], Step [80/1248], Loss: 0.2420\n",
      "Epoch [24/100], Step [100/1248], Loss: 0.0581\n",
      "Epoch [24/100], Step [120/1248], Loss: 0.0057\n",
      "Epoch [24/100], Step [140/1248], Loss: 0.3028\n",
      "Epoch [24/100], Step [160/1248], Loss: 0.0070\n",
      "Epoch [24/100], Step [180/1248], Loss: 0.1058\n",
      "Epoch [24/100], Step [200/1248], Loss: 0.0154\n",
      "Epoch [24/100], Step [220/1248], Loss: 0.0020\n",
      "Epoch [24/100], Step [240/1248], Loss: 0.1111\n",
      "Epoch [24/100], Step [260/1248], Loss: 0.0134\n",
      "Epoch [24/100], Step [280/1248], Loss: 0.0131\n",
      "Epoch [24/100], Step [300/1248], Loss: 0.0064\n",
      "Epoch [24/100], Step [320/1248], Loss: 0.0037\n",
      "Epoch [24/100], Step [340/1248], Loss: 0.0222\n",
      "Epoch [24/100], Step [360/1248], Loss: 0.0838\n",
      "Epoch [24/100], Step [380/1248], Loss: 0.0032\n",
      "Epoch [24/100], Step [400/1248], Loss: 0.0363\n",
      "Epoch [24/100], Step [420/1248], Loss: 0.0243\n",
      "Epoch [24/100], Step [440/1248], Loss: 0.0343\n",
      "Epoch [24/100], Step [460/1248], Loss: 0.0827\n",
      "Epoch [24/100], Step [480/1248], Loss: 0.0288\n",
      "Epoch [24/100], Step [500/1248], Loss: 0.0092\n",
      "Epoch [24/100], Step [520/1248], Loss: 0.0313\n",
      "Epoch [24/100], Step [540/1248], Loss: 0.0128\n",
      "Epoch [24/100], Step [560/1248], Loss: 0.0334\n",
      "Epoch [24/100], Step [580/1248], Loss: 0.0645\n",
      "Epoch [24/100], Step [600/1248], Loss: 0.1204\n",
      "Epoch [24/100], Step [620/1248], Loss: 0.0012\n",
      "Epoch [24/100], Step [640/1248], Loss: 0.0037\n",
      "Epoch [24/100], Step [660/1248], Loss: 0.0017\n",
      "Epoch [24/100], Step [680/1248], Loss: 0.0092\n",
      "Epoch [24/100], Step [700/1248], Loss: 0.0016\n",
      "Epoch [24/100], Step [720/1248], Loss: 0.0089\n",
      "Epoch [24/100], Step [740/1248], Loss: 0.0968\n",
      "Epoch [24/100], Step [760/1248], Loss: 0.2957\n",
      "Epoch [24/100], Step [780/1248], Loss: 0.0238\n",
      "Epoch [24/100], Step [800/1248], Loss: 0.0273\n",
      "Epoch [24/100], Step [820/1248], Loss: 0.0038\n",
      "Epoch [24/100], Step [840/1248], Loss: 0.1821\n",
      "Epoch [24/100], Step [860/1248], Loss: 0.0040\n",
      "Epoch [24/100], Step [880/1248], Loss: 0.1158\n",
      "Epoch [24/100], Step [900/1248], Loss: 0.2415\n",
      "Epoch [24/100], Step [920/1248], Loss: 0.1912\n",
      "Epoch [24/100], Step [940/1248], Loss: 0.0277\n",
      "Epoch [24/100], Step [960/1248], Loss: 0.0704\n",
      "Epoch [24/100], Step [980/1248], Loss: 0.0138\n",
      "Epoch [24/100], Step [1000/1248], Loss: 0.0163\n",
      "Epoch [24/100], Step [1020/1248], Loss: 0.0519\n",
      "Epoch [24/100], Step [1040/1248], Loss: 0.0159\n",
      "Epoch [24/100], Step [1060/1248], Loss: 0.2905\n",
      "Epoch [24/100], Step [1080/1248], Loss: 0.0342\n",
      "Epoch [24/100], Step [1100/1248], Loss: 0.0226\n",
      "Epoch [24/100], Step [1120/1248], Loss: 0.0066\n",
      "Epoch [24/100], Step [1140/1248], Loss: 0.1246\n",
      "Epoch [24/100], Step [1160/1248], Loss: 0.0084\n",
      "Epoch [24/100], Step [1180/1248], Loss: 0.0135\n",
      "Epoch [24/100], Step [1200/1248], Loss: 0.0106\n",
      "Epoch [24/100], Step [1220/1248], Loss: 0.0457\n",
      "Epoch [24/100], Step [1240/1248], Loss: 0.0374\n",
      "\n",
      "train-loss: 0.2982, train-acc: 97.6603\n",
      "validation loss: 1.3706, validation acc: 67.6909\n",
      "\n",
      "Epoch 25\n",
      "\n",
      "Epoch [25/100], Step [0/1248], Loss: 0.0115\n",
      "Epoch [25/100], Step [20/1248], Loss: 0.1500\n",
      "Epoch [25/100], Step [40/1248], Loss: 0.2863\n",
      "Epoch [25/100], Step [60/1248], Loss: 0.0131\n",
      "Epoch [25/100], Step [80/1248], Loss: 0.0309\n",
      "Epoch [25/100], Step [100/1248], Loss: 0.0113\n",
      "Epoch [25/100], Step [120/1248], Loss: 0.0205\n",
      "Epoch [25/100], Step [140/1248], Loss: 0.0287\n",
      "Epoch [25/100], Step [160/1248], Loss: 0.0015\n",
      "Epoch [25/100], Step [180/1248], Loss: 0.2252\n",
      "Epoch [25/100], Step [200/1248], Loss: 0.0180\n",
      "Epoch [25/100], Step [220/1248], Loss: 0.2606\n",
      "Epoch [25/100], Step [240/1248], Loss: 0.0720\n",
      "Epoch [25/100], Step [260/1248], Loss: 0.0025\n",
      "Epoch [25/100], Step [280/1248], Loss: 0.2135\n",
      "Epoch [25/100], Step [300/1248], Loss: 0.0270\n",
      "Epoch [25/100], Step [320/1248], Loss: 0.0102\n",
      "Epoch [25/100], Step [340/1248], Loss: 0.1134\n",
      "Epoch [25/100], Step [360/1248], Loss: 0.0045\n",
      "Epoch [25/100], Step [380/1248], Loss: 0.0250\n",
      "Epoch [25/100], Step [400/1248], Loss: 0.0281\n",
      "Epoch [25/100], Step [420/1248], Loss: 0.0130\n",
      "Epoch [25/100], Step [440/1248], Loss: 0.0939\n",
      "Epoch [25/100], Step [460/1248], Loss: 0.0025\n",
      "Epoch [25/100], Step [480/1248], Loss: 0.0039\n",
      "Epoch [25/100], Step [500/1248], Loss: 0.0198\n",
      "Epoch [25/100], Step [520/1248], Loss: 0.0050\n",
      "Epoch [25/100], Step [540/1248], Loss: 0.0222\n",
      "Epoch [25/100], Step [560/1248], Loss: 0.0037\n",
      "Epoch [25/100], Step [580/1248], Loss: 0.0050\n",
      "Epoch [25/100], Step [600/1248], Loss: 0.0213\n",
      "Epoch [25/100], Step [620/1248], Loss: 0.0065\n",
      "Epoch [25/100], Step [640/1248], Loss: 0.0830\n",
      "Epoch [25/100], Step [660/1248], Loss: 0.0074\n",
      "Epoch [25/100], Step [680/1248], Loss: 0.0328\n",
      "Epoch [25/100], Step [700/1248], Loss: 0.0068\n",
      "Epoch [25/100], Step [720/1248], Loss: 0.0159\n",
      "Epoch [25/100], Step [740/1248], Loss: 0.0044\n",
      "Epoch [25/100], Step [760/1248], Loss: 0.0329\n",
      "Epoch [25/100], Step [780/1248], Loss: 0.0497\n",
      "Epoch [25/100], Step [800/1248], Loss: 0.0314\n",
      "Epoch [25/100], Step [820/1248], Loss: 0.0038\n",
      "Epoch [25/100], Step [840/1248], Loss: 0.1233\n",
      "Epoch [25/100], Step [860/1248], Loss: 0.0089\n",
      "Epoch [25/100], Step [880/1248], Loss: 0.0210\n",
      "Epoch [25/100], Step [900/1248], Loss: 0.0155\n",
      "Epoch [25/100], Step [920/1248], Loss: 0.0078\n",
      "Epoch [25/100], Step [940/1248], Loss: 0.0397\n",
      "Epoch [25/100], Step [960/1248], Loss: 0.0399\n",
      "Epoch [25/100], Step [980/1248], Loss: 0.0078\n",
      "Epoch [25/100], Step [1000/1248], Loss: 0.0120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Step [1020/1248], Loss: 0.2896\n",
      "Epoch [25/100], Step [1040/1248], Loss: 0.1237\n",
      "Epoch [25/100], Step [1060/1248], Loss: 0.0028\n",
      "Epoch [25/100], Step [1080/1248], Loss: 0.2107\n",
      "Epoch [25/100], Step [1100/1248], Loss: 0.1267\n",
      "Epoch [25/100], Step [1120/1248], Loss: 0.2550\n",
      "Epoch [25/100], Step [1140/1248], Loss: 0.1680\n",
      "Epoch [25/100], Step [1160/1248], Loss: 0.0308\n",
      "Epoch [25/100], Step [1180/1248], Loss: 0.0033\n",
      "Epoch [25/100], Step [1200/1248], Loss: 0.0272\n",
      "Epoch [25/100], Step [1220/1248], Loss: 0.3918\n",
      "Epoch [25/100], Step [1240/1248], Loss: 0.0082\n",
      "\n",
      "train-loss: 0.2886, train-acc: 98.1964\n",
      "validation loss: 1.3843, validation acc: 67.7795\n",
      "\n",
      "Epoch 26\n",
      "\n",
      "Epoch [26/100], Step [0/1248], Loss: 0.0034\n",
      "Epoch [26/100], Step [20/1248], Loss: 0.0648\n",
      "Epoch [26/100], Step [40/1248], Loss: 0.1236\n",
      "Epoch [26/100], Step [60/1248], Loss: 0.0122\n",
      "Epoch [26/100], Step [80/1248], Loss: 0.0297\n",
      "Epoch [26/100], Step [100/1248], Loss: 0.0291\n",
      "Epoch [26/100], Step [120/1248], Loss: 0.0019\n",
      "Epoch [26/100], Step [140/1248], Loss: 0.0009\n",
      "Epoch [26/100], Step [160/1248], Loss: 0.0192\n",
      "Epoch [26/100], Step [180/1248], Loss: 0.0526\n",
      "Epoch [26/100], Step [200/1248], Loss: 0.0056\n",
      "Epoch [26/100], Step [220/1248], Loss: 0.0067\n",
      "Epoch [26/100], Step [240/1248], Loss: 0.0045\n",
      "Epoch [26/100], Step [260/1248], Loss: 0.2977\n",
      "Epoch [26/100], Step [280/1248], Loss: 0.0030\n",
      "Epoch [26/100], Step [300/1248], Loss: 0.0098\n",
      "Epoch [26/100], Step [320/1248], Loss: 0.0066\n",
      "Epoch [26/100], Step [340/1248], Loss: 0.0213\n",
      "Epoch [26/100], Step [360/1248], Loss: 0.1686\n",
      "Epoch [26/100], Step [380/1248], Loss: 0.0237\n",
      "Epoch [26/100], Step [400/1248], Loss: 0.0462\n",
      "Epoch [26/100], Step [420/1248], Loss: 0.0017\n",
      "Epoch [26/100], Step [440/1248], Loss: 0.0105\n",
      "Epoch [26/100], Step [460/1248], Loss: 0.0029\n",
      "Epoch [26/100], Step [480/1248], Loss: 0.1400\n",
      "Epoch [26/100], Step [500/1248], Loss: 0.0628\n",
      "Epoch [26/100], Step [520/1248], Loss: 0.0088\n",
      "Epoch [26/100], Step [540/1248], Loss: 0.1054\n",
      "Epoch [26/100], Step [560/1248], Loss: 0.0510\n",
      "Epoch [26/100], Step [580/1248], Loss: 0.0322\n",
      "Epoch [26/100], Step [600/1248], Loss: 0.1260\n",
      "Epoch [26/100], Step [620/1248], Loss: 0.0659\n",
      "Epoch [26/100], Step [640/1248], Loss: 0.0574\n",
      "Epoch [26/100], Step [660/1248], Loss: 0.1478\n",
      "Epoch [26/100], Step [680/1248], Loss: 0.0101\n",
      "Epoch [26/100], Step [700/1248], Loss: 0.6876\n",
      "Epoch [26/100], Step [720/1248], Loss: 0.0474\n",
      "Epoch [26/100], Step [740/1248], Loss: 0.0640\n",
      "Epoch [26/100], Step [760/1248], Loss: 0.0280\n",
      "Epoch [26/100], Step [780/1248], Loss: 0.0361\n",
      "Epoch [26/100], Step [800/1248], Loss: 0.0402\n",
      "Epoch [26/100], Step [820/1248], Loss: 0.1857\n",
      "Epoch [26/100], Step [840/1248], Loss: 0.0111\n",
      "Epoch [26/100], Step [860/1248], Loss: 0.0113\n",
      "Epoch [26/100], Step [880/1248], Loss: 0.0045\n",
      "Epoch [26/100], Step [900/1248], Loss: 0.1069\n",
      "Epoch [26/100], Step [920/1248], Loss: 0.0012\n",
      "Epoch [26/100], Step [940/1248], Loss: 0.0218\n",
      "Epoch [26/100], Step [960/1248], Loss: 0.0064\n",
      "Epoch [26/100], Step [980/1248], Loss: 0.0400\n",
      "Epoch [26/100], Step [1000/1248], Loss: 0.0118\n",
      "Epoch [26/100], Step [1020/1248], Loss: 0.1144\n",
      "Epoch [26/100], Step [1040/1248], Loss: 0.0041\n",
      "Epoch [26/100], Step [1060/1248], Loss: 0.0167\n",
      "Epoch [26/100], Step [1080/1248], Loss: 0.0053\n",
      "Epoch [26/100], Step [1100/1248], Loss: 0.0510\n",
      "Epoch [26/100], Step [1120/1248], Loss: 0.0645\n",
      "Epoch [26/100], Step [1140/1248], Loss: 0.0258\n",
      "Epoch [26/100], Step [1160/1248], Loss: 0.0014\n",
      "Epoch [26/100], Step [1180/1248], Loss: 0.0124\n",
      "Epoch [26/100], Step [1200/1248], Loss: 0.0695\n",
      "Epoch [26/100], Step [1220/1248], Loss: 0.0469\n",
      "Epoch [26/100], Step [1240/1248], Loss: 0.0049\n",
      "\n",
      "train-loss: 0.2798, train-acc: 97.9910\n",
      "validation loss: 1.3989, validation acc: 68.2979\n",
      "\n",
      "Epoch 27\n",
      "\n",
      "Epoch [27/100], Step [0/1248], Loss: 0.0201\n",
      "Epoch [27/100], Step [20/1248], Loss: 0.0018\n",
      "Epoch [27/100], Step [40/1248], Loss: 0.0689\n",
      "Epoch [27/100], Step [60/1248], Loss: 0.0205\n",
      "Epoch [27/100], Step [80/1248], Loss: 0.0620\n",
      "Epoch [27/100], Step [100/1248], Loss: 0.0044\n",
      "Epoch [27/100], Step [120/1248], Loss: 0.0221\n",
      "Epoch [27/100], Step [140/1248], Loss: 0.0457\n",
      "Epoch [27/100], Step [160/1248], Loss: 0.0231\n",
      "Epoch [27/100], Step [180/1248], Loss: 0.0930\n",
      "Epoch [27/100], Step [200/1248], Loss: 0.0217\n",
      "Epoch [27/100], Step [220/1248], Loss: 0.0066\n",
      "Epoch [27/100], Step [240/1248], Loss: 0.2409\n",
      "Epoch [27/100], Step [260/1248], Loss: 0.4637\n",
      "Epoch [27/100], Step [280/1248], Loss: 0.0881\n",
      "Epoch [27/100], Step [300/1248], Loss: 0.0161\n",
      "Epoch [27/100], Step [320/1248], Loss: 0.2187\n",
      "Epoch [27/100], Step [340/1248], Loss: 0.0090\n",
      "Epoch [27/100], Step [360/1248], Loss: 0.1095\n",
      "Epoch [27/100], Step [380/1248], Loss: 0.1498\n",
      "Epoch [27/100], Step [400/1248], Loss: 0.0185\n",
      "Epoch [27/100], Step [420/1248], Loss: 0.0254\n",
      "Epoch [27/100], Step [440/1248], Loss: 0.0307\n",
      "Epoch [27/100], Step [460/1248], Loss: 0.4850\n",
      "Epoch [27/100], Step [480/1248], Loss: 0.3761\n",
      "Epoch [27/100], Step [500/1248], Loss: 0.0103\n",
      "Epoch [27/100], Step [520/1248], Loss: 0.0075\n",
      "Epoch [27/100], Step [540/1248], Loss: 0.0753\n",
      "Epoch [27/100], Step [560/1248], Loss: 0.0727\n",
      "Epoch [27/100], Step [580/1248], Loss: 0.0995\n",
      "Epoch [27/100], Step [600/1248], Loss: 0.2160\n",
      "Epoch [27/100], Step [620/1248], Loss: 0.0255\n",
      "Epoch [27/100], Step [640/1248], Loss: 0.0248\n",
      "Epoch [27/100], Step [660/1248], Loss: 0.1654\n",
      "Epoch [27/100], Step [680/1248], Loss: 0.0054\n",
      "Epoch [27/100], Step [700/1248], Loss: 0.1894\n",
      "Epoch [27/100], Step [720/1248], Loss: 0.0046\n",
      "Epoch [27/100], Step [740/1248], Loss: 0.0606\n",
      "Epoch [27/100], Step [760/1248], Loss: 0.0095\n",
      "Epoch [27/100], Step [780/1248], Loss: 0.0095\n",
      "Epoch [27/100], Step [800/1248], Loss: 0.1096\n",
      "Epoch [27/100], Step [820/1248], Loss: 0.1023\n",
      "Epoch [27/100], Step [840/1248], Loss: 0.0023\n",
      "Epoch [27/100], Step [860/1248], Loss: 0.0019\n",
      "Epoch [27/100], Step [880/1248], Loss: 0.0585\n",
      "Epoch [27/100], Step [900/1248], Loss: 0.2407\n",
      "Epoch [27/100], Step [920/1248], Loss: 0.0036\n",
      "Epoch [27/100], Step [940/1248], Loss: 0.0561\n",
      "Epoch [27/100], Step [960/1248], Loss: 0.6071\n",
      "Epoch [27/100], Step [980/1248], Loss: 0.0078\n",
      "Epoch [27/100], Step [1000/1248], Loss: 0.2178\n",
      "Epoch [27/100], Step [1020/1248], Loss: 0.0659\n",
      "Epoch [27/100], Step [1040/1248], Loss: 0.0020\n",
      "Epoch [27/100], Step [1060/1248], Loss: 0.0158\n",
      "Epoch [27/100], Step [1080/1248], Loss: 0.0079\n",
      "Epoch [27/100], Step [1100/1248], Loss: 0.0168\n",
      "Epoch [27/100], Step [1120/1248], Loss: 0.3059\n",
      "Epoch [27/100], Step [1140/1248], Loss: 0.0323\n",
      "Epoch [27/100], Step [1160/1248], Loss: 0.0058\n",
      "Epoch [27/100], Step [1180/1248], Loss: 0.0746\n",
      "Epoch [27/100], Step [1200/1248], Loss: 0.0146\n",
      "Epoch [27/100], Step [1220/1248], Loss: 0.2458\n",
      "Epoch [27/100], Step [1240/1248], Loss: 0.7491\n",
      "\n",
      "train-loss: 0.2720, train-acc: 97.6503\n",
      "validation loss: 1.4087, validation acc: 68.5508\n",
      "\n",
      "Epoch 28\n",
      "\n",
      "Epoch [28/100], Step [0/1248], Loss: 0.0006\n",
      "Epoch [28/100], Step [20/1248], Loss: 0.0083\n",
      "Epoch [28/100], Step [40/1248], Loss: 0.0031\n",
      "Epoch [28/100], Step [60/1248], Loss: 0.0869\n",
      "Epoch [28/100], Step [80/1248], Loss: 0.0481\n",
      "Epoch [28/100], Step [100/1248], Loss: 0.0485\n",
      "Epoch [28/100], Step [120/1248], Loss: 0.0073\n",
      "Epoch [28/100], Step [140/1248], Loss: 0.0252\n",
      "Epoch [28/100], Step [160/1248], Loss: 0.0218\n",
      "Epoch [28/100], Step [180/1248], Loss: 0.0792\n",
      "Epoch [28/100], Step [200/1248], Loss: 0.0216\n",
      "Epoch [28/100], Step [220/1248], Loss: 0.0007\n",
      "Epoch [28/100], Step [240/1248], Loss: 0.0654\n",
      "Epoch [28/100], Step [260/1248], Loss: 0.0392\n",
      "Epoch [28/100], Step [280/1248], Loss: 0.0219\n",
      "Epoch [28/100], Step [300/1248], Loss: 0.0151\n",
      "Epoch [28/100], Step [320/1248], Loss: 0.0014\n",
      "Epoch [28/100], Step [340/1248], Loss: 0.0089\n",
      "Epoch [28/100], Step [360/1248], Loss: 0.0002\n",
      "Epoch [28/100], Step [380/1248], Loss: 0.1825\n",
      "Epoch [28/100], Step [400/1248], Loss: 0.0002\n",
      "Epoch [28/100], Step [420/1248], Loss: 0.0340\n",
      "Epoch [28/100], Step [440/1248], Loss: 0.0253\n",
      "Epoch [28/100], Step [460/1248], Loss: 0.0248\n",
      "Epoch [28/100], Step [480/1248], Loss: 0.0020\n",
      "Epoch [28/100], Step [500/1248], Loss: 0.1263\n",
      "Epoch [28/100], Step [520/1248], Loss: 0.0282\n",
      "Epoch [28/100], Step [540/1248], Loss: 0.0069\n",
      "Epoch [28/100], Step [560/1248], Loss: 0.0166\n",
      "Epoch [28/100], Step [580/1248], Loss: 0.0617\n",
      "Epoch [28/100], Step [600/1248], Loss: 0.0011\n",
      "Epoch [28/100], Step [620/1248], Loss: 0.0259\n",
      "Epoch [28/100], Step [640/1248], Loss: 0.0128\n",
      "Epoch [28/100], Step [660/1248], Loss: 0.0974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Step [680/1248], Loss: 0.0630\n",
      "Epoch [28/100], Step [700/1248], Loss: 0.0011\n",
      "Epoch [28/100], Step [720/1248], Loss: 0.0097\n",
      "Epoch [28/100], Step [740/1248], Loss: 0.1682\n",
      "Epoch [28/100], Step [760/1248], Loss: 0.0645\n",
      "Epoch [28/100], Step [780/1248], Loss: 0.1288\n",
      "Epoch [28/100], Step [800/1248], Loss: 0.0097\n",
      "Epoch [28/100], Step [820/1248], Loss: 0.0920\n",
      "Epoch [28/100], Step [840/1248], Loss: 0.0207\n",
      "Epoch [28/100], Step [860/1248], Loss: 0.0357\n",
      "Epoch [28/100], Step [880/1248], Loss: 0.1624\n",
      "Epoch [28/100], Step [900/1248], Loss: 0.0602\n",
      "Epoch [28/100], Step [920/1248], Loss: 0.0019\n",
      "Epoch [28/100], Step [940/1248], Loss: 0.0250\n",
      "Epoch [28/100], Step [960/1248], Loss: 0.0046\n",
      "Epoch [28/100], Step [980/1248], Loss: 0.0417\n",
      "Epoch [28/100], Step [1000/1248], Loss: 0.0857\n",
      "Epoch [28/100], Step [1020/1248], Loss: 0.0353\n",
      "Epoch [28/100], Step [1040/1248], Loss: 0.5467\n",
      "Epoch [28/100], Step [1060/1248], Loss: 0.4233\n",
      "Epoch [28/100], Step [1080/1248], Loss: 0.0094\n",
      "Epoch [28/100], Step [1100/1248], Loss: 0.0266\n",
      "Epoch [28/100], Step [1120/1248], Loss: 0.0455\n",
      "Epoch [28/100], Step [1140/1248], Loss: 0.0153\n",
      "Epoch [28/100], Step [1160/1248], Loss: 0.0457\n",
      "Epoch [28/100], Step [1180/1248], Loss: 0.0498\n",
      "Epoch [28/100], Step [1200/1248], Loss: 0.0421\n",
      "Epoch [28/100], Step [1220/1248], Loss: 0.0312\n",
      "Epoch [28/100], Step [1240/1248], Loss: 0.0070\n",
      "\n",
      "train-loss: 0.2642, train-acc: 98.2966\n",
      "validation loss: 1.4183, validation acc: 68.0450\n",
      "\n",
      "Epoch 29\n",
      "\n",
      "Epoch [29/100], Step [0/1248], Loss: 0.0170\n",
      "Epoch [29/100], Step [20/1248], Loss: 0.0190\n",
      "Epoch [29/100], Step [40/1248], Loss: 0.0761\n",
      "Epoch [29/100], Step [60/1248], Loss: 0.3218\n",
      "Epoch [29/100], Step [80/1248], Loss: 0.0044\n",
      "Epoch [29/100], Step [100/1248], Loss: 0.0052\n",
      "Epoch [29/100], Step [120/1248], Loss: 0.0276\n",
      "Epoch [29/100], Step [140/1248], Loss: 0.0625\n",
      "Epoch [29/100], Step [160/1248], Loss: 0.0169\n",
      "Epoch [29/100], Step [180/1248], Loss: 0.1156\n",
      "Epoch [29/100], Step [200/1248], Loss: 0.0814\n",
      "Epoch [29/100], Step [220/1248], Loss: 0.0263\n",
      "Epoch [29/100], Step [240/1248], Loss: 0.0459\n",
      "Epoch [29/100], Step [260/1248], Loss: 0.3112\n",
      "Epoch [29/100], Step [280/1248], Loss: 0.0566\n",
      "Epoch [29/100], Step [300/1248], Loss: 0.0042\n",
      "Epoch [29/100], Step [320/1248], Loss: 0.0047\n",
      "Epoch [29/100], Step [340/1248], Loss: 0.2137\n",
      "Epoch [29/100], Step [360/1248], Loss: 0.0056\n",
      "Epoch [29/100], Step [380/1248], Loss: 0.0147\n",
      "Epoch [29/100], Step [400/1248], Loss: 0.0846\n",
      "Epoch [29/100], Step [420/1248], Loss: 0.0475\n",
      "Epoch [29/100], Step [440/1248], Loss: 0.1839\n",
      "Epoch [29/100], Step [460/1248], Loss: 0.0163\n",
      "Epoch [29/100], Step [480/1248], Loss: 0.0078\n",
      "Epoch [29/100], Step [500/1248], Loss: 0.0174\n",
      "Epoch [29/100], Step [520/1248], Loss: 0.0033\n",
      "Epoch [29/100], Step [540/1248], Loss: 0.0982\n",
      "Epoch [29/100], Step [560/1248], Loss: 0.0408\n",
      "Epoch [29/100], Step [580/1248], Loss: 0.0186\n",
      "Epoch [29/100], Step [600/1248], Loss: 0.0643\n",
      "Epoch [29/100], Step [620/1248], Loss: 0.0658\n",
      "Epoch [29/100], Step [640/1248], Loss: 0.0107\n",
      "Epoch [29/100], Step [660/1248], Loss: 0.1492\n",
      "Epoch [29/100], Step [680/1248], Loss: 0.0011\n",
      "Epoch [29/100], Step [700/1248], Loss: 0.1074\n",
      "Epoch [29/100], Step [720/1248], Loss: 0.0139\n",
      "Epoch [29/100], Step [740/1248], Loss: 0.1797\n",
      "Epoch [29/100], Step [760/1248], Loss: 0.0148\n",
      "Epoch [29/100], Step [780/1248], Loss: 0.0006\n",
      "Epoch [29/100], Step [800/1248], Loss: 0.0131\n",
      "Epoch [29/100], Step [820/1248], Loss: 0.0294\n",
      "Epoch [29/100], Step [840/1248], Loss: 0.0460\n",
      "Epoch [29/100], Step [860/1248], Loss: 0.0265\n",
      "Epoch [29/100], Step [880/1248], Loss: 0.0448\n",
      "Epoch [29/100], Step [900/1248], Loss: 0.0058\n",
      "Epoch [29/100], Step [920/1248], Loss: 0.0598\n",
      "Epoch [29/100], Step [940/1248], Loss: 0.0792\n",
      "Epoch [29/100], Step [960/1248], Loss: 0.0104\n",
      "Epoch [29/100], Step [980/1248], Loss: 0.0374\n",
      "Epoch [29/100], Step [1000/1248], Loss: 0.0267\n",
      "Epoch [29/100], Step [1020/1248], Loss: 0.0610\n",
      "Epoch [29/100], Step [1040/1248], Loss: 0.0328\n",
      "Epoch [29/100], Step [1060/1248], Loss: 0.1746\n",
      "Epoch [29/100], Step [1080/1248], Loss: 0.0190\n",
      "Epoch [29/100], Step [1100/1248], Loss: 0.0330\n",
      "Epoch [29/100], Step [1120/1248], Loss: 0.0506\n",
      "Epoch [29/100], Step [1140/1248], Loss: 0.0012\n",
      "Epoch [29/100], Step [1160/1248], Loss: 0.2105\n",
      "Epoch [29/100], Step [1180/1248], Loss: 0.0181\n",
      "Epoch [29/100], Step [1200/1248], Loss: 0.0290\n",
      "Epoch [29/100], Step [1220/1248], Loss: 0.0021\n",
      "Epoch [29/100], Step [1240/1248], Loss: 0.0164\n",
      "\n",
      "train-loss: 0.2569, train-acc: 98.1263\n",
      "validation loss: 1.4310, validation acc: 67.9059\n",
      "\n",
      "Epoch 30\n",
      "\n",
      "Epoch [30/100], Step [0/1248], Loss: 0.0018\n",
      "Epoch [30/100], Step [20/1248], Loss: 0.0026\n",
      "Epoch [30/100], Step [40/1248], Loss: 0.0955\n",
      "Epoch [30/100], Step [60/1248], Loss: 0.0082\n",
      "Epoch [30/100], Step [80/1248], Loss: 0.0149\n",
      "Epoch [30/100], Step [100/1248], Loss: 0.0458\n",
      "Epoch [30/100], Step [120/1248], Loss: 0.0028\n",
      "Epoch [30/100], Step [140/1248], Loss: 0.0272\n",
      "Epoch [30/100], Step [160/1248], Loss: 0.1103\n",
      "Epoch [30/100], Step [180/1248], Loss: 0.0017\n",
      "Epoch [30/100], Step [200/1248], Loss: 0.0509\n",
      "Epoch [30/100], Step [220/1248], Loss: 0.3494\n",
      "Epoch [30/100], Step [240/1248], Loss: 0.0392\n",
      "Epoch [30/100], Step [260/1248], Loss: 0.0112\n",
      "Epoch [30/100], Step [280/1248], Loss: 0.1591\n",
      "Epoch [30/100], Step [300/1248], Loss: 0.0570\n",
      "Epoch [30/100], Step [320/1248], Loss: 0.0256\n",
      "Epoch [30/100], Step [340/1248], Loss: 0.0041\n",
      "Epoch [30/100], Step [360/1248], Loss: 0.0131\n",
      "Epoch [30/100], Step [380/1248], Loss: 0.0333\n",
      "Epoch [30/100], Step [400/1248], Loss: 0.0033\n",
      "Epoch [30/100], Step [420/1248], Loss: 0.0122\n",
      "Epoch [30/100], Step [440/1248], Loss: 0.3213\n",
      "Epoch [30/100], Step [460/1248], Loss: 0.0115\n",
      "Epoch [30/100], Step [480/1248], Loss: 0.0146\n",
      "Epoch [30/100], Step [500/1248], Loss: 0.0085\n",
      "Epoch [30/100], Step [520/1248], Loss: 0.0228\n",
      "Epoch [30/100], Step [540/1248], Loss: 0.1807\n",
      "Epoch [30/100], Step [560/1248], Loss: 0.0263\n",
      "Epoch [30/100], Step [580/1248], Loss: 0.0706\n",
      "Epoch [30/100], Step [600/1248], Loss: 0.0815\n",
      "Epoch [30/100], Step [620/1248], Loss: 0.0286\n",
      "Epoch [30/100], Step [640/1248], Loss: 0.0156\n",
      "Epoch [30/100], Step [660/1248], Loss: 0.0227\n",
      "Epoch [30/100], Step [680/1248], Loss: 0.0153\n",
      "Epoch [30/100], Step [700/1248], Loss: 0.0056\n",
      "Epoch [30/100], Step [720/1248], Loss: 0.1236\n",
      "Epoch [30/100], Step [740/1248], Loss: 0.0638\n",
      "Epoch [30/100], Step [760/1248], Loss: 0.0024\n",
      "Epoch [30/100], Step [780/1248], Loss: 0.1354\n",
      "Epoch [30/100], Step [800/1248], Loss: 0.0370\n",
      "Epoch [30/100], Step [820/1248], Loss: 0.0512\n",
      "Epoch [30/100], Step [840/1248], Loss: 0.0225\n",
      "Epoch [30/100], Step [860/1248], Loss: 0.0204\n",
      "Epoch [30/100], Step [880/1248], Loss: 0.0126\n",
      "Epoch [30/100], Step [900/1248], Loss: 0.0104\n",
      "Epoch [30/100], Step [920/1248], Loss: 0.0013\n",
      "Epoch [30/100], Step [940/1248], Loss: 0.0016\n",
      "Epoch [30/100], Step [960/1248], Loss: 0.0014\n",
      "Epoch [30/100], Step [980/1248], Loss: 0.0476\n",
      "Epoch [30/100], Step [1000/1248], Loss: 0.0044\n",
      "Epoch [30/100], Step [1020/1248], Loss: 0.2365\n",
      "Epoch [30/100], Step [1040/1248], Loss: 0.0238\n",
      "Epoch [30/100], Step [1060/1248], Loss: 0.0929\n",
      "Epoch [30/100], Step [1080/1248], Loss: 0.0851\n",
      "Epoch [30/100], Step [1100/1248], Loss: 0.0011\n",
      "Epoch [30/100], Step [1120/1248], Loss: 0.0164\n",
      "Epoch [30/100], Step [1140/1248], Loss: 0.2423\n",
      "Epoch [30/100], Step [1160/1248], Loss: 0.0063\n",
      "Epoch [30/100], Step [1180/1248], Loss: 0.1158\n",
      "Epoch [30/100], Step [1200/1248], Loss: 0.3753\n",
      "Epoch [30/100], Step [1220/1248], Loss: 0.1649\n",
      "Epoch [30/100], Step [1240/1248], Loss: 0.0624\n",
      "\n",
      "train-loss: 0.2504, train-acc: 98.0110\n",
      "validation loss: 1.4417, validation acc: 67.5392\n",
      "\n",
      "Epoch 31\n",
      "\n",
      "Epoch [31/100], Step [0/1248], Loss: 0.0124\n",
      "Epoch [31/100], Step [20/1248], Loss: 0.1071\n",
      "Epoch [31/100], Step [40/1248], Loss: 0.0594\n",
      "Epoch [31/100], Step [60/1248], Loss: 0.0073\n",
      "Epoch [31/100], Step [80/1248], Loss: 0.0200\n",
      "Epoch [31/100], Step [100/1248], Loss: 0.0023\n",
      "Epoch [31/100], Step [120/1248], Loss: 0.0026\n",
      "Epoch [31/100], Step [140/1248], Loss: 0.0300\n",
      "Epoch [31/100], Step [160/1248], Loss: 0.0035\n",
      "Epoch [31/100], Step [180/1248], Loss: 0.0330\n",
      "Epoch [31/100], Step [200/1248], Loss: 0.0003\n",
      "Epoch [31/100], Step [220/1248], Loss: 0.0028\n",
      "Epoch [31/100], Step [240/1248], Loss: 0.0040\n",
      "Epoch [31/100], Step [260/1248], Loss: 0.0046\n",
      "Epoch [31/100], Step [280/1248], Loss: 0.0916\n",
      "Epoch [31/100], Step [300/1248], Loss: 0.0045\n",
      "Epoch [31/100], Step [320/1248], Loss: 0.0387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Step [340/1248], Loss: 0.0037\n",
      "Epoch [31/100], Step [360/1248], Loss: 0.0069\n",
      "Epoch [31/100], Step [380/1248], Loss: 0.0013\n",
      "Epoch [31/100], Step [400/1248], Loss: 0.0047\n",
      "Epoch [31/100], Step [420/1248], Loss: 0.0101\n",
      "Epoch [31/100], Step [440/1248], Loss: 0.0383\n",
      "Epoch [31/100], Step [460/1248], Loss: 0.0213\n",
      "Epoch [31/100], Step [480/1248], Loss: 0.0252\n",
      "Epoch [31/100], Step [500/1248], Loss: 0.0010\n",
      "Epoch [31/100], Step [520/1248], Loss: 0.0052\n",
      "Epoch [31/100], Step [540/1248], Loss: 0.0423\n",
      "Epoch [31/100], Step [560/1248], Loss: 0.0048\n",
      "Epoch [31/100], Step [580/1248], Loss: 0.0192\n",
      "Epoch [31/100], Step [600/1248], Loss: 0.0029\n",
      "Epoch [31/100], Step [620/1248], Loss: 0.0059\n",
      "Epoch [31/100], Step [640/1248], Loss: 0.0315\n",
      "Epoch [31/100], Step [660/1248], Loss: 0.1257\n",
      "Epoch [31/100], Step [680/1248], Loss: 0.0140\n",
      "Epoch [31/100], Step [700/1248], Loss: 0.0089\n",
      "Epoch [31/100], Step [720/1248], Loss: 0.0077\n",
      "Epoch [31/100], Step [740/1248], Loss: 0.0043\n",
      "Epoch [31/100], Step [760/1248], Loss: 0.0164\n",
      "Epoch [31/100], Step [780/1248], Loss: 0.0277\n",
      "Epoch [31/100], Step [800/1248], Loss: 0.0138\n",
      "Epoch [31/100], Step [820/1248], Loss: 0.0008\n",
      "Epoch [31/100], Step [840/1248], Loss: 0.0061\n",
      "Epoch [31/100], Step [860/1248], Loss: 0.0276\n",
      "Epoch [31/100], Step [880/1248], Loss: 0.0003\n",
      "Epoch [31/100], Step [900/1248], Loss: 0.0017\n",
      "Epoch [31/100], Step [920/1248], Loss: 0.3949\n",
      "Epoch [31/100], Step [940/1248], Loss: 0.0016\n",
      "Epoch [31/100], Step [960/1248], Loss: 0.0042\n",
      "Epoch [31/100], Step [980/1248], Loss: 0.0028\n",
      "Epoch [31/100], Step [1000/1248], Loss: 0.0084\n",
      "Epoch [31/100], Step [1020/1248], Loss: 0.0885\n",
      "Epoch [31/100], Step [1040/1248], Loss: 0.1067\n",
      "Epoch [31/100], Step [1060/1248], Loss: 0.0275\n",
      "Epoch [31/100], Step [1080/1248], Loss: 0.0122\n",
      "Epoch [31/100], Step [1100/1248], Loss: 0.0115\n",
      "Epoch [31/100], Step [1120/1248], Loss: 0.1166\n",
      "Epoch [31/100], Step [1140/1248], Loss: 0.0690\n",
      "Epoch [31/100], Step [1160/1248], Loss: 0.0335\n",
      "Epoch [31/100], Step [1180/1248], Loss: 0.0409\n",
      "Epoch [31/100], Step [1200/1248], Loss: 0.0153\n",
      "Epoch [31/100], Step [1220/1248], Loss: 0.0173\n",
      "Epoch [31/100], Step [1240/1248], Loss: 0.0103\n",
      "\n",
      "train-loss: 0.2438, train-acc: 98.4820\n",
      "validation loss: 1.4509, validation acc: 68.3106\n",
      "\n",
      "Epoch 32\n",
      "\n",
      "Epoch [32/100], Step [0/1248], Loss: 0.0194\n",
      "Epoch [32/100], Step [20/1248], Loss: 0.0615\n",
      "Epoch [32/100], Step [40/1248], Loss: 0.0089\n",
      "Epoch [32/100], Step [60/1248], Loss: 0.0047\n",
      "Epoch [32/100], Step [80/1248], Loss: 0.0675\n",
      "Epoch [32/100], Step [100/1248], Loss: 0.0112\n",
      "Epoch [32/100], Step [120/1248], Loss: 0.1058\n",
      "Epoch [32/100], Step [140/1248], Loss: 0.0476\n",
      "Epoch [32/100], Step [160/1248], Loss: 0.0032\n",
      "Epoch [32/100], Step [180/1248], Loss: 0.0039\n",
      "Epoch [32/100], Step [200/1248], Loss: 0.0501\n",
      "Epoch [32/100], Step [220/1248], Loss: 0.0065\n",
      "Epoch [32/100], Step [240/1248], Loss: 0.0046\n",
      "Epoch [32/100], Step [260/1248], Loss: 0.0011\n",
      "Epoch [32/100], Step [280/1248], Loss: 0.0033\n",
      "Epoch [32/100], Step [300/1248], Loss: 0.0069\n",
      "Epoch [32/100], Step [320/1248], Loss: 0.0032\n",
      "Epoch [32/100], Step [340/1248], Loss: 0.0003\n",
      "Epoch [32/100], Step [360/1248], Loss: 0.0022\n",
      "Epoch [32/100], Step [380/1248], Loss: 0.0523\n",
      "Epoch [32/100], Step [400/1248], Loss: 0.0054\n",
      "Epoch [32/100], Step [420/1248], Loss: 0.0512\n",
      "Epoch [32/100], Step [440/1248], Loss: 0.0006\n",
      "Epoch [32/100], Step [460/1248], Loss: 0.0024\n",
      "Epoch [32/100], Step [480/1248], Loss: 0.0186\n",
      "Epoch [32/100], Step [500/1248], Loss: 0.0017\n",
      "Epoch [32/100], Step [520/1248], Loss: 0.0013\n",
      "Epoch [32/100], Step [540/1248], Loss: 0.0044\n",
      "Epoch [32/100], Step [560/1248], Loss: 0.0176\n",
      "Epoch [32/100], Step [580/1248], Loss: 0.1192\n",
      "Epoch [32/100], Step [600/1248], Loss: 0.0399\n",
      "Epoch [32/100], Step [620/1248], Loss: 0.0311\n",
      "Epoch [32/100], Step [640/1248], Loss: 0.0356\n",
      "Epoch [32/100], Step [660/1248], Loss: 0.0070\n",
      "Epoch [32/100], Step [680/1248], Loss: 0.0091\n",
      "Epoch [32/100], Step [700/1248], Loss: 0.0179\n",
      "Epoch [32/100], Step [720/1248], Loss: 0.1710\n",
      "Epoch [32/100], Step [740/1248], Loss: 0.0116\n",
      "Epoch [32/100], Step [760/1248], Loss: 0.0020\n",
      "Epoch [32/100], Step [780/1248], Loss: 0.0107\n",
      "Epoch [32/100], Step [800/1248], Loss: 0.0031\n",
      "Epoch [32/100], Step [820/1248], Loss: 0.0368\n",
      "Epoch [32/100], Step [840/1248], Loss: 0.0454\n",
      "Epoch [32/100], Step [860/1248], Loss: 0.0547\n",
      "Epoch [32/100], Step [880/1248], Loss: 0.0346\n",
      "Epoch [32/100], Step [900/1248], Loss: 0.0422\n",
      "Epoch [32/100], Step [920/1248], Loss: 0.0063\n",
      "Epoch [32/100], Step [940/1248], Loss: 0.4299\n",
      "Epoch [32/100], Step [960/1248], Loss: 0.0167\n",
      "Epoch [32/100], Step [980/1248], Loss: 0.0052\n",
      "Epoch [32/100], Step [1000/1248], Loss: 0.1331\n",
      "Epoch [32/100], Step [1020/1248], Loss: 0.0186\n",
      "Epoch [32/100], Step [1040/1248], Loss: 0.0110\n",
      "Epoch [32/100], Step [1060/1248], Loss: 0.0327\n",
      "Epoch [32/100], Step [1080/1248], Loss: 0.1095\n",
      "Epoch [32/100], Step [1100/1248], Loss: 0.2119\n",
      "Epoch [32/100], Step [1120/1248], Loss: 0.0408\n",
      "Epoch [32/100], Step [1140/1248], Loss: 0.0296\n",
      "Epoch [32/100], Step [1160/1248], Loss: 0.0450\n",
      "Epoch [32/100], Step [1180/1248], Loss: 0.0402\n",
      "Epoch [32/100], Step [1200/1248], Loss: 0.0241\n",
      "Epoch [32/100], Step [1220/1248], Loss: 0.1595\n",
      "Epoch [32/100], Step [1240/1248], Loss: 0.0138\n",
      "\n",
      "train-loss: 0.2380, train-acc: 98.2114\n",
      "validation loss: 1.4604, validation acc: 67.4886\n",
      "\n",
      "Epoch 33\n",
      "\n",
      "Epoch [33/100], Step [0/1248], Loss: 0.0256\n",
      "Epoch [33/100], Step [20/1248], Loss: 0.0018\n",
      "Epoch [33/100], Step [40/1248], Loss: 0.1165\n",
      "Epoch [33/100], Step [60/1248], Loss: 0.0024\n",
      "Epoch [33/100], Step [80/1248], Loss: 0.0030\n",
      "Epoch [33/100], Step [100/1248], Loss: 0.0182\n",
      "Epoch [33/100], Step [120/1248], Loss: 0.0230\n",
      "Epoch [33/100], Step [140/1248], Loss: 0.0455\n",
      "Epoch [33/100], Step [160/1248], Loss: 0.0113\n",
      "Epoch [33/100], Step [180/1248], Loss: 0.0023\n",
      "Epoch [33/100], Step [200/1248], Loss: 0.0574\n",
      "Epoch [33/100], Step [220/1248], Loss: 0.0356\n",
      "Epoch [33/100], Step [240/1248], Loss: 0.0023\n",
      "Epoch [33/100], Step [260/1248], Loss: 0.0175\n",
      "Epoch [33/100], Step [280/1248], Loss: 0.0216\n",
      "Epoch [33/100], Step [300/1248], Loss: 0.1113\n",
      "Epoch [33/100], Step [320/1248], Loss: 0.2495\n",
      "Epoch [33/100], Step [340/1248], Loss: 0.0229\n",
      "Epoch [33/100], Step [360/1248], Loss: 0.0175\n",
      "Epoch [33/100], Step [380/1248], Loss: 0.0063\n",
      "Epoch [33/100], Step [400/1248], Loss: 0.0008\n",
      "Epoch [33/100], Step [420/1248], Loss: 0.0043\n",
      "Epoch [33/100], Step [440/1248], Loss: 0.0076\n",
      "Epoch [33/100], Step [460/1248], Loss: 0.0037\n",
      "Epoch [33/100], Step [480/1248], Loss: 0.1453\n",
      "Epoch [33/100], Step [500/1248], Loss: 0.0138\n",
      "Epoch [33/100], Step [520/1248], Loss: 0.0030\n",
      "Epoch [33/100], Step [540/1248], Loss: 0.0038\n",
      "Epoch [33/100], Step [560/1248], Loss: 0.0011\n",
      "Epoch [33/100], Step [580/1248], Loss: 0.0090\n",
      "Epoch [33/100], Step [600/1248], Loss: 0.0132\n",
      "Epoch [33/100], Step [620/1248], Loss: 0.1151\n",
      "Epoch [33/100], Step [640/1248], Loss: 0.0038\n",
      "Epoch [33/100], Step [660/1248], Loss: 0.0095\n",
      "Epoch [33/100], Step [680/1248], Loss: 0.1132\n",
      "Epoch [33/100], Step [700/1248], Loss: 0.0802\n",
      "Epoch [33/100], Step [720/1248], Loss: 0.0242\n",
      "Epoch [33/100], Step [740/1248], Loss: 0.2485\n",
      "Epoch [33/100], Step [760/1248], Loss: 0.0055\n",
      "Epoch [33/100], Step [780/1248], Loss: 0.0078\n",
      "Epoch [33/100], Step [800/1248], Loss: 0.1522\n",
      "Epoch [33/100], Step [820/1248], Loss: 0.0230\n",
      "Epoch [33/100], Step [840/1248], Loss: 0.0083\n",
      "Epoch [33/100], Step [860/1248], Loss: 0.0085\n",
      "Epoch [33/100], Step [880/1248], Loss: 0.0853\n",
      "Epoch [33/100], Step [900/1248], Loss: 0.0011\n",
      "Epoch [33/100], Step [920/1248], Loss: 0.2927\n",
      "Epoch [33/100], Step [940/1248], Loss: 0.0869\n",
      "Epoch [33/100], Step [960/1248], Loss: 0.1655\n",
      "Epoch [33/100], Step [980/1248], Loss: 0.1108\n",
      "Epoch [33/100], Step [1000/1248], Loss: 0.0176\n",
      "Epoch [33/100], Step [1020/1248], Loss: 0.0004\n",
      "Epoch [33/100], Step [1040/1248], Loss: 0.0110\n",
      "Epoch [33/100], Step [1060/1248], Loss: 0.2107\n",
      "Epoch [33/100], Step [1080/1248], Loss: 0.0962\n",
      "Epoch [33/100], Step [1100/1248], Loss: 0.0249\n",
      "Epoch [33/100], Step [1120/1248], Loss: 0.2116\n",
      "Epoch [33/100], Step [1140/1248], Loss: 0.0175\n",
      "Epoch [33/100], Step [1160/1248], Loss: 0.0078\n",
      "Epoch [33/100], Step [1180/1248], Loss: 0.2040\n",
      "Epoch [33/100], Step [1200/1248], Loss: 0.0059\n",
      "Epoch [33/100], Step [1220/1248], Loss: 0.0039\n",
      "Epoch [33/100], Step [1240/1248], Loss: 0.0216\n",
      "\n",
      "train-loss: 0.2324, train-acc: 98.2515\n",
      "validation loss: 1.4691, validation acc: 68.0830\n",
      "\n",
      "Epoch 34\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Step [0/1248], Loss: 0.0075\n",
      "Epoch [34/100], Step [20/1248], Loss: 0.0010\n",
      "Epoch [34/100], Step [40/1248], Loss: 0.0057\n",
      "Epoch [34/100], Step [60/1248], Loss: 0.0042\n",
      "Epoch [34/100], Step [80/1248], Loss: 0.0086\n",
      "Epoch [34/100], Step [100/1248], Loss: 0.0161\n",
      "Epoch [34/100], Step [120/1248], Loss: 0.2162\n",
      "Epoch [34/100], Step [140/1248], Loss: 0.0018\n",
      "Epoch [34/100], Step [160/1248], Loss: 0.0158\n",
      "Epoch [34/100], Step [180/1248], Loss: 0.0916\n",
      "Epoch [34/100], Step [200/1248], Loss: 0.0169\n",
      "Epoch [34/100], Step [220/1248], Loss: 0.0223\n",
      "Epoch [34/100], Step [240/1248], Loss: 0.0040\n",
      "Epoch [34/100], Step [260/1248], Loss: 0.0009\n",
      "Epoch [34/100], Step [280/1248], Loss: 0.0269\n",
      "Epoch [34/100], Step [300/1248], Loss: 0.0361\n",
      "Epoch [34/100], Step [320/1248], Loss: 0.0932\n",
      "Epoch [34/100], Step [340/1248], Loss: 0.0621\n",
      "Epoch [34/100], Step [360/1248], Loss: 0.0344\n",
      "Epoch [34/100], Step [380/1248], Loss: 0.0077\n",
      "Epoch [34/100], Step [400/1248], Loss: 0.0302\n",
      "Epoch [34/100], Step [420/1248], Loss: 0.0069\n",
      "Epoch [34/100], Step [440/1248], Loss: 0.0028\n",
      "Epoch [34/100], Step [460/1248], Loss: 0.0216\n",
      "Epoch [34/100], Step [480/1248], Loss: 0.3122\n",
      "Epoch [34/100], Step [500/1248], Loss: 0.0105\n",
      "Epoch [34/100], Step [520/1248], Loss: 0.0230\n",
      "Epoch [34/100], Step [540/1248], Loss: 0.0096\n",
      "Epoch [34/100], Step [560/1248], Loss: 0.0293\n",
      "Epoch [34/100], Step [580/1248], Loss: 0.3324\n",
      "Epoch [34/100], Step [600/1248], Loss: 0.0739\n",
      "Epoch [34/100], Step [620/1248], Loss: 0.2247\n",
      "Epoch [34/100], Step [640/1248], Loss: 0.0025\n",
      "Epoch [34/100], Step [660/1248], Loss: 0.0061\n",
      "Epoch [34/100], Step [680/1248], Loss: 0.0388\n",
      "Epoch [34/100], Step [700/1248], Loss: 0.0037\n",
      "Epoch [34/100], Step [720/1248], Loss: 0.1643\n",
      "Epoch [34/100], Step [740/1248], Loss: 0.0102\n",
      "Epoch [34/100], Step [760/1248], Loss: 0.0296\n",
      "Epoch [34/100], Step [780/1248], Loss: 0.0083\n",
      "Epoch [34/100], Step [800/1248], Loss: 0.0028\n",
      "Epoch [34/100], Step [820/1248], Loss: 0.0664\n",
      "Epoch [34/100], Step [840/1248], Loss: 0.0036\n",
      "Epoch [34/100], Step [860/1248], Loss: 0.0022\n",
      "Epoch [34/100], Step [880/1248], Loss: 0.1634\n",
      "Epoch [34/100], Step [900/1248], Loss: 0.0606\n",
      "Epoch [34/100], Step [920/1248], Loss: 0.1254\n",
      "Epoch [34/100], Step [940/1248], Loss: 0.0642\n",
      "Epoch [34/100], Step [960/1248], Loss: 0.0073\n",
      "Epoch [34/100], Step [980/1248], Loss: 0.0156\n",
      "Epoch [34/100], Step [1000/1248], Loss: 0.0258\n",
      "Epoch [34/100], Step [1020/1248], Loss: 0.0437\n",
      "Epoch [34/100], Step [1040/1248], Loss: 0.2941\n",
      "Epoch [34/100], Step [1060/1248], Loss: 0.0057\n",
      "Epoch [34/100], Step [1080/1248], Loss: 0.0238\n",
      "Epoch [34/100], Step [1100/1248], Loss: 0.0161\n",
      "Epoch [34/100], Step [1120/1248], Loss: 0.0062\n",
      "Epoch [34/100], Step [1140/1248], Loss: 0.1540\n",
      "Epoch [34/100], Step [1160/1248], Loss: 0.1414\n",
      "Epoch [34/100], Step [1180/1248], Loss: 0.0020\n",
      "Epoch [34/100], Step [1200/1248], Loss: 0.0339\n",
      "Epoch [34/100], Step [1220/1248], Loss: 0.0426\n",
      "Epoch [34/100], Step [1240/1248], Loss: 0.0066\n",
      "\n",
      "train-loss: 0.2269, train-acc: 98.4218\n",
      "validation loss: 1.4780, validation acc: 67.7415\n",
      "\n",
      "Epoch 35\n",
      "\n",
      "Epoch [35/100], Step [0/1248], Loss: 0.0032\n",
      "Epoch [35/100], Step [20/1248], Loss: 0.0100\n",
      "Epoch [35/100], Step [40/1248], Loss: 0.0075\n",
      "Epoch [35/100], Step [60/1248], Loss: 0.0513\n",
      "Epoch [35/100], Step [80/1248], Loss: 0.0038\n",
      "Epoch [35/100], Step [100/1248], Loss: 0.0070\n",
      "Epoch [35/100], Step [120/1248], Loss: 0.0024\n",
      "Epoch [35/100], Step [140/1248], Loss: 0.0011\n",
      "Epoch [35/100], Step [160/1248], Loss: 0.0183\n",
      "Epoch [35/100], Step [180/1248], Loss: 0.0142\n",
      "Epoch [35/100], Step [200/1248], Loss: 0.0012\n",
      "Epoch [35/100], Step [220/1248], Loss: 0.0342\n",
      "Epoch [35/100], Step [240/1248], Loss: 0.0218\n",
      "Epoch [35/100], Step [260/1248], Loss: 0.0039\n",
      "Epoch [35/100], Step [280/1248], Loss: 0.0261\n",
      "Epoch [35/100], Step [300/1248], Loss: 0.0033\n",
      "Epoch [35/100], Step [320/1248], Loss: 0.0088\n",
      "Epoch [35/100], Step [340/1248], Loss: 0.0348\n",
      "Epoch [35/100], Step [360/1248], Loss: 0.0052\n",
      "Epoch [35/100], Step [380/1248], Loss: 0.0065\n",
      "Epoch [35/100], Step [400/1248], Loss: 0.0036\n",
      "Epoch [35/100], Step [420/1248], Loss: 0.0044\n",
      "Epoch [35/100], Step [440/1248], Loss: 0.3050\n",
      "Epoch [35/100], Step [460/1248], Loss: 0.0155\n",
      "Epoch [35/100], Step [480/1248], Loss: 0.0021\n",
      "Epoch [35/100], Step [500/1248], Loss: 0.3959\n",
      "Epoch [35/100], Step [520/1248], Loss: 0.3257\n",
      "Epoch [35/100], Step [540/1248], Loss: 0.0130\n",
      "Epoch [35/100], Step [560/1248], Loss: 0.0221\n",
      "Epoch [35/100], Step [580/1248], Loss: 0.0291\n",
      "Epoch [35/100], Step [600/1248], Loss: 0.0037\n",
      "Epoch [35/100], Step [620/1248], Loss: 0.1013\n",
      "Epoch [35/100], Step [640/1248], Loss: 0.4169\n",
      "Epoch [35/100], Step [660/1248], Loss: 0.0041\n",
      "Epoch [35/100], Step [680/1248], Loss: 0.1385\n",
      "Epoch [35/100], Step [700/1248], Loss: 0.0596\n",
      "Epoch [35/100], Step [720/1248], Loss: 0.0589\n",
      "Epoch [35/100], Step [740/1248], Loss: 0.0107\n",
      "Epoch [35/100], Step [760/1248], Loss: 0.0067\n",
      "Epoch [35/100], Step [780/1248], Loss: 0.0423\n",
      "Epoch [35/100], Step [800/1248], Loss: 0.0508\n",
      "Epoch [35/100], Step [820/1248], Loss: 0.0657\n",
      "Epoch [35/100], Step [840/1248], Loss: 0.0744\n",
      "Epoch [35/100], Step [860/1248], Loss: 0.0210\n",
      "Epoch [35/100], Step [880/1248], Loss: 0.0092\n",
      "Epoch [35/100], Step [900/1248], Loss: 0.0019\n",
      "Epoch [35/100], Step [920/1248], Loss: 0.0122\n",
      "Epoch [35/100], Step [940/1248], Loss: 0.0470\n",
      "Epoch [35/100], Step [960/1248], Loss: 0.0040\n",
      "Epoch [35/100], Step [980/1248], Loss: 0.3446\n",
      "Epoch [35/100], Step [1000/1248], Loss: 0.0081\n",
      "Epoch [35/100], Step [1020/1248], Loss: 0.0156\n",
      "Epoch [35/100], Step [1040/1248], Loss: 0.0908\n",
      "Epoch [35/100], Step [1060/1248], Loss: 0.0543\n",
      "Epoch [35/100], Step [1080/1248], Loss: 0.0066\n",
      "Epoch [35/100], Step [1100/1248], Loss: 0.0105\n",
      "Epoch [35/100], Step [1120/1248], Loss: 0.2827\n",
      "Epoch [35/100], Step [1140/1248], Loss: 0.0137\n",
      "Epoch [35/100], Step [1160/1248], Loss: 0.1162\n",
      "Epoch [35/100], Step [1180/1248], Loss: 0.0396\n",
      "Epoch [35/100], Step [1200/1248], Loss: 0.0858\n",
      "Epoch [35/100], Step [1220/1248], Loss: 0.0044\n",
      "Epoch [35/100], Step [1240/1248], Loss: 0.0038\n",
      "\n",
      "train-loss: 0.2219, train-acc: 98.2465\n",
      "validation loss: 1.4834, validation acc: 67.6404\n",
      "\n",
      "Epoch 36\n",
      "\n",
      "Epoch [36/100], Step [0/1248], Loss: 0.0083\n",
      "Epoch [36/100], Step [20/1248], Loss: 0.0072\n",
      "Epoch [36/100], Step [40/1248], Loss: 0.0034\n",
      "Epoch [36/100], Step [60/1248], Loss: 0.0081\n",
      "Epoch [36/100], Step [80/1248], Loss: 0.0075\n",
      "Epoch [36/100], Step [100/1248], Loss: 0.0017\n",
      "Epoch [36/100], Step [120/1248], Loss: 0.0070\n",
      "Epoch [36/100], Step [140/1248], Loss: 0.1333\n",
      "Epoch [36/100], Step [160/1248], Loss: 0.0286\n",
      "Epoch [36/100], Step [180/1248], Loss: 0.0023\n",
      "Epoch [36/100], Step [200/1248], Loss: 0.0089\n",
      "Epoch [36/100], Step [220/1248], Loss: 0.0213\n",
      "Epoch [36/100], Step [240/1248], Loss: 0.0322\n",
      "Epoch [36/100], Step [260/1248], Loss: 0.0171\n",
      "Epoch [36/100], Step [280/1248], Loss: 0.0080\n",
      "Epoch [36/100], Step [300/1248], Loss: 0.0014\n",
      "Epoch [36/100], Step [320/1248], Loss: 0.0112\n",
      "Epoch [36/100], Step [340/1248], Loss: 0.0032\n",
      "Epoch [36/100], Step [360/1248], Loss: 0.1872\n",
      "Epoch [36/100], Step [380/1248], Loss: 0.1433\n",
      "Epoch [36/100], Step [400/1248], Loss: 0.0087\n",
      "Epoch [36/100], Step [420/1248], Loss: 0.0059\n",
      "Epoch [36/100], Step [440/1248], Loss: 0.0108\n",
      "Epoch [36/100], Step [460/1248], Loss: 0.0007\n",
      "Epoch [36/100], Step [480/1248], Loss: 0.0310\n",
      "Epoch [36/100], Step [500/1248], Loss: 0.0194\n",
      "Epoch [36/100], Step [520/1248], Loss: 0.0173\n",
      "Epoch [36/100], Step [540/1248], Loss: 0.0021\n",
      "Epoch [36/100], Step [560/1248], Loss: 0.0437\n",
      "Epoch [36/100], Step [580/1248], Loss: 0.0015\n",
      "Epoch [36/100], Step [600/1248], Loss: 0.0325\n",
      "Epoch [36/100], Step [620/1248], Loss: 0.0985\n",
      "Epoch [36/100], Step [640/1248], Loss: 0.0102\n",
      "Epoch [36/100], Step [660/1248], Loss: 0.0126\n",
      "Epoch [36/100], Step [680/1248], Loss: 0.0277\n",
      "Epoch [36/100], Step [700/1248], Loss: 0.0024\n",
      "Epoch [36/100], Step [720/1248], Loss: 0.0337\n",
      "Epoch [36/100], Step [740/1248], Loss: 0.0610\n",
      "Epoch [36/100], Step [760/1248], Loss: 0.0036\n",
      "Epoch [36/100], Step [780/1248], Loss: 0.0049\n",
      "Epoch [36/100], Step [800/1248], Loss: 0.0024\n",
      "Epoch [36/100], Step [820/1248], Loss: 0.0151\n",
      "Epoch [36/100], Step [840/1248], Loss: 0.0792\n",
      "Epoch [36/100], Step [860/1248], Loss: 0.0638\n",
      "Epoch [36/100], Step [880/1248], Loss: 0.0035\n",
      "Epoch [36/100], Step [900/1248], Loss: 0.0061\n",
      "Epoch [36/100], Step [920/1248], Loss: 0.0006\n",
      "Epoch [36/100], Step [940/1248], Loss: 0.0413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Step [960/1248], Loss: 0.0072\n",
      "Epoch [36/100], Step [980/1248], Loss: 0.0037\n",
      "Epoch [36/100], Step [1000/1248], Loss: 0.0095\n",
      "Epoch [36/100], Step [1020/1248], Loss: 0.0215\n",
      "Epoch [36/100], Step [1040/1248], Loss: 0.0012\n",
      "Epoch [36/100], Step [1060/1248], Loss: 0.0422\n",
      "Epoch [36/100], Step [1080/1248], Loss: 0.3169\n",
      "Epoch [36/100], Step [1100/1248], Loss: 0.0222\n",
      "Epoch [36/100], Step [1120/1248], Loss: 0.1012\n",
      "Epoch [36/100], Step [1140/1248], Loss: 0.0272\n",
      "Epoch [36/100], Step [1160/1248], Loss: 0.0260\n",
      "Epoch [36/100], Step [1180/1248], Loss: 0.5627\n",
      "Epoch [36/100], Step [1200/1248], Loss: 0.0191\n",
      "Epoch [36/100], Step [1220/1248], Loss: 0.0030\n",
      "Epoch [36/100], Step [1240/1248], Loss: 0.0015\n",
      "\n",
      "train-loss: 0.2170, train-acc: 98.4118\n",
      "validation loss: 1.4923, validation acc: 68.3232\n",
      "\n",
      "Epoch 37\n",
      "\n",
      "Epoch [37/100], Step [0/1248], Loss: 0.0099\n",
      "Epoch [37/100], Step [20/1248], Loss: 0.0233\n",
      "Epoch [37/100], Step [40/1248], Loss: 0.5398\n",
      "Epoch [37/100], Step [60/1248], Loss: 0.0474\n",
      "Epoch [37/100], Step [80/1248], Loss: 0.0017\n",
      "Epoch [37/100], Step [100/1248], Loss: 0.1565\n",
      "Epoch [37/100], Step [120/1248], Loss: 0.0808\n",
      "Epoch [37/100], Step [140/1248], Loss: 0.0399\n",
      "Epoch [37/100], Step [160/1248], Loss: 0.1064\n",
      "Epoch [37/100], Step [180/1248], Loss: 0.0481\n",
      "Epoch [37/100], Step [200/1248], Loss: 0.0089\n",
      "Epoch [37/100], Step [220/1248], Loss: 0.0067\n",
      "Epoch [37/100], Step [240/1248], Loss: 0.0008\n",
      "Epoch [37/100], Step [260/1248], Loss: 0.0092\n",
      "Epoch [37/100], Step [280/1248], Loss: 0.0999\n",
      "Epoch [37/100], Step [300/1248], Loss: 0.0008\n",
      "Epoch [37/100], Step [320/1248], Loss: 0.0028\n",
      "Epoch [37/100], Step [340/1248], Loss: 0.1008\n",
      "Epoch [37/100], Step [360/1248], Loss: 0.0021\n",
      "Epoch [37/100], Step [380/1248], Loss: 0.1368\n",
      "Epoch [37/100], Step [400/1248], Loss: 0.0195\n",
      "Epoch [37/100], Step [420/1248], Loss: 0.0871\n",
      "Epoch [37/100], Step [440/1248], Loss: 0.0013\n",
      "Epoch [37/100], Step [460/1248], Loss: 0.0024\n",
      "Epoch [37/100], Step [480/1248], Loss: 0.0111\n",
      "Epoch [37/100], Step [500/1248], Loss: 0.0246\n",
      "Epoch [37/100], Step [520/1248], Loss: 0.3547\n",
      "Epoch [37/100], Step [540/1248], Loss: 0.3031\n",
      "Epoch [37/100], Step [560/1248], Loss: 0.1571\n",
      "Epoch [37/100], Step [580/1248], Loss: 0.0044\n",
      "Epoch [37/100], Step [600/1248], Loss: 0.0016\n",
      "Epoch [37/100], Step [620/1248], Loss: 0.0710\n",
      "Epoch [37/100], Step [640/1248], Loss: 0.0074\n",
      "Epoch [37/100], Step [660/1248], Loss: 0.0923\n",
      "Epoch [37/100], Step [680/1248], Loss: 0.0032\n",
      "Epoch [37/100], Step [700/1248], Loss: 0.0730\n",
      "Epoch [37/100], Step [720/1248], Loss: 0.0435\n",
      "Epoch [37/100], Step [740/1248], Loss: 0.0472\n",
      "Epoch [37/100], Step [760/1248], Loss: 0.0657\n",
      "Epoch [37/100], Step [780/1248], Loss: 0.0816\n",
      "Epoch [37/100], Step [800/1248], Loss: 0.0060\n",
      "Epoch [37/100], Step [820/1248], Loss: 0.0462\n",
      "Epoch [37/100], Step [840/1248], Loss: 0.0660\n",
      "Epoch [37/100], Step [860/1248], Loss: 0.0171\n",
      "Epoch [37/100], Step [880/1248], Loss: 0.1210\n",
      "Epoch [37/100], Step [900/1248], Loss: 0.0005\n",
      "Epoch [37/100], Step [920/1248], Loss: 0.0154\n",
      "Epoch [37/100], Step [940/1248], Loss: 0.0083\n",
      "Epoch [37/100], Step [960/1248], Loss: 0.0023\n",
      "Epoch [37/100], Step [980/1248], Loss: 0.0070\n",
      "Epoch [37/100], Step [1000/1248], Loss: 0.0018\n",
      "Epoch [37/100], Step [1020/1248], Loss: 0.4644\n",
      "Epoch [37/100], Step [1040/1248], Loss: 0.0002\n",
      "Epoch [37/100], Step [1060/1248], Loss: 0.0085\n",
      "Epoch [37/100], Step [1080/1248], Loss: 0.0059\n",
      "Epoch [37/100], Step [1100/1248], Loss: 0.0016\n",
      "Epoch [37/100], Step [1120/1248], Loss: 0.0044\n",
      "Epoch [37/100], Step [1140/1248], Loss: 0.0033\n",
      "Epoch [37/100], Step [1160/1248], Loss: 0.0029\n",
      "Epoch [37/100], Step [1180/1248], Loss: 0.0179\n",
      "Epoch [37/100], Step [1200/1248], Loss: 0.1216\n",
      "Epoch [37/100], Step [1220/1248], Loss: 0.1109\n",
      "Epoch [37/100], Step [1240/1248], Loss: 0.1962\n",
      "\n",
      "train-loss: 0.2124, train-acc: 98.5872\n",
      "validation loss: 1.5008, validation acc: 67.3748\n",
      "\n",
      "Epoch 38\n",
      "\n",
      "Epoch [38/100], Step [0/1248], Loss: 0.0455\n",
      "Epoch [38/100], Step [20/1248], Loss: 0.1633\n",
      "Epoch [38/100], Step [40/1248], Loss: 0.0099\n",
      "Epoch [38/100], Step [60/1248], Loss: 0.0598\n",
      "Epoch [38/100], Step [80/1248], Loss: 0.1788\n",
      "Epoch [38/100], Step [100/1248], Loss: 0.0047\n",
      "Epoch [38/100], Step [120/1248], Loss: 0.0031\n",
      "Epoch [38/100], Step [140/1248], Loss: 0.0241\n",
      "Epoch [38/100], Step [160/1248], Loss: 0.0575\n",
      "Epoch [38/100], Step [180/1248], Loss: 0.0130\n",
      "Epoch [38/100], Step [200/1248], Loss: 0.0101\n",
      "Epoch [38/100], Step [220/1248], Loss: 0.0858\n",
      "Epoch [38/100], Step [240/1248], Loss: 0.0492\n",
      "Epoch [38/100], Step [260/1248], Loss: 0.0012\n",
      "Epoch [38/100], Step [280/1248], Loss: 0.1474\n",
      "Epoch [38/100], Step [300/1248], Loss: 0.0056\n",
      "Epoch [38/100], Step [320/1248], Loss: 0.0009\n",
      "Epoch [38/100], Step [340/1248], Loss: 0.0068\n",
      "Epoch [38/100], Step [360/1248], Loss: 0.0012\n",
      "Epoch [38/100], Step [380/1248], Loss: 0.0213\n",
      "Epoch [38/100], Step [400/1248], Loss: 0.0009\n",
      "Epoch [38/100], Step [420/1248], Loss: 0.0047\n",
      "Epoch [38/100], Step [440/1248], Loss: 0.0131\n",
      "Epoch [38/100], Step [460/1248], Loss: 0.0528\n",
      "Epoch [38/100], Step [480/1248], Loss: 0.0165\n",
      "Epoch [38/100], Step [500/1248], Loss: 0.1142\n",
      "Epoch [38/100], Step [520/1248], Loss: 0.0056\n",
      "Epoch [38/100], Step [540/1248], Loss: 0.0175\n",
      "Epoch [38/100], Step [560/1248], Loss: 0.0051\n",
      "Epoch [38/100], Step [580/1248], Loss: 0.0985\n",
      "Epoch [38/100], Step [600/1248], Loss: 0.0468\n",
      "Epoch [38/100], Step [620/1248], Loss: 0.0014\n",
      "Epoch [38/100], Step [640/1248], Loss: 0.0054\n",
      "Epoch [38/100], Step [660/1248], Loss: 0.0024\n",
      "Epoch [38/100], Step [680/1248], Loss: 0.0048\n",
      "Epoch [38/100], Step [700/1248], Loss: 0.0008\n",
      "Epoch [38/100], Step [720/1248], Loss: 0.0051\n",
      "Epoch [38/100], Step [740/1248], Loss: 0.0285\n",
      "Epoch [38/100], Step [760/1248], Loss: 0.0835\n",
      "Epoch [38/100], Step [780/1248], Loss: 0.0029\n",
      "Epoch [38/100], Step [800/1248], Loss: 0.0212\n",
      "Epoch [38/100], Step [820/1248], Loss: 0.0068\n",
      "Epoch [38/100], Step [840/1248], Loss: 0.0024\n",
      "Epoch [38/100], Step [860/1248], Loss: 0.0089\n",
      "Epoch [38/100], Step [880/1248], Loss: 0.0327\n",
      "Epoch [38/100], Step [900/1248], Loss: 0.1519\n",
      "Epoch [38/100], Step [920/1248], Loss: 0.1095\n",
      "Epoch [38/100], Step [940/1248], Loss: 0.0467\n",
      "Epoch [38/100], Step [960/1248], Loss: 0.1297\n",
      "Epoch [38/100], Step [980/1248], Loss: 0.0220\n",
      "Epoch [38/100], Step [1000/1248], Loss: 0.1343\n",
      "Epoch [38/100], Step [1020/1248], Loss: 0.0106\n",
      "Epoch [38/100], Step [1040/1248], Loss: 0.1543\n",
      "Epoch [38/100], Step [1060/1248], Loss: 0.0093\n",
      "Epoch [38/100], Step [1080/1248], Loss: 0.0570\n",
      "Epoch [38/100], Step [1100/1248], Loss: 0.2374\n",
      "Epoch [38/100], Step [1120/1248], Loss: 0.0502\n",
      "Epoch [38/100], Step [1140/1248], Loss: 0.0612\n",
      "Epoch [38/100], Step [1160/1248], Loss: 0.0779\n",
      "Epoch [38/100], Step [1180/1248], Loss: 0.1319\n",
      "Epoch [38/100], Step [1200/1248], Loss: 0.0065\n",
      "Epoch [38/100], Step [1220/1248], Loss: 0.0018\n",
      "Epoch [38/100], Step [1240/1248], Loss: 0.0263\n",
      "\n",
      "train-loss: 0.2080, train-acc: 98.4218\n",
      "validation loss: 1.5068, validation acc: 67.8933\n",
      "\n",
      "Epoch 39\n",
      "\n",
      "Epoch [39/100], Step [0/1248], Loss: 0.0015\n",
      "Epoch [39/100], Step [20/1248], Loss: 0.0039\n",
      "Epoch [39/100], Step [40/1248], Loss: 0.0003\n",
      "Epoch [39/100], Step [60/1248], Loss: 0.1086\n",
      "Epoch [39/100], Step [80/1248], Loss: 0.0214\n",
      "Epoch [39/100], Step [100/1248], Loss: 0.0048\n",
      "Epoch [39/100], Step [120/1248], Loss: 0.0071\n",
      "Epoch [39/100], Step [140/1248], Loss: 0.0055\n",
      "Epoch [39/100], Step [160/1248], Loss: 0.0004\n",
      "Epoch [39/100], Step [180/1248], Loss: 0.0018\n",
      "Epoch [39/100], Step [200/1248], Loss: 0.0035\n",
      "Epoch [39/100], Step [220/1248], Loss: 0.0489\n",
      "Epoch [39/100], Step [240/1248], Loss: 0.0072\n",
      "Epoch [39/100], Step [260/1248], Loss: 0.0761\n",
      "Epoch [39/100], Step [280/1248], Loss: 0.0030\n",
      "Epoch [39/100], Step [300/1248], Loss: 0.0018\n",
      "Epoch [39/100], Step [320/1248], Loss: 0.0223\n",
      "Epoch [39/100], Step [340/1248], Loss: 0.0153\n",
      "Epoch [39/100], Step [360/1248], Loss: 0.0003\n",
      "Epoch [39/100], Step [380/1248], Loss: 0.0757\n",
      "Epoch [39/100], Step [400/1248], Loss: 0.0158\n",
      "Epoch [39/100], Step [420/1248], Loss: 0.0021\n",
      "Epoch [39/100], Step [440/1248], Loss: 0.0027\n",
      "Epoch [39/100], Step [460/1248], Loss: 0.0094\n",
      "Epoch [39/100], Step [480/1248], Loss: 0.0005\n",
      "Epoch [39/100], Step [500/1248], Loss: 0.0016\n",
      "Epoch [39/100], Step [520/1248], Loss: 0.0139\n",
      "Epoch [39/100], Step [540/1248], Loss: 0.0040\n",
      "Epoch [39/100], Step [560/1248], Loss: 0.0279\n",
      "Epoch [39/100], Step [580/1248], Loss: 0.0007\n",
      "Epoch [39/100], Step [600/1248], Loss: 0.0131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100], Step [620/1248], Loss: 0.0030\n",
      "Epoch [39/100], Step [640/1248], Loss: 0.0051\n",
      "Epoch [39/100], Step [660/1248], Loss: 0.0009\n",
      "Epoch [39/100], Step [680/1248], Loss: 0.0007\n",
      "Epoch [39/100], Step [700/1248], Loss: 0.0472\n",
      "Epoch [39/100], Step [720/1248], Loss: 0.0110\n",
      "Epoch [39/100], Step [740/1248], Loss: 0.0026\n",
      "Epoch [39/100], Step [760/1248], Loss: 0.0261\n",
      "Epoch [39/100], Step [780/1248], Loss: 0.0372\n",
      "Epoch [39/100], Step [800/1248], Loss: 0.1191\n",
      "Epoch [39/100], Step [820/1248], Loss: 0.0370\n",
      "Epoch [39/100], Step [840/1248], Loss: 0.0084\n",
      "Epoch [39/100], Step [860/1248], Loss: 0.0264\n",
      "Epoch [39/100], Step [880/1248], Loss: 0.0076\n",
      "Epoch [39/100], Step [900/1248], Loss: 0.1694\n",
      "Epoch [39/100], Step [920/1248], Loss: 0.0023\n",
      "Epoch [39/100], Step [940/1248], Loss: 0.0118\n",
      "Epoch [39/100], Step [960/1248], Loss: 0.1197\n",
      "Epoch [39/100], Step [980/1248], Loss: 0.2689\n",
      "Epoch [39/100], Step [1000/1248], Loss: 0.0160\n",
      "Epoch [39/100], Step [1020/1248], Loss: 0.0067\n",
      "Epoch [39/100], Step [1040/1248], Loss: 0.0099\n",
      "Epoch [39/100], Step [1060/1248], Loss: 0.1747\n",
      "Epoch [39/100], Step [1080/1248], Loss: 0.0025\n",
      "Epoch [39/100], Step [1100/1248], Loss: 0.0489\n",
      "Epoch [39/100], Step [1120/1248], Loss: 0.0946\n",
      "Epoch [39/100], Step [1140/1248], Loss: 0.0569\n",
      "Epoch [39/100], Step [1160/1248], Loss: 0.0625\n",
      "Epoch [39/100], Step [1180/1248], Loss: 0.0638\n",
      "Epoch [39/100], Step [1200/1248], Loss: 0.0311\n",
      "Epoch [39/100], Step [1220/1248], Loss: 0.0619\n",
      "Epoch [39/100], Step [1240/1248], Loss: 0.1260\n",
      "\n",
      "train-loss: 0.2037, train-acc: 98.6774\n",
      "validation loss: 1.5167, validation acc: 68.5003\n",
      "\n",
      "Epoch 40\n",
      "\n",
      "Epoch [40/100], Step [0/1248], Loss: 0.0115\n",
      "Epoch [40/100], Step [20/1248], Loss: 0.0749\n",
      "Epoch [40/100], Step [40/1248], Loss: 0.0287\n",
      "Epoch [40/100], Step [60/1248], Loss: 0.0192\n",
      "Epoch [40/100], Step [80/1248], Loss: 0.0022\n",
      "Epoch [40/100], Step [100/1248], Loss: 0.0049\n",
      "Epoch [40/100], Step [120/1248], Loss: 0.0019\n",
      "Epoch [40/100], Step [140/1248], Loss: 0.0443\n",
      "Epoch [40/100], Step [160/1248], Loss: 0.0163\n",
      "Epoch [40/100], Step [180/1248], Loss: 0.0826\n",
      "Epoch [40/100], Step [200/1248], Loss: 0.0092\n",
      "Epoch [40/100], Step [220/1248], Loss: 0.0268\n",
      "Epoch [40/100], Step [240/1248], Loss: 0.0019\n",
      "Epoch [40/100], Step [260/1248], Loss: 0.0022\n",
      "Epoch [40/100], Step [280/1248], Loss: 0.0119\n",
      "Epoch [40/100], Step [300/1248], Loss: 0.0088\n",
      "Epoch [40/100], Step [320/1248], Loss: 0.0449\n",
      "Epoch [40/100], Step [340/1248], Loss: 0.0037\n",
      "Epoch [40/100], Step [360/1248], Loss: 0.0633\n",
      "Epoch [40/100], Step [380/1248], Loss: 0.0165\n",
      "Epoch [40/100], Step [400/1248], Loss: 0.1891\n",
      "Epoch [40/100], Step [420/1248], Loss: 0.2306\n",
      "Epoch [40/100], Step [440/1248], Loss: 0.0237\n",
      "Epoch [40/100], Step [460/1248], Loss: 0.0050\n",
      "Epoch [40/100], Step [480/1248], Loss: 0.0496\n",
      "Epoch [40/100], Step [500/1248], Loss: 0.0264\n",
      "Epoch [40/100], Step [520/1248], Loss: 0.0010\n",
      "Epoch [40/100], Step [540/1248], Loss: 0.0081\n",
      "Epoch [40/100], Step [560/1248], Loss: 0.0090\n",
      "Epoch [40/100], Step [580/1248], Loss: 0.0311\n",
      "Epoch [40/100], Step [600/1248], Loss: 0.1012\n",
      "Epoch [40/100], Step [620/1248], Loss: 0.0914\n",
      "Epoch [40/100], Step [640/1248], Loss: 0.0138\n",
      "Epoch [40/100], Step [660/1248], Loss: 0.0177\n",
      "Epoch [40/100], Step [680/1248], Loss: 0.0448\n",
      "Epoch [40/100], Step [700/1248], Loss: 0.0528\n",
      "Epoch [40/100], Step [720/1248], Loss: 0.0464\n",
      "Epoch [40/100], Step [740/1248], Loss: 0.0357\n",
      "Epoch [40/100], Step [760/1248], Loss: 0.0033\n",
      "Epoch [40/100], Step [780/1248], Loss: 0.0073\n",
      "Epoch [40/100], Step [800/1248], Loss: 0.0044\n",
      "Epoch [40/100], Step [820/1248], Loss: 0.0786\n",
      "Epoch [40/100], Step [840/1248], Loss: 0.0737\n",
      "Epoch [40/100], Step [860/1248], Loss: 0.0173\n",
      "Epoch [40/100], Step [880/1248], Loss: 0.1826\n",
      "Epoch [40/100], Step [900/1248], Loss: 0.0072\n",
      "Epoch [40/100], Step [920/1248], Loss: 0.0218\n",
      "Epoch [40/100], Step [940/1248], Loss: 0.0251\n",
      "Epoch [40/100], Step [960/1248], Loss: 0.0507\n",
      "Epoch [40/100], Step [980/1248], Loss: 0.0050\n",
      "Epoch [40/100], Step [1000/1248], Loss: 0.1683\n",
      "Epoch [40/100], Step [1020/1248], Loss: 0.0098\n",
      "Epoch [40/100], Step [1040/1248], Loss: 0.0056\n",
      "Epoch [40/100], Step [1060/1248], Loss: 0.0078\n",
      "Epoch [40/100], Step [1080/1248], Loss: 0.0012\n",
      "Epoch [40/100], Step [1100/1248], Loss: 0.0771\n",
      "Epoch [40/100], Step [1120/1248], Loss: 0.0145\n",
      "Epoch [40/100], Step [1140/1248], Loss: 0.0172\n",
      "Epoch [40/100], Step [1160/1248], Loss: 0.0006\n",
      "Epoch [40/100], Step [1180/1248], Loss: 0.0762\n",
      "Epoch [40/100], Step [1200/1248], Loss: 0.0065\n",
      "Epoch [40/100], Step [1220/1248], Loss: 0.0259\n",
      "Epoch [40/100], Step [1240/1248], Loss: 0.0075\n",
      "\n",
      "train-loss: 0.1999, train-acc: 98.3567\n",
      "validation loss: 1.5220, validation acc: 67.9565\n",
      "\n",
      "Epoch 41\n",
      "\n",
      "Epoch [41/100], Step [0/1248], Loss: 0.0030\n",
      "Epoch [41/100], Step [20/1248], Loss: 0.0046\n",
      "Epoch [41/100], Step [40/1248], Loss: 0.0016\n",
      "Epoch [41/100], Step [60/1248], Loss: 0.0392\n",
      "Epoch [41/100], Step [80/1248], Loss: 0.0021\n",
      "Epoch [41/100], Step [100/1248], Loss: 0.0542\n",
      "Epoch [41/100], Step [120/1248], Loss: 0.0766\n",
      "Epoch [41/100], Step [140/1248], Loss: 0.0002\n",
      "Epoch [41/100], Step [160/1248], Loss: 0.0356\n",
      "Epoch [41/100], Step [180/1248], Loss: 0.0663\n",
      "Epoch [41/100], Step [200/1248], Loss: 0.0004\n",
      "Epoch [41/100], Step [220/1248], Loss: 0.0005\n",
      "Epoch [41/100], Step [240/1248], Loss: 0.0130\n",
      "Epoch [41/100], Step [260/1248], Loss: 0.0243\n",
      "Epoch [41/100], Step [280/1248], Loss: 0.0034\n",
      "Epoch [41/100], Step [300/1248], Loss: 0.0058\n",
      "Epoch [41/100], Step [320/1248], Loss: 0.0983\n",
      "Epoch [41/100], Step [340/1248], Loss: 0.0103\n",
      "Epoch [41/100], Step [360/1248], Loss: 0.0991\n",
      "Epoch [41/100], Step [380/1248], Loss: 0.0007\n",
      "Epoch [41/100], Step [400/1248], Loss: 0.1563\n",
      "Epoch [41/100], Step [420/1248], Loss: 0.0011\n",
      "Epoch [41/100], Step [440/1248], Loss: 0.0191\n",
      "Epoch [41/100], Step [460/1248], Loss: 0.0396\n",
      "Epoch [41/100], Step [480/1248], Loss: 0.0158\n",
      "Epoch [41/100], Step [500/1248], Loss: 0.0734\n",
      "Epoch [41/100], Step [520/1248], Loss: 0.0074\n",
      "Epoch [41/100], Step [540/1248], Loss: 0.0255\n",
      "Epoch [41/100], Step [560/1248], Loss: 0.0004\n",
      "Epoch [41/100], Step [580/1248], Loss: 0.1622\n",
      "Epoch [41/100], Step [600/1248], Loss: 0.0638\n",
      "Epoch [41/100], Step [620/1248], Loss: 0.0009\n",
      "Epoch [41/100], Step [640/1248], Loss: 0.0341\n",
      "Epoch [41/100], Step [660/1248], Loss: 0.0031\n",
      "Epoch [41/100], Step [680/1248], Loss: 0.0010\n",
      "Epoch [41/100], Step [700/1248], Loss: 0.0017\n",
      "Epoch [41/100], Step [720/1248], Loss: 0.0063\n",
      "Epoch [41/100], Step [740/1248], Loss: 0.0156\n",
      "Epoch [41/100], Step [760/1248], Loss: 0.1406\n",
      "Epoch [41/100], Step [780/1248], Loss: 0.0023\n",
      "Epoch [41/100], Step [800/1248], Loss: 0.0009\n",
      "Epoch [41/100], Step [820/1248], Loss: 0.0055\n",
      "Epoch [41/100], Step [840/1248], Loss: 0.0071\n",
      "Epoch [41/100], Step [860/1248], Loss: 0.0024\n",
      "Epoch [41/100], Step [880/1248], Loss: 0.0095\n",
      "Epoch [41/100], Step [900/1248], Loss: 0.0191\n",
      "Epoch [41/100], Step [920/1248], Loss: 0.0267\n",
      "Epoch [41/100], Step [940/1248], Loss: 0.0008\n",
      "Epoch [41/100], Step [960/1248], Loss: 0.0030\n",
      "Epoch [41/100], Step [980/1248], Loss: 0.0080\n",
      "Epoch [41/100], Step [1000/1248], Loss: 0.1475\n",
      "Epoch [41/100], Step [1020/1248], Loss: 0.0006\n",
      "Epoch [41/100], Step [1040/1248], Loss: 0.0025\n",
      "Epoch [41/100], Step [1060/1248], Loss: 0.0005\n",
      "Epoch [41/100], Step [1080/1248], Loss: 0.0051\n",
      "Epoch [41/100], Step [1100/1248], Loss: 0.0100\n",
      "Epoch [41/100], Step [1120/1248], Loss: 0.0342\n",
      "Epoch [41/100], Step [1140/1248], Loss: 0.0004\n",
      "Epoch [41/100], Step [1160/1248], Loss: 0.0133\n",
      "Epoch [41/100], Step [1180/1248], Loss: 0.0249\n",
      "Epoch [41/100], Step [1200/1248], Loss: 0.0202\n",
      "Epoch [41/100], Step [1220/1248], Loss: 0.0018\n",
      "Epoch [41/100], Step [1240/1248], Loss: 0.1136\n",
      "\n",
      "train-loss: 0.1958, train-acc: 98.7826\n",
      "validation loss: 1.5294, validation acc: 68.3991\n",
      "\n",
      "Epoch 42\n",
      "\n",
      "Epoch [42/100], Step [0/1248], Loss: 0.0253\n",
      "Epoch [42/100], Step [20/1248], Loss: 0.4692\n",
      "Epoch [42/100], Step [40/1248], Loss: 0.0054\n",
      "Epoch [42/100], Step [60/1248], Loss: 0.0010\n",
      "Epoch [42/100], Step [80/1248], Loss: 0.0019\n",
      "Epoch [42/100], Step [100/1248], Loss: 0.0016\n",
      "Epoch [42/100], Step [120/1248], Loss: 0.0059\n",
      "Epoch [42/100], Step [140/1248], Loss: 0.0006\n",
      "Epoch [42/100], Step [160/1248], Loss: 0.1315\n",
      "Epoch [42/100], Step [180/1248], Loss: 0.0697\n",
      "Epoch [42/100], Step [200/1248], Loss: 0.0084\n",
      "Epoch [42/100], Step [220/1248], Loss: 0.0047\n",
      "Epoch [42/100], Step [240/1248], Loss: 0.3804\n",
      "Epoch [42/100], Step [260/1248], Loss: 0.0003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Step [280/1248], Loss: 0.0178\n",
      "Epoch [42/100], Step [300/1248], Loss: 0.0178\n",
      "Epoch [42/100], Step [320/1248], Loss: 0.0171\n",
      "Epoch [42/100], Step [340/1248], Loss: 0.1116\n",
      "Epoch [42/100], Step [360/1248], Loss: 0.0007\n",
      "Epoch [42/100], Step [380/1248], Loss: 0.0150\n",
      "Epoch [42/100], Step [400/1248], Loss: 0.0479\n",
      "Epoch [42/100], Step [420/1248], Loss: 0.0092\n",
      "Epoch [42/100], Step [440/1248], Loss: 0.0225\n",
      "Epoch [42/100], Step [460/1248], Loss: 0.0922\n",
      "Epoch [42/100], Step [480/1248], Loss: 0.0021\n",
      "Epoch [42/100], Step [500/1248], Loss: 0.0079\n",
      "Epoch [42/100], Step [520/1248], Loss: 0.1730\n",
      "Epoch [42/100], Step [540/1248], Loss: 0.0009\n",
      "Epoch [42/100], Step [560/1248], Loss: 0.5861\n",
      "Epoch [42/100], Step [580/1248], Loss: 0.0812\n",
      "Epoch [42/100], Step [600/1248], Loss: 0.0123\n",
      "Epoch [42/100], Step [620/1248], Loss: 0.0047\n",
      "Epoch [42/100], Step [640/1248], Loss: 0.0117\n",
      "Epoch [42/100], Step [660/1248], Loss: 0.0012\n",
      "Epoch [42/100], Step [680/1248], Loss: 0.1912\n",
      "Epoch [42/100], Step [700/1248], Loss: 0.0013\n",
      "Epoch [42/100], Step [720/1248], Loss: 0.0010\n",
      "Epoch [42/100], Step [740/1248], Loss: 0.0070\n",
      "Epoch [42/100], Step [760/1248], Loss: 0.0283\n",
      "Epoch [42/100], Step [780/1248], Loss: 0.3246\n",
      "Epoch [42/100], Step [800/1248], Loss: 0.0043\n",
      "Epoch [42/100], Step [820/1248], Loss: 0.1591\n",
      "Epoch [42/100], Step [840/1248], Loss: 0.0174\n",
      "Epoch [42/100], Step [860/1248], Loss: 0.0011\n",
      "Epoch [42/100], Step [880/1248], Loss: 0.0500\n",
      "Epoch [42/100], Step [900/1248], Loss: 0.0315\n",
      "Epoch [42/100], Step [920/1248], Loss: 0.0012\n",
      "Epoch [42/100], Step [940/1248], Loss: 0.0131\n",
      "Epoch [42/100], Step [960/1248], Loss: 0.0012\n",
      "Epoch [42/100], Step [980/1248], Loss: 0.0111\n",
      "Epoch [42/100], Step [1000/1248], Loss: 0.0016\n",
      "Epoch [42/100], Step [1020/1248], Loss: 0.0083\n",
      "Epoch [42/100], Step [1040/1248], Loss: 0.2103\n",
      "Epoch [42/100], Step [1060/1248], Loss: 0.0026\n",
      "Epoch [42/100], Step [1080/1248], Loss: 0.0520\n",
      "Epoch [42/100], Step [1100/1248], Loss: 0.1881\n",
      "Epoch [42/100], Step [1120/1248], Loss: 0.0024\n",
      "Epoch [42/100], Step [1140/1248], Loss: 0.2765\n",
      "Epoch [42/100], Step [1160/1248], Loss: 0.0248\n",
      "Epoch [42/100], Step [1180/1248], Loss: 0.0026\n",
      "Epoch [42/100], Step [1200/1248], Loss: 0.0112\n",
      "Epoch [42/100], Step [1220/1248], Loss: 0.0310\n",
      "Epoch [42/100], Step [1240/1248], Loss: 0.0015\n",
      "\n",
      "train-loss: 0.1923, train-acc: 98.4269\n",
      "validation loss: 1.5368, validation acc: 68.6141\n",
      "\n",
      "Epoch 43\n",
      "\n",
      "Epoch [43/100], Step [0/1248], Loss: 0.0033\n",
      "Epoch [43/100], Step [20/1248], Loss: 0.1052\n",
      "Epoch [43/100], Step [40/1248], Loss: 0.1421\n",
      "Epoch [43/100], Step [60/1248], Loss: 0.0013\n",
      "Epoch [43/100], Step [80/1248], Loss: 0.0026\n",
      "Epoch [43/100], Step [100/1248], Loss: 0.0440\n",
      "Epoch [43/100], Step [120/1248], Loss: 0.0054\n",
      "Epoch [43/100], Step [140/1248], Loss: 0.0010\n",
      "Epoch [43/100], Step [160/1248], Loss: 0.0038\n",
      "Epoch [43/100], Step [180/1248], Loss: 0.0052\n",
      "Epoch [43/100], Step [200/1248], Loss: 0.0090\n",
      "Epoch [43/100], Step [220/1248], Loss: 0.0198\n",
      "Epoch [43/100], Step [240/1248], Loss: 0.0043\n",
      "Epoch [43/100], Step [260/1248], Loss: 0.0298\n",
      "Epoch [43/100], Step [280/1248], Loss: 0.0037\n",
      "Epoch [43/100], Step [300/1248], Loss: 0.0658\n",
      "Epoch [43/100], Step [320/1248], Loss: 0.1053\n",
      "Epoch [43/100], Step [340/1248], Loss: 0.1477\n",
      "Epoch [43/100], Step [360/1248], Loss: 0.0016\n",
      "Epoch [43/100], Step [380/1248], Loss: 0.1340\n",
      "Epoch [43/100], Step [400/1248], Loss: 0.0913\n",
      "Epoch [43/100], Step [420/1248], Loss: 0.2275\n",
      "Epoch [43/100], Step [440/1248], Loss: 0.0290\n",
      "Epoch [43/100], Step [460/1248], Loss: 0.0135\n",
      "Epoch [43/100], Step [480/1248], Loss: 0.0055\n",
      "Epoch [43/100], Step [500/1248], Loss: 0.0684\n",
      "Epoch [43/100], Step [520/1248], Loss: 0.0020\n",
      "Epoch [43/100], Step [540/1248], Loss: 0.0284\n",
      "Epoch [43/100], Step [560/1248], Loss: 0.0497\n",
      "Epoch [43/100], Step [580/1248], Loss: 0.1454\n",
      "Epoch [43/100], Step [600/1248], Loss: 0.0040\n",
      "Epoch [43/100], Step [620/1248], Loss: 0.0070\n",
      "Epoch [43/100], Step [640/1248], Loss: 0.0144\n",
      "Epoch [43/100], Step [660/1248], Loss: 0.0060\n",
      "Epoch [43/100], Step [680/1248], Loss: 0.0970\n",
      "Epoch [43/100], Step [700/1248], Loss: 0.0020\n",
      "Epoch [43/100], Step [720/1248], Loss: 0.0001\n",
      "Epoch [43/100], Step [740/1248], Loss: 0.0009\n",
      "Epoch [43/100], Step [760/1248], Loss: 0.0148\n",
      "Epoch [43/100], Step [780/1248], Loss: 0.0182\n",
      "Epoch [43/100], Step [800/1248], Loss: 0.0020\n",
      "Epoch [43/100], Step [820/1248], Loss: 0.3193\n",
      "Epoch [43/100], Step [840/1248], Loss: 0.0026\n",
      "Epoch [43/100], Step [860/1248], Loss: 0.0005\n",
      "Epoch [43/100], Step [880/1248], Loss: 0.0020\n",
      "Epoch [43/100], Step [900/1248], Loss: 0.2492\n",
      "Epoch [43/100], Step [920/1248], Loss: 0.0023\n",
      "Epoch [43/100], Step [940/1248], Loss: 0.0143\n",
      "Epoch [43/100], Step [960/1248], Loss: 0.0072\n",
      "Epoch [43/100], Step [980/1248], Loss: 0.0711\n",
      "Epoch [43/100], Step [1000/1248], Loss: 0.0028\n",
      "Epoch [43/100], Step [1020/1248], Loss: 0.2081\n",
      "Epoch [43/100], Step [1040/1248], Loss: 0.0473\n",
      "Epoch [43/100], Step [1060/1248], Loss: 0.0420\n",
      "Epoch [43/100], Step [1080/1248], Loss: 0.0351\n",
      "Epoch [43/100], Step [1100/1248], Loss: 0.0047\n",
      "Epoch [43/100], Step [1120/1248], Loss: 0.0482\n",
      "Epoch [43/100], Step [1140/1248], Loss: 0.2385\n",
      "Epoch [43/100], Step [1160/1248], Loss: 0.0048\n",
      "Epoch [43/100], Step [1180/1248], Loss: 0.0005\n",
      "Epoch [43/100], Step [1200/1248], Loss: 0.0348\n",
      "Epoch [43/100], Step [1220/1248], Loss: 0.0017\n",
      "Epoch [43/100], Step [1240/1248], Loss: 0.0168\n",
      "\n",
      "train-loss: 0.1888, train-acc: 98.6172\n",
      "validation loss: 1.5465, validation acc: 66.8816\n",
      "\n",
      "Epoch 44\n",
      "\n",
      "Epoch [44/100], Step [0/1248], Loss: 0.0071\n",
      "Epoch [44/100], Step [20/1248], Loss: 0.1036\n",
      "Epoch [44/100], Step [40/1248], Loss: 0.0630\n",
      "Epoch [44/100], Step [60/1248], Loss: 0.0530\n",
      "Epoch [44/100], Step [80/1248], Loss: 0.0025\n",
      "Epoch [44/100], Step [100/1248], Loss: 0.0034\n",
      "Epoch [44/100], Step [120/1248], Loss: 0.0247\n",
      "Epoch [44/100], Step [140/1248], Loss: 0.1316\n",
      "Epoch [44/100], Step [160/1248], Loss: 0.0221\n",
      "Epoch [44/100], Step [180/1248], Loss: 0.0028\n",
      "Epoch [44/100], Step [200/1248], Loss: 0.0276\n",
      "Epoch [44/100], Step [220/1248], Loss: 0.0031\n",
      "Epoch [44/100], Step [240/1248], Loss: 0.0022\n",
      "Epoch [44/100], Step [260/1248], Loss: 0.0044\n",
      "Epoch [44/100], Step [280/1248], Loss: 0.0107\n",
      "Epoch [44/100], Step [300/1248], Loss: 0.0061\n",
      "Epoch [44/100], Step [320/1248], Loss: 0.0004\n",
      "Epoch [44/100], Step [340/1248], Loss: 0.0155\n",
      "Epoch [44/100], Step [360/1248], Loss: 0.0009\n",
      "Epoch [44/100], Step [380/1248], Loss: 0.0055\n",
      "Epoch [44/100], Step [400/1248], Loss: 0.0175\n",
      "Epoch [44/100], Step [420/1248], Loss: 0.1179\n",
      "Epoch [44/100], Step [440/1248], Loss: 0.0084\n",
      "Epoch [44/100], Step [460/1248], Loss: 0.0518\n",
      "Epoch [44/100], Step [480/1248], Loss: 0.0089\n",
      "Epoch [44/100], Step [500/1248], Loss: 0.0019\n",
      "Epoch [44/100], Step [520/1248], Loss: 0.0326\n",
      "Epoch [44/100], Step [540/1248], Loss: 0.0018\n",
      "Epoch [44/100], Step [560/1248], Loss: 0.0202\n",
      "Epoch [44/100], Step [580/1248], Loss: 0.1034\n",
      "Epoch [44/100], Step [600/1248], Loss: 0.0062\n",
      "Epoch [44/100], Step [620/1248], Loss: 0.1286\n",
      "Epoch [44/100], Step [640/1248], Loss: 0.0221\n",
      "Epoch [44/100], Step [660/1248], Loss: 0.0039\n",
      "Epoch [44/100], Step [680/1248], Loss: 0.0007\n",
      "Epoch [44/100], Step [700/1248], Loss: 0.1663\n",
      "Epoch [44/100], Step [720/1248], Loss: 0.0007\n",
      "Epoch [44/100], Step [740/1248], Loss: 0.0020\n",
      "Epoch [44/100], Step [760/1248], Loss: 0.0109\n",
      "Epoch [44/100], Step [780/1248], Loss: 0.0233\n",
      "Epoch [44/100], Step [800/1248], Loss: 0.0045\n",
      "Epoch [44/100], Step [820/1248], Loss: 0.0042\n",
      "Epoch [44/100], Step [840/1248], Loss: 0.1003\n",
      "Epoch [44/100], Step [860/1248], Loss: 0.0079\n",
      "Epoch [44/100], Step [880/1248], Loss: 0.0064\n",
      "Epoch [44/100], Step [900/1248], Loss: 0.0027\n",
      "Epoch [44/100], Step [920/1248], Loss: 0.0400\n",
      "Epoch [44/100], Step [940/1248], Loss: 0.0014\n",
      "Epoch [44/100], Step [960/1248], Loss: 0.0031\n",
      "Epoch [44/100], Step [980/1248], Loss: 0.0076\n",
      "Epoch [44/100], Step [1000/1248], Loss: 0.0214\n",
      "Epoch [44/100], Step [1020/1248], Loss: 0.0014\n",
      "Epoch [44/100], Step [1040/1248], Loss: 0.1115\n",
      "Epoch [44/100], Step [1060/1248], Loss: 0.0067\n",
      "Epoch [44/100], Step [1080/1248], Loss: 0.0058\n",
      "Epoch [44/100], Step [1100/1248], Loss: 0.0033\n",
      "Epoch [44/100], Step [1120/1248], Loss: 0.0012\n",
      "Epoch [44/100], Step [1140/1248], Loss: 0.0266\n",
      "Epoch [44/100], Step [1160/1248], Loss: 0.0086\n",
      "Epoch [44/100], Step [1180/1248], Loss: 0.0008\n",
      "Epoch [44/100], Step [1200/1248], Loss: 0.0510\n",
      "Epoch [44/100], Step [1220/1248], Loss: 0.0960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Step [1240/1248], Loss: 0.0022\n",
      "\n",
      "train-loss: 0.1853, train-acc: 98.7725\n",
      "validation loss: 1.5564, validation acc: 68.1462\n",
      "\n",
      "Epoch 45\n",
      "\n",
      "Epoch [45/100], Step [0/1248], Loss: 0.0055\n",
      "Epoch [45/100], Step [20/1248], Loss: 0.0027\n",
      "Epoch [45/100], Step [40/1248], Loss: 0.0155\n",
      "Epoch [45/100], Step [60/1248], Loss: 0.0075\n",
      "Epoch [45/100], Step [80/1248], Loss: 0.0004\n",
      "Epoch [45/100], Step [100/1248], Loss: 0.0177\n",
      "Epoch [45/100], Step [120/1248], Loss: 0.0009\n",
      "Epoch [45/100], Step [140/1248], Loss: 0.0027\n",
      "Epoch [45/100], Step [160/1248], Loss: 0.0015\n",
      "Epoch [45/100], Step [180/1248], Loss: 0.0100\n",
      "Epoch [45/100], Step [200/1248], Loss: 0.0280\n",
      "Epoch [45/100], Step [220/1248], Loss: 0.0074\n",
      "Epoch [45/100], Step [240/1248], Loss: 0.0018\n",
      "Epoch [45/100], Step [260/1248], Loss: 0.1345\n",
      "Epoch [45/100], Step [280/1248], Loss: 0.0233\n",
      "Epoch [45/100], Step [300/1248], Loss: 0.0711\n",
      "Epoch [45/100], Step [320/1248], Loss: 0.0400\n",
      "Epoch [45/100], Step [340/1248], Loss: 0.0217\n",
      "Epoch [45/100], Step [360/1248], Loss: 0.0963\n",
      "Epoch [45/100], Step [380/1248], Loss: 0.0088\n",
      "Epoch [45/100], Step [400/1248], Loss: 0.0143\n",
      "Epoch [45/100], Step [420/1248], Loss: 0.0120\n",
      "Epoch [45/100], Step [440/1248], Loss: 0.0116\n",
      "Epoch [45/100], Step [460/1248], Loss: 0.0096\n",
      "Epoch [45/100], Step [480/1248], Loss: 0.0033\n",
      "Epoch [45/100], Step [500/1248], Loss: 0.3268\n",
      "Epoch [45/100], Step [520/1248], Loss: 0.0199\n",
      "Epoch [45/100], Step [540/1248], Loss: 0.1883\n",
      "Epoch [45/100], Step [560/1248], Loss: 0.0030\n",
      "Epoch [45/100], Step [580/1248], Loss: 0.0313\n",
      "Epoch [45/100], Step [600/1248], Loss: 0.0178\n",
      "Epoch [45/100], Step [620/1248], Loss: 0.0014\n",
      "Epoch [45/100], Step [640/1248], Loss: 0.1056\n",
      "Epoch [45/100], Step [660/1248], Loss: 0.0023\n",
      "Epoch [45/100], Step [680/1248], Loss: 0.0024\n",
      "Epoch [45/100], Step [700/1248], Loss: 0.0210\n",
      "Epoch [45/100], Step [720/1248], Loss: 0.0026\n",
      "Epoch [45/100], Step [740/1248], Loss: 0.0020\n",
      "Epoch [45/100], Step [760/1248], Loss: 0.0172\n",
      "Epoch [45/100], Step [780/1248], Loss: 0.0160\n",
      "Epoch [45/100], Step [800/1248], Loss: 0.1403\n",
      "Epoch [45/100], Step [820/1248], Loss: 0.0138\n",
      "Epoch [45/100], Step [840/1248], Loss: 0.1152\n",
      "Epoch [45/100], Step [860/1248], Loss: 0.0376\n",
      "Epoch [45/100], Step [880/1248], Loss: 0.0343\n",
      "Epoch [45/100], Step [900/1248], Loss: 0.0010\n",
      "Epoch [45/100], Step [920/1248], Loss: 0.0021\n",
      "Epoch [45/100], Step [940/1248], Loss: 0.0792\n",
      "Epoch [45/100], Step [960/1248], Loss: 0.2401\n",
      "Epoch [45/100], Step [980/1248], Loss: 0.0006\n",
      "Epoch [45/100], Step [1000/1248], Loss: 0.0029\n",
      "Epoch [45/100], Step [1020/1248], Loss: 0.2063\n",
      "Epoch [45/100], Step [1040/1248], Loss: 0.0004\n",
      "Epoch [45/100], Step [1060/1248], Loss: 0.0173\n",
      "Epoch [45/100], Step [1080/1248], Loss: 0.0210\n",
      "Epoch [45/100], Step [1100/1248], Loss: 0.0953\n",
      "Epoch [45/100], Step [1120/1248], Loss: 0.0135\n",
      "Epoch [45/100], Step [1140/1248], Loss: 0.0543\n",
      "Epoch [45/100], Step [1160/1248], Loss: 0.0170\n",
      "Epoch [45/100], Step [1180/1248], Loss: 0.0216\n",
      "Epoch [45/100], Step [1200/1248], Loss: 0.0205\n",
      "Epoch [45/100], Step [1220/1248], Loss: 0.0047\n",
      "Epoch [45/100], Step [1240/1248], Loss: 0.0044\n",
      "\n",
      "train-loss: 0.1821, train-acc: 98.6874\n",
      "validation loss: 1.5634, validation acc: 69.1452\n",
      "\n",
      "Epoch 46\n",
      "\n",
      "Epoch [46/100], Step [0/1248], Loss: 0.0005\n",
      "Epoch [46/100], Step [20/1248], Loss: 0.0136\n",
      "Epoch [46/100], Step [40/1248], Loss: 0.0166\n",
      "Epoch [46/100], Step [60/1248], Loss: 0.0016\n",
      "Epoch [46/100], Step [80/1248], Loss: 0.0043\n",
      "Epoch [46/100], Step [100/1248], Loss: 0.0034\n",
      "Epoch [46/100], Step [120/1248], Loss: 0.0096\n",
      "Epoch [46/100], Step [140/1248], Loss: 0.6011\n",
      "Epoch [46/100], Step [160/1248], Loss: 0.0530\n",
      "Epoch [46/100], Step [180/1248], Loss: 0.0118\n",
      "Epoch [46/100], Step [200/1248], Loss: 0.0008\n",
      "Epoch [46/100], Step [220/1248], Loss: 0.0547\n",
      "Epoch [46/100], Step [240/1248], Loss: 0.0511\n",
      "Epoch [46/100], Step [260/1248], Loss: 0.0155\n",
      "Epoch [46/100], Step [280/1248], Loss: 0.0519\n",
      "Epoch [46/100], Step [300/1248], Loss: 0.0005\n",
      "Epoch [46/100], Step [320/1248], Loss: 0.0100\n",
      "Epoch [46/100], Step [340/1248], Loss: 0.0040\n",
      "Epoch [46/100], Step [360/1248], Loss: 0.0029\n",
      "Epoch [46/100], Step [380/1248], Loss: 0.0026\n",
      "Epoch [46/100], Step [400/1248], Loss: 0.0034\n",
      "Epoch [46/100], Step [420/1248], Loss: 0.0124\n",
      "Epoch [46/100], Step [440/1248], Loss: 0.0090\n",
      "Epoch [46/100], Step [460/1248], Loss: 0.0005\n",
      "Epoch [46/100], Step [480/1248], Loss: 0.0093\n",
      "Epoch [46/100], Step [500/1248], Loss: 0.0008\n",
      "Epoch [46/100], Step [520/1248], Loss: 0.0025\n",
      "Epoch [46/100], Step [540/1248], Loss: 0.0372\n",
      "Epoch [46/100], Step [560/1248], Loss: 0.0035\n",
      "Epoch [46/100], Step [580/1248], Loss: 0.0008\n",
      "Epoch [46/100], Step [600/1248], Loss: 0.1129\n",
      "Epoch [46/100], Step [620/1248], Loss: 0.0088\n",
      "Epoch [46/100], Step [640/1248], Loss: 0.0002\n",
      "Epoch [46/100], Step [660/1248], Loss: 0.0014\n",
      "Epoch [46/100], Step [680/1248], Loss: 0.0004\n",
      "Epoch [46/100], Step [700/1248], Loss: 0.0606\n",
      "Epoch [46/100], Step [720/1248], Loss: 0.1016\n",
      "Epoch [46/100], Step [740/1248], Loss: 0.0028\n",
      "Epoch [46/100], Step [760/1248], Loss: 0.0056\n",
      "Epoch [46/100], Step [780/1248], Loss: 0.0166\n",
      "Epoch [46/100], Step [800/1248], Loss: 0.0072\n",
      "Epoch [46/100], Step [820/1248], Loss: 0.0101\n",
      "Epoch [46/100], Step [840/1248], Loss: 0.0374\n",
      "Epoch [46/100], Step [860/1248], Loss: 0.2464\n",
      "Epoch [46/100], Step [880/1248], Loss: 0.0010\n",
      "Epoch [46/100], Step [900/1248], Loss: 0.0455\n",
      "Epoch [46/100], Step [920/1248], Loss: 0.0092\n",
      "Epoch [46/100], Step [940/1248], Loss: 0.0024\n",
      "Epoch [46/100], Step [960/1248], Loss: 0.0067\n",
      "Epoch [46/100], Step [980/1248], Loss: 0.0053\n",
      "Epoch [46/100], Step [1000/1248], Loss: 0.0136\n",
      "Epoch [46/100], Step [1020/1248], Loss: 0.0093\n",
      "Epoch [46/100], Step [1040/1248], Loss: 0.0033\n",
      "Epoch [46/100], Step [1060/1248], Loss: 0.0491\n",
      "Epoch [46/100], Step [1080/1248], Loss: 0.1453\n",
      "Epoch [46/100], Step [1100/1248], Loss: 0.0058\n",
      "Epoch [46/100], Step [1120/1248], Loss: 0.0110\n",
      "Epoch [46/100], Step [1140/1248], Loss: 0.1103\n",
      "Epoch [46/100], Step [1160/1248], Loss: 0.0036\n",
      "Epoch [46/100], Step [1180/1248], Loss: 0.0016\n",
      "Epoch [46/100], Step [1200/1248], Loss: 0.2338\n",
      "Epoch [46/100], Step [1220/1248], Loss: 0.2959\n",
      "Epoch [46/100], Step [1240/1248], Loss: 0.0008\n",
      "\n",
      "train-loss: 0.1789, train-acc: 98.7325\n",
      "validation loss: 1.5691, validation acc: 67.8048\n",
      "\n",
      "Epoch 47\n",
      "\n",
      "Epoch [47/100], Step [0/1248], Loss: 0.0020\n",
      "Epoch [47/100], Step [20/1248], Loss: 0.0174\n",
      "Epoch [47/100], Step [40/1248], Loss: 0.0845\n",
      "Epoch [47/100], Step [60/1248], Loss: 0.0113\n",
      "Epoch [47/100], Step [80/1248], Loss: 0.0057\n",
      "Epoch [47/100], Step [100/1248], Loss: 0.0097\n",
      "Epoch [47/100], Step [120/1248], Loss: 0.0026\n",
      "Epoch [47/100], Step [140/1248], Loss: 0.0355\n",
      "Epoch [47/100], Step [160/1248], Loss: 0.0083\n",
      "Epoch [47/100], Step [180/1248], Loss: 0.0017\n",
      "Epoch [47/100], Step [200/1248], Loss: 0.0017\n",
      "Epoch [47/100], Step [220/1248], Loss: 0.0009\n",
      "Epoch [47/100], Step [240/1248], Loss: 0.0054\n",
      "Epoch [47/100], Step [260/1248], Loss: 0.0105\n",
      "Epoch [47/100], Step [280/1248], Loss: 0.0013\n",
      "Epoch [47/100], Step [300/1248], Loss: 0.0527\n",
      "Epoch [47/100], Step [320/1248], Loss: 0.0122\n",
      "Epoch [47/100], Step [340/1248], Loss: 0.0001\n",
      "Epoch [47/100], Step [360/1248], Loss: 0.0644\n",
      "Epoch [47/100], Step [380/1248], Loss: 0.0021\n",
      "Epoch [47/100], Step [400/1248], Loss: 0.0007\n",
      "Epoch [47/100], Step [420/1248], Loss: 0.0147\n",
      "Epoch [47/100], Step [440/1248], Loss: 0.0027\n",
      "Epoch [47/100], Step [460/1248], Loss: 0.0173\n",
      "Epoch [47/100], Step [480/1248], Loss: 0.0009\n",
      "Epoch [47/100], Step [500/1248], Loss: 0.0076\n",
      "Epoch [47/100], Step [520/1248], Loss: 0.0032\n",
      "Epoch [47/100], Step [540/1248], Loss: 0.2210\n",
      "Epoch [47/100], Step [560/1248], Loss: 0.0026\n",
      "Epoch [47/100], Step [580/1248], Loss: 0.0120\n",
      "Epoch [47/100], Step [600/1248], Loss: 0.2212\n",
      "Epoch [47/100], Step [620/1248], Loss: 0.0021\n",
      "Epoch [47/100], Step [640/1248], Loss: 0.0033\n",
      "Epoch [47/100], Step [660/1248], Loss: 0.0015\n",
      "Epoch [47/100], Step [680/1248], Loss: 0.0120\n",
      "Epoch [47/100], Step [700/1248], Loss: 0.0029\n",
      "Epoch [47/100], Step [720/1248], Loss: 0.0278\n",
      "Epoch [47/100], Step [740/1248], Loss: 0.0539\n",
      "Epoch [47/100], Step [760/1248], Loss: 0.0029\n",
      "Epoch [47/100], Step [780/1248], Loss: 0.0010\n",
      "Epoch [47/100], Step [800/1248], Loss: 0.0070\n",
      "Epoch [47/100], Step [820/1248], Loss: 0.0437\n",
      "Epoch [47/100], Step [840/1248], Loss: 0.2339\n",
      "Epoch [47/100], Step [860/1248], Loss: 0.0015\n",
      "Epoch [47/100], Step [880/1248], Loss: 0.0006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100], Step [900/1248], Loss: 0.0006\n",
      "Epoch [47/100], Step [920/1248], Loss: 0.0034\n",
      "Epoch [47/100], Step [940/1248], Loss: 0.0295\n",
      "Epoch [47/100], Step [960/1248], Loss: 0.0017\n",
      "Epoch [47/100], Step [980/1248], Loss: 0.0053\n",
      "Epoch [47/100], Step [1000/1248], Loss: 0.0039\n",
      "Epoch [47/100], Step [1020/1248], Loss: 0.0442\n",
      "Epoch [47/100], Step [1040/1248], Loss: 0.1015\n",
      "Epoch [47/100], Step [1060/1248], Loss: 0.1163\n",
      "Epoch [47/100], Step [1080/1248], Loss: 0.0136\n",
      "Epoch [47/100], Step [1100/1248], Loss: 0.0047\n",
      "Epoch [47/100], Step [1120/1248], Loss: 0.0161\n",
      "Epoch [47/100], Step [1140/1248], Loss: 0.0175\n",
      "Epoch [47/100], Step [1160/1248], Loss: 0.0451\n",
      "Epoch [47/100], Step [1180/1248], Loss: 0.0166\n",
      "Epoch [47/100], Step [1200/1248], Loss: 0.0069\n",
      "Epoch [47/100], Step [1220/1248], Loss: 0.0370\n",
      "Epoch [47/100], Step [1240/1248], Loss: 0.0013\n",
      "\n",
      "train-loss: 0.1758, train-acc: 98.8277\n",
      "validation loss: 1.5781, validation acc: 68.1841\n",
      "\n",
      "Epoch 48\n",
      "\n",
      "Epoch [48/100], Step [0/1248], Loss: 0.0027\n",
      "Epoch [48/100], Step [20/1248], Loss: 0.0112\n",
      "Epoch [48/100], Step [40/1248], Loss: 0.0205\n",
      "Epoch [48/100], Step [60/1248], Loss: 0.0017\n",
      "Epoch [48/100], Step [80/1248], Loss: 0.3309\n",
      "Epoch [48/100], Step [100/1248], Loss: 0.0118\n",
      "Epoch [48/100], Step [120/1248], Loss: 0.0092\n",
      "Epoch [48/100], Step [140/1248], Loss: 0.0084\n",
      "Epoch [48/100], Step [160/1248], Loss: 0.0007\n",
      "Epoch [48/100], Step [180/1248], Loss: 0.0015\n",
      "Epoch [48/100], Step [200/1248], Loss: 0.0007\n",
      "Epoch [48/100], Step [220/1248], Loss: 0.0013\n",
      "Epoch [48/100], Step [240/1248], Loss: 0.0006\n",
      "Epoch [48/100], Step [260/1248], Loss: 0.0119\n",
      "Epoch [48/100], Step [280/1248], Loss: 0.0028\n",
      "Epoch [48/100], Step [300/1248], Loss: 0.0375\n",
      "Epoch [48/100], Step [320/1248], Loss: 0.1000\n",
      "Epoch [48/100], Step [340/1248], Loss: 0.0001\n",
      "Epoch [48/100], Step [360/1248], Loss: 0.0520\n",
      "Epoch [48/100], Step [380/1248], Loss: 0.0110\n",
      "Epoch [48/100], Step [400/1248], Loss: 0.0009\n",
      "Epoch [48/100], Step [420/1248], Loss: 0.0012\n",
      "Epoch [48/100], Step [440/1248], Loss: 0.0028\n",
      "Epoch [48/100], Step [460/1248], Loss: 0.0117\n",
      "Epoch [48/100], Step [480/1248], Loss: 0.0055\n",
      "Epoch [48/100], Step [500/1248], Loss: 0.0008\n",
      "Epoch [48/100], Step [520/1248], Loss: 0.0080\n",
      "Epoch [48/100], Step [540/1248], Loss: 0.0128\n",
      "Epoch [48/100], Step [560/1248], Loss: 0.0034\n",
      "Epoch [48/100], Step [580/1248], Loss: 0.0148\n",
      "Epoch [48/100], Step [600/1248], Loss: 0.0091\n",
      "Epoch [48/100], Step [620/1248], Loss: 0.0412\n",
      "Epoch [48/100], Step [640/1248], Loss: 0.0161\n",
      "Epoch [48/100], Step [660/1248], Loss: 0.0045\n",
      "Epoch [48/100], Step [680/1248], Loss: 0.0129\n",
      "Epoch [48/100], Step [700/1248], Loss: 0.0033\n",
      "Epoch [48/100], Step [720/1248], Loss: 0.0006\n",
      "Epoch [48/100], Step [740/1248], Loss: 0.0021\n",
      "Epoch [48/100], Step [760/1248], Loss: 0.0008\n",
      "Epoch [48/100], Step [780/1248], Loss: 0.0642\n",
      "Epoch [48/100], Step [800/1248], Loss: 0.0022\n",
      "Epoch [48/100], Step [820/1248], Loss: 0.0039\n",
      "Epoch [48/100], Step [840/1248], Loss: 0.0979\n",
      "Epoch [48/100], Step [860/1248], Loss: 0.0021\n",
      "Epoch [48/100], Step [880/1248], Loss: 0.0209\n",
      "Epoch [48/100], Step [900/1248], Loss: 0.0374\n",
      "Epoch [48/100], Step [920/1248], Loss: 0.0049\n",
      "Epoch [48/100], Step [940/1248], Loss: 0.0015\n",
      "Epoch [48/100], Step [960/1248], Loss: 0.0163\n",
      "Epoch [48/100], Step [980/1248], Loss: 0.0015\n",
      "Epoch [48/100], Step [1000/1248], Loss: 0.0012\n",
      "Epoch [48/100], Step [1020/1248], Loss: 0.0098\n",
      "Epoch [48/100], Step [1040/1248], Loss: 0.0117\n",
      "Epoch [48/100], Step [1060/1248], Loss: 0.0048\n",
      "Epoch [48/100], Step [1080/1248], Loss: 0.0546\n",
      "Epoch [48/100], Step [1100/1248], Loss: 0.0117\n",
      "Epoch [48/100], Step [1120/1248], Loss: 0.0019\n",
      "Epoch [48/100], Step [1140/1248], Loss: 0.0150\n",
      "Epoch [48/100], Step [1160/1248], Loss: 0.0329\n",
      "Epoch [48/100], Step [1180/1248], Loss: 0.0002\n",
      "Epoch [48/100], Step [1200/1248], Loss: 0.0024\n",
      "Epoch [48/100], Step [1220/1248], Loss: 0.0003\n",
      "Epoch [48/100], Step [1240/1248], Loss: 0.0009\n",
      "\n",
      "train-loss: 0.1728, train-acc: 98.9028\n",
      "validation loss: 1.5865, validation acc: 67.1093\n",
      "\n",
      "Epoch 49\n",
      "\n",
      "Epoch [49/100], Step [0/1248], Loss: 0.0011\n",
      "Epoch [49/100], Step [20/1248], Loss: 0.0099\n",
      "Epoch [49/100], Step [40/1248], Loss: 0.0061\n",
      "Epoch [49/100], Step [60/1248], Loss: 0.0072\n",
      "Epoch [49/100], Step [80/1248], Loss: 0.0075\n",
      "Epoch [49/100], Step [100/1248], Loss: 0.0029\n",
      "Epoch [49/100], Step [120/1248], Loss: 0.0187\n",
      "Epoch [49/100], Step [140/1248], Loss: 0.0166\n",
      "Epoch [49/100], Step [160/1248], Loss: 0.0002\n",
      "Epoch [49/100], Step [180/1248], Loss: 0.0408\n",
      "Epoch [49/100], Step [200/1248], Loss: 0.4233\n",
      "Epoch [49/100], Step [220/1248], Loss: 0.0844\n",
      "Epoch [49/100], Step [240/1248], Loss: 0.0004\n",
      "Epoch [49/100], Step [260/1248], Loss: 0.0005\n",
      "Epoch [49/100], Step [280/1248], Loss: 0.0179\n",
      "Epoch [49/100], Step [300/1248], Loss: 0.0087\n",
      "Epoch [49/100], Step [320/1248], Loss: 0.0047\n",
      "Epoch [49/100], Step [340/1248], Loss: 0.0005\n",
      "Epoch [49/100], Step [360/1248], Loss: 0.0017\n",
      "Epoch [49/100], Step [380/1248], Loss: 0.0005\n",
      "Epoch [49/100], Step [400/1248], Loss: 0.0268\n",
      "Epoch [49/100], Step [420/1248], Loss: 0.0296\n",
      "Epoch [49/100], Step [440/1248], Loss: 0.0047\n",
      "Epoch [49/100], Step [460/1248], Loss: 0.0052\n",
      "Epoch [49/100], Step [480/1248], Loss: 0.0321\n",
      "Epoch [49/100], Step [500/1248], Loss: 0.0123\n",
      "Epoch [49/100], Step [520/1248], Loss: 0.0008\n",
      "Epoch [49/100], Step [540/1248], Loss: 0.0082\n",
      "Epoch [49/100], Step [560/1248], Loss: 0.0182\n",
      "Epoch [49/100], Step [580/1248], Loss: 0.1761\n",
      "Epoch [49/100], Step [600/1248], Loss: 0.0008\n",
      "Epoch [49/100], Step [620/1248], Loss: 0.0079\n",
      "Epoch [49/100], Step [640/1248], Loss: 0.0151\n",
      "Epoch [49/100], Step [660/1248], Loss: 0.1287\n",
      "Epoch [49/100], Step [680/1248], Loss: 0.1010\n",
      "Epoch [49/100], Step [700/1248], Loss: 0.1259\n",
      "Epoch [49/100], Step [720/1248], Loss: 0.0165\n",
      "Epoch [49/100], Step [740/1248], Loss: 0.0388\n",
      "Epoch [49/100], Step [760/1248], Loss: 0.0127\n",
      "Epoch [49/100], Step [780/1248], Loss: 0.0257\n",
      "Epoch [49/100], Step [800/1248], Loss: 0.0074\n",
      "Epoch [49/100], Step [820/1248], Loss: 0.0753\n",
      "Epoch [49/100], Step [840/1248], Loss: 0.0084\n",
      "Epoch [49/100], Step [860/1248], Loss: 0.0396\n",
      "Epoch [49/100], Step [880/1248], Loss: 0.0586\n",
      "Epoch [49/100], Step [900/1248], Loss: 0.0225\n",
      "Epoch [49/100], Step [920/1248], Loss: 0.0032\n",
      "Epoch [49/100], Step [940/1248], Loss: 0.0094\n",
      "Epoch [49/100], Step [960/1248], Loss: 0.0287\n",
      "Epoch [49/100], Step [980/1248], Loss: 0.1005\n",
      "Epoch [49/100], Step [1000/1248], Loss: 0.1649\n",
      "Epoch [49/100], Step [1020/1248], Loss: 0.1177\n",
      "Epoch [49/100], Step [1040/1248], Loss: 0.0236\n",
      "Epoch [49/100], Step [1060/1248], Loss: 0.0952\n",
      "Epoch [49/100], Step [1080/1248], Loss: 0.2174\n",
      "Epoch [49/100], Step [1100/1248], Loss: 0.0422\n",
      "Epoch [49/100], Step [1120/1248], Loss: 0.0129\n",
      "Epoch [49/100], Step [1140/1248], Loss: 0.0090\n",
      "Epoch [49/100], Step [1160/1248], Loss: 0.0056\n",
      "Epoch [49/100], Step [1180/1248], Loss: 0.0466\n",
      "Epoch [49/100], Step [1200/1248], Loss: 0.0295\n",
      "Epoch [49/100], Step [1220/1248], Loss: 0.0459\n",
      "Epoch [49/100], Step [1240/1248], Loss: 0.0618\n",
      "\n",
      "train-loss: 0.1702, train-acc: 98.5170\n",
      "validation loss: 1.5933, validation acc: 67.7668\n",
      "\n",
      "Epoch 50\n",
      "\n",
      "Epoch [50/100], Step [0/1248], Loss: 0.0125\n",
      "Epoch [50/100], Step [20/1248], Loss: 0.0932\n",
      "Epoch [50/100], Step [40/1248], Loss: 0.0091\n",
      "Epoch [50/100], Step [60/1248], Loss: 0.0067\n",
      "Epoch [50/100], Step [80/1248], Loss: 0.0025\n",
      "Epoch [50/100], Step [100/1248], Loss: 0.0511\n",
      "Epoch [50/100], Step [120/1248], Loss: 0.0151\n",
      "Epoch [50/100], Step [140/1248], Loss: 0.0037\n",
      "Epoch [50/100], Step [160/1248], Loss: 0.0157\n",
      "Epoch [50/100], Step [180/1248], Loss: 0.1582\n",
      "Epoch [50/100], Step [200/1248], Loss: 0.0216\n",
      "Epoch [50/100], Step [220/1248], Loss: 0.0002\n",
      "Epoch [50/100], Step [240/1248], Loss: 0.0023\n",
      "Epoch [50/100], Step [260/1248], Loss: 0.0015\n",
      "Epoch [50/100], Step [280/1248], Loss: 0.0182\n",
      "Epoch [50/100], Step [300/1248], Loss: 0.0185\n",
      "Epoch [50/100], Step [320/1248], Loss: 0.0014\n",
      "Epoch [50/100], Step [340/1248], Loss: 0.0049\n",
      "Epoch [50/100], Step [360/1248], Loss: 0.0023\n",
      "Epoch [50/100], Step [380/1248], Loss: 0.0058\n",
      "Epoch [50/100], Step [400/1248], Loss: 0.0008\n",
      "Epoch [50/100], Step [420/1248], Loss: 0.0009\n",
      "Epoch [50/100], Step [440/1248], Loss: 0.0048\n",
      "Epoch [50/100], Step [460/1248], Loss: 0.0010\n",
      "Epoch [50/100], Step [480/1248], Loss: 0.1660\n",
      "Epoch [50/100], Step [500/1248], Loss: 0.0059\n",
      "Epoch [50/100], Step [520/1248], Loss: 0.0200\n",
      "Epoch [50/100], Step [540/1248], Loss: 0.0154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Step [560/1248], Loss: 0.0076\n",
      "Epoch [50/100], Step [580/1248], Loss: 0.0040\n",
      "Epoch [50/100], Step [600/1248], Loss: 0.0296\n",
      "Epoch [50/100], Step [620/1248], Loss: 0.0698\n",
      "Epoch [50/100], Step [640/1248], Loss: 0.0045\n",
      "Epoch [50/100], Step [660/1248], Loss: 0.0202\n",
      "Epoch [50/100], Step [680/1248], Loss: 0.0357\n",
      "Epoch [50/100], Step [700/1248], Loss: 0.0042\n",
      "Epoch [50/100], Step [720/1248], Loss: 0.0952\n",
      "Epoch [50/100], Step [740/1248], Loss: 0.0436\n",
      "Epoch [50/100], Step [760/1248], Loss: 0.0101\n",
      "Epoch [50/100], Step [780/1248], Loss: 0.0025\n",
      "Epoch [50/100], Step [800/1248], Loss: 0.0393\n",
      "Epoch [50/100], Step [820/1248], Loss: 0.0032\n",
      "Epoch [50/100], Step [840/1248], Loss: 0.0018\n",
      "Epoch [50/100], Step [860/1248], Loss: 0.4242\n",
      "Epoch [50/100], Step [880/1248], Loss: 0.0128\n",
      "Epoch [50/100], Step [900/1248], Loss: 0.0095\n",
      "Epoch [50/100], Step [920/1248], Loss: 0.0087\n",
      "Epoch [50/100], Step [940/1248], Loss: 0.1338\n",
      "Epoch [50/100], Step [960/1248], Loss: 0.0374\n",
      "Epoch [50/100], Step [980/1248], Loss: 0.0311\n",
      "Epoch [50/100], Step [1000/1248], Loss: 0.1466\n",
      "Epoch [50/100], Step [1020/1248], Loss: 0.0051\n",
      "Epoch [50/100], Step [1040/1248], Loss: 0.0027\n",
      "Epoch [50/100], Step [1060/1248], Loss: 0.0066\n",
      "Epoch [50/100], Step [1080/1248], Loss: 0.0148\n",
      "Epoch [50/100], Step [1100/1248], Loss: 0.0468\n",
      "Epoch [50/100], Step [1120/1248], Loss: 0.0026\n",
      "Epoch [50/100], Step [1140/1248], Loss: 0.2691\n",
      "Epoch [50/100], Step [1160/1248], Loss: 0.0026\n",
      "Epoch [50/100], Step [1180/1248], Loss: 0.0009\n",
      "Epoch [50/100], Step [1200/1248], Loss: 0.0005\n",
      "Epoch [50/100], Step [1220/1248], Loss: 0.0117\n",
      "Epoch [50/100], Step [1240/1248], Loss: 0.0004\n",
      "\n",
      "train-loss: 0.1675, train-acc: 98.7725\n",
      "validation loss: 1.5989, validation acc: 69.1578\n",
      "\n",
      "Epoch 51\n",
      "\n",
      "Epoch [51/100], Step [0/1248], Loss: 0.0083\n",
      "Epoch [51/100], Step [20/1248], Loss: 0.0029\n",
      "Epoch [51/100], Step [40/1248], Loss: 0.0083\n",
      "Epoch [51/100], Step [60/1248], Loss: 0.0002\n",
      "Epoch [51/100], Step [80/1248], Loss: 0.0009\n",
      "Epoch [51/100], Step [100/1248], Loss: 0.0031\n",
      "Epoch [51/100], Step [120/1248], Loss: 0.0011\n",
      "Epoch [51/100], Step [140/1248], Loss: 0.0506\n",
      "Epoch [51/100], Step [160/1248], Loss: 0.0645\n",
      "Epoch [51/100], Step [180/1248], Loss: 0.0101\n",
      "Epoch [51/100], Step [200/1248], Loss: 0.0042\n",
      "Epoch [51/100], Step [220/1248], Loss: 0.0002\n",
      "Epoch [51/100], Step [240/1248], Loss: 0.0066\n",
      "Epoch [51/100], Step [260/1248], Loss: 0.0024\n",
      "Epoch [51/100], Step [280/1248], Loss: 0.0076\n",
      "Epoch [51/100], Step [300/1248], Loss: 0.0014\n",
      "Epoch [51/100], Step [320/1248], Loss: 0.0257\n",
      "Epoch [51/100], Step [340/1248], Loss: 0.0020\n",
      "Epoch [51/100], Step [360/1248], Loss: 0.0164\n",
      "Epoch [51/100], Step [380/1248], Loss: 0.0032\n",
      "Epoch [51/100], Step [400/1248], Loss: 0.0003\n",
      "Epoch [51/100], Step [420/1248], Loss: 0.0001\n",
      "Epoch [51/100], Step [440/1248], Loss: 0.0057\n",
      "Epoch [51/100], Step [460/1248], Loss: 0.0015\n",
      "Epoch [51/100], Step [480/1248], Loss: 0.0047\n",
      "Epoch [51/100], Step [500/1248], Loss: 0.0022\n",
      "Epoch [51/100], Step [520/1248], Loss: 0.0049\n",
      "Epoch [51/100], Step [540/1248], Loss: 0.0012\n",
      "Epoch [51/100], Step [560/1248], Loss: 0.0392\n",
      "Epoch [51/100], Step [580/1248], Loss: 0.0074\n",
      "Epoch [51/100], Step [600/1248], Loss: 0.0047\n",
      "Epoch [51/100], Step [620/1248], Loss: 0.0014\n",
      "Epoch [51/100], Step [640/1248], Loss: 0.0014\n",
      "Epoch [51/100], Step [660/1248], Loss: 0.0103\n",
      "Epoch [51/100], Step [680/1248], Loss: 0.0005\n",
      "Epoch [51/100], Step [700/1248], Loss: 0.0064\n",
      "Epoch [51/100], Step [720/1248], Loss: 0.0412\n",
      "Epoch [51/100], Step [740/1248], Loss: 0.0030\n",
      "Epoch [51/100], Step [760/1248], Loss: 0.0017\n",
      "Epoch [51/100], Step [780/1248], Loss: 0.0014\n",
      "Epoch [51/100], Step [800/1248], Loss: 0.0127\n",
      "Epoch [51/100], Step [820/1248], Loss: 0.0077\n",
      "Epoch [51/100], Step [840/1248], Loss: 0.0008\n",
      "Epoch [51/100], Step [860/1248], Loss: 0.0659\n",
      "Epoch [51/100], Step [880/1248], Loss: 0.0171\n",
      "Epoch [51/100], Step [900/1248], Loss: 0.0061\n",
      "Epoch [51/100], Step [920/1248], Loss: 0.0086\n",
      "Epoch [51/100], Step [940/1248], Loss: 0.0046\n",
      "Epoch [51/100], Step [960/1248], Loss: 0.0004\n",
      "Epoch [51/100], Step [980/1248], Loss: 0.0012\n",
      "Epoch [51/100], Step [1000/1248], Loss: 0.0160\n",
      "Epoch [51/100], Step [1020/1248], Loss: 0.0020\n",
      "Epoch [51/100], Step [1040/1248], Loss: 0.0312\n",
      "Epoch [51/100], Step [1060/1248], Loss: 0.0008\n",
      "Epoch [51/100], Step [1080/1248], Loss: 0.0125\n",
      "Epoch [51/100], Step [1100/1248], Loss: 0.0376\n",
      "Epoch [51/100], Step [1120/1248], Loss: 0.0005\n",
      "Epoch [51/100], Step [1140/1248], Loss: 0.0011\n",
      "Epoch [51/100], Step [1160/1248], Loss: 0.0047\n",
      "Epoch [51/100], Step [1180/1248], Loss: 0.0501\n",
      "Epoch [51/100], Step [1200/1248], Loss: 0.0039\n",
      "Epoch [51/100], Step [1220/1248], Loss: 0.0097\n",
      "Epoch [51/100], Step [1240/1248], Loss: 0.0028\n",
      "\n",
      "train-loss: 0.1647, train-acc: 99.1383\n",
      "validation loss: 1.6071, validation acc: 67.7921\n",
      "\n",
      "Epoch 52\n",
      "\n",
      "Epoch [52/100], Step [0/1248], Loss: 0.0388\n",
      "Epoch [52/100], Step [20/1248], Loss: 0.0042\n",
      "Epoch [52/100], Step [40/1248], Loss: 0.0937\n",
      "Epoch [52/100], Step [60/1248], Loss: 0.0037\n",
      "Epoch [52/100], Step [80/1248], Loss: 0.0037\n",
      "Epoch [52/100], Step [100/1248], Loss: 0.0099\n",
      "Epoch [52/100], Step [120/1248], Loss: 0.0047\n",
      "Epoch [52/100], Step [140/1248], Loss: 0.0132\n",
      "Epoch [52/100], Step [160/1248], Loss: 0.0030\n",
      "Epoch [52/100], Step [180/1248], Loss: 0.0051\n",
      "Epoch [52/100], Step [200/1248], Loss: 0.0203\n",
      "Epoch [52/100], Step [220/1248], Loss: 0.0042\n",
      "Epoch [52/100], Step [240/1248], Loss: 0.0043\n",
      "Epoch [52/100], Step [260/1248], Loss: 0.0013\n",
      "Epoch [52/100], Step [280/1248], Loss: 0.0466\n",
      "Epoch [52/100], Step [300/1248], Loss: 0.0210\n",
      "Epoch [52/100], Step [320/1248], Loss: 0.0085\n",
      "Epoch [52/100], Step [340/1248], Loss: 0.0311\n",
      "Epoch [52/100], Step [360/1248], Loss: 0.0836\n",
      "Epoch [52/100], Step [380/1248], Loss: 0.0014\n",
      "Epoch [52/100], Step [400/1248], Loss: 0.0026\n",
      "Epoch [52/100], Step [420/1248], Loss: 0.0042\n",
      "Epoch [52/100], Step [440/1248], Loss: 0.0893\n",
      "Epoch [52/100], Step [460/1248], Loss: 0.2939\n",
      "Epoch [52/100], Step [480/1248], Loss: 0.0167\n",
      "Epoch [52/100], Step [500/1248], Loss: 0.0057\n",
      "Epoch [52/100], Step [520/1248], Loss: 0.0019\n",
      "Epoch [52/100], Step [540/1248], Loss: 0.0098\n",
      "Epoch [52/100], Step [560/1248], Loss: 0.0010\n",
      "Epoch [52/100], Step [580/1248], Loss: 0.0043\n",
      "Epoch [52/100], Step [600/1248], Loss: 0.0113\n",
      "Epoch [52/100], Step [620/1248], Loss: 0.0017\n",
      "Epoch [52/100], Step [640/1248], Loss: 0.0011\n",
      "Epoch [52/100], Step [660/1248], Loss: 0.0213\n",
      "Epoch [52/100], Step [680/1248], Loss: 0.0094\n",
      "Epoch [52/100], Step [700/1248], Loss: 0.0727\n",
      "Epoch [52/100], Step [720/1248], Loss: 0.0354\n",
      "Epoch [52/100], Step [740/1248], Loss: 0.0041\n",
      "Epoch [52/100], Step [760/1248], Loss: 0.0423\n",
      "Epoch [52/100], Step [780/1248], Loss: 0.1204\n",
      "Epoch [52/100], Step [800/1248], Loss: 0.0221\n",
      "Epoch [52/100], Step [820/1248], Loss: 0.0270\n",
      "Epoch [52/100], Step [840/1248], Loss: 0.0034\n",
      "Epoch [52/100], Step [860/1248], Loss: 0.0054\n",
      "Epoch [52/100], Step [880/1248], Loss: 0.0084\n",
      "Epoch [52/100], Step [900/1248], Loss: 0.0469\n",
      "Epoch [52/100], Step [920/1248], Loss: 0.0018\n",
      "Epoch [52/100], Step [940/1248], Loss: 0.0025\n",
      "Epoch [52/100], Step [960/1248], Loss: 0.0014\n",
      "Epoch [52/100], Step [980/1248], Loss: 0.0067\n",
      "Epoch [52/100], Step [1000/1248], Loss: 0.0011\n",
      "Epoch [52/100], Step [1020/1248], Loss: 0.0007\n",
      "Epoch [52/100], Step [1040/1248], Loss: 0.0056\n",
      "Epoch [52/100], Step [1060/1248], Loss: 0.0111\n",
      "Epoch [52/100], Step [1080/1248], Loss: 0.0244\n",
      "Epoch [52/100], Step [1100/1248], Loss: 0.0452\n",
      "Epoch [52/100], Step [1120/1248], Loss: 0.0018\n",
      "Epoch [52/100], Step [1140/1248], Loss: 0.0191\n",
      "Epoch [52/100], Step [1160/1248], Loss: 0.0071\n",
      "Epoch [52/100], Step [1180/1248], Loss: 0.0131\n",
      "Epoch [52/100], Step [1200/1248], Loss: 0.0025\n",
      "Epoch [52/100], Step [1220/1248], Loss: 0.0009\n",
      "Epoch [52/100], Step [1240/1248], Loss: 0.0485\n",
      "\n",
      "train-loss: 0.1624, train-acc: 98.5220\n",
      "validation loss: 1.6122, validation acc: 68.3359\n",
      "\n",
      "Epoch 53\n",
      "\n",
      "Epoch [53/100], Step [0/1248], Loss: 0.0034\n",
      "Epoch [53/100], Step [20/1248], Loss: 0.0019\n",
      "Epoch [53/100], Step [40/1248], Loss: 0.0197\n",
      "Epoch [53/100], Step [60/1248], Loss: 0.0085\n",
      "Epoch [53/100], Step [80/1248], Loss: 0.0076\n",
      "Epoch [53/100], Step [100/1248], Loss: 0.0136\n",
      "Epoch [53/100], Step [120/1248], Loss: 0.0026\n",
      "Epoch [53/100], Step [140/1248], Loss: 0.0013\n",
      "Epoch [53/100], Step [160/1248], Loss: 0.0008\n",
      "Epoch [53/100], Step [180/1248], Loss: 0.0001\n",
      "Epoch [53/100], Step [200/1248], Loss: 0.0498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Step [220/1248], Loss: 0.0008\n",
      "Epoch [53/100], Step [240/1248], Loss: 0.0030\n",
      "Epoch [53/100], Step [260/1248], Loss: 0.1075\n",
      "Epoch [53/100], Step [280/1248], Loss: 0.0838\n",
      "Epoch [53/100], Step [300/1248], Loss: 0.0012\n",
      "Epoch [53/100], Step [320/1248], Loss: 0.0016\n",
      "Epoch [53/100], Step [340/1248], Loss: 0.0040\n",
      "Epoch [53/100], Step [360/1248], Loss: 0.0015\n",
      "Epoch [53/100], Step [380/1248], Loss: 0.0016\n",
      "Epoch [53/100], Step [400/1248], Loss: 0.0389\n",
      "Epoch [53/100], Step [420/1248], Loss: 0.0005\n",
      "Epoch [53/100], Step [440/1248], Loss: 0.2552\n",
      "Epoch [53/100], Step [460/1248], Loss: 0.0010\n",
      "Epoch [53/100], Step [480/1248], Loss: 0.0030\n",
      "Epoch [53/100], Step [500/1248], Loss: 0.0018\n",
      "Epoch [53/100], Step [520/1248], Loss: 0.1345\n",
      "Epoch [53/100], Step [540/1248], Loss: 0.0029\n",
      "Epoch [53/100], Step [560/1248], Loss: 0.0015\n",
      "Epoch [53/100], Step [580/1248], Loss: 0.0136\n",
      "Epoch [53/100], Step [600/1248], Loss: 0.0019\n",
      "Epoch [53/100], Step [620/1248], Loss: 0.0148\n",
      "Epoch [53/100], Step [640/1248], Loss: 0.0025\n",
      "Epoch [53/100], Step [660/1248], Loss: 0.0057\n",
      "Epoch [53/100], Step [680/1248], Loss: 0.0067\n",
      "Epoch [53/100], Step [700/1248], Loss: 0.0002\n",
      "Epoch [53/100], Step [720/1248], Loss: 0.0009\n",
      "Epoch [53/100], Step [740/1248], Loss: 0.0022\n",
      "Epoch [53/100], Step [760/1248], Loss: 0.0092\n",
      "Epoch [53/100], Step [780/1248], Loss: 0.0074\n",
      "Epoch [53/100], Step [800/1248], Loss: 0.0008\n",
      "Epoch [53/100], Step [820/1248], Loss: 0.0030\n",
      "Epoch [53/100], Step [840/1248], Loss: 0.0024\n",
      "Epoch [53/100], Step [860/1248], Loss: 0.1263\n",
      "Epoch [53/100], Step [880/1248], Loss: 0.0008\n",
      "Epoch [53/100], Step [900/1248], Loss: 0.0108\n",
      "Epoch [53/100], Step [920/1248], Loss: 0.0041\n",
      "Epoch [53/100], Step [940/1248], Loss: 0.0085\n",
      "Epoch [53/100], Step [960/1248], Loss: 0.0001\n",
      "Epoch [53/100], Step [980/1248], Loss: 0.0084\n",
      "Epoch [53/100], Step [1000/1248], Loss: 0.0019\n",
      "Epoch [53/100], Step [1020/1248], Loss: 0.0134\n",
      "Epoch [53/100], Step [1040/1248], Loss: 0.0001\n",
      "Epoch [53/100], Step [1060/1248], Loss: 0.0067\n",
      "Epoch [53/100], Step [1080/1248], Loss: 0.0216\n",
      "Epoch [53/100], Step [1100/1248], Loss: 0.0077\n",
      "Epoch [53/100], Step [1120/1248], Loss: 0.0941\n",
      "Epoch [53/100], Step [1140/1248], Loss: 0.0037\n",
      "Epoch [53/100], Step [1160/1248], Loss: 0.0026\n",
      "Epoch [53/100], Step [1180/1248], Loss: 0.0506\n",
      "Epoch [53/100], Step [1200/1248], Loss: 0.0399\n",
      "Epoch [53/100], Step [1220/1248], Loss: 0.0121\n",
      "Epoch [53/100], Step [1240/1248], Loss: 0.0336\n",
      "\n",
      "train-loss: 0.1598, train-acc: 99.1633\n",
      "validation loss: 1.6208, validation acc: 68.3485\n",
      "\n",
      "Epoch 54\n",
      "\n",
      "Epoch [54/100], Step [0/1248], Loss: 0.0022\n",
      "Epoch [54/100], Step [20/1248], Loss: 0.0095\n",
      "Epoch [54/100], Step [40/1248], Loss: 0.0009\n",
      "Epoch [54/100], Step [60/1248], Loss: 0.0265\n",
      "Epoch [54/100], Step [80/1248], Loss: 0.0312\n",
      "Epoch [54/100], Step [100/1248], Loss: 0.0020\n",
      "Epoch [54/100], Step [120/1248], Loss: 0.0679\n",
      "Epoch [54/100], Step [140/1248], Loss: 0.0002\n",
      "Epoch [54/100], Step [160/1248], Loss: 0.0042\n",
      "Epoch [54/100], Step [180/1248], Loss: 0.0101\n",
      "Epoch [54/100], Step [200/1248], Loss: 0.0671\n",
      "Epoch [54/100], Step [220/1248], Loss: 0.0015\n",
      "Epoch [54/100], Step [240/1248], Loss: 0.0106\n",
      "Epoch [54/100], Step [260/1248], Loss: 0.0009\n",
      "Epoch [54/100], Step [280/1248], Loss: 0.0045\n",
      "Epoch [54/100], Step [300/1248], Loss: 0.0109\n",
      "Epoch [54/100], Step [320/1248], Loss: 0.0247\n",
      "Epoch [54/100], Step [340/1248], Loss: 0.0147\n",
      "Epoch [54/100], Step [360/1248], Loss: 0.0299\n",
      "Epoch [54/100], Step [380/1248], Loss: 0.0756\n",
      "Epoch [54/100], Step [400/1248], Loss: 0.0233\n",
      "Epoch [54/100], Step [420/1248], Loss: 0.0193\n",
      "Epoch [54/100], Step [440/1248], Loss: 0.0085\n",
      "Epoch [54/100], Step [460/1248], Loss: 0.0009\n",
      "Epoch [54/100], Step [480/1248], Loss: 0.0018\n",
      "Epoch [54/100], Step [500/1248], Loss: 0.0003\n",
      "Epoch [54/100], Step [520/1248], Loss: 0.0003\n",
      "Epoch [54/100], Step [540/1248], Loss: 0.0003\n",
      "Epoch [54/100], Step [560/1248], Loss: 0.0720\n",
      "Epoch [54/100], Step [580/1248], Loss: 0.0070\n",
      "Epoch [54/100], Step [600/1248], Loss: 0.0003\n",
      "Epoch [54/100], Step [620/1248], Loss: 0.0031\n",
      "Epoch [54/100], Step [640/1248], Loss: 0.0014\n",
      "Epoch [54/100], Step [660/1248], Loss: 0.0101\n",
      "Epoch [54/100], Step [680/1248], Loss: 0.0202\n",
      "Epoch [54/100], Step [700/1248], Loss: 0.0002\n",
      "Epoch [54/100], Step [720/1248], Loss: 0.0025\n",
      "Epoch [54/100], Step [740/1248], Loss: 0.0011\n",
      "Epoch [54/100], Step [760/1248], Loss: 0.0133\n",
      "Epoch [54/100], Step [780/1248], Loss: 0.0009\n",
      "Epoch [54/100], Step [800/1248], Loss: 0.0139\n",
      "Epoch [54/100], Step [820/1248], Loss: 0.1440\n",
      "Epoch [54/100], Step [840/1248], Loss: 0.0158\n",
      "Epoch [54/100], Step [860/1248], Loss: 0.0064\n",
      "Epoch [54/100], Step [880/1248], Loss: 0.0016\n",
      "Epoch [54/100], Step [900/1248], Loss: 0.0038\n",
      "Epoch [54/100], Step [920/1248], Loss: 0.0103\n",
      "Epoch [54/100], Step [940/1248], Loss: 0.2570\n",
      "Epoch [54/100], Step [960/1248], Loss: 0.1668\n",
      "Epoch [54/100], Step [980/1248], Loss: 0.0061\n",
      "Epoch [54/100], Step [1000/1248], Loss: 0.0127\n",
      "Epoch [54/100], Step [1020/1248], Loss: 0.0017\n",
      "Epoch [54/100], Step [1040/1248], Loss: 0.0093\n",
      "Epoch [54/100], Step [1060/1248], Loss: 0.0841\n",
      "Epoch [54/100], Step [1080/1248], Loss: 0.0016\n",
      "Epoch [54/100], Step [1100/1248], Loss: 0.0083\n",
      "Epoch [54/100], Step [1120/1248], Loss: 0.0038\n",
      "Epoch [54/100], Step [1140/1248], Loss: 0.0018\n",
      "Epoch [54/100], Step [1160/1248], Loss: 0.0008\n",
      "Epoch [54/100], Step [1180/1248], Loss: 0.0152\n",
      "Epoch [54/100], Step [1200/1248], Loss: 0.0067\n",
      "Epoch [54/100], Step [1220/1248], Loss: 0.0081\n",
      "Epoch [54/100], Step [1240/1248], Loss: 0.0149\n",
      "\n",
      "train-loss: 0.1577, train-acc: 98.5120\n",
      "validation loss: 1.6251, validation acc: 68.5003\n",
      "\n",
      "Epoch 55\n",
      "\n",
      "Epoch [55/100], Step [0/1248], Loss: 0.0083\n",
      "Epoch [55/100], Step [20/1248], Loss: 0.1244\n",
      "Epoch [55/100], Step [40/1248], Loss: 0.0043\n",
      "Epoch [55/100], Step [60/1248], Loss: 0.0083\n",
      "Epoch [55/100], Step [80/1248], Loss: 0.0515\n",
      "Epoch [55/100], Step [100/1248], Loss: 0.0018\n",
      "Epoch [55/100], Step [120/1248], Loss: 0.0750\n",
      "Epoch [55/100], Step [140/1248], Loss: 0.0616\n",
      "Epoch [55/100], Step [160/1248], Loss: 0.0109\n",
      "Epoch [55/100], Step [180/1248], Loss: 0.0017\n",
      "Epoch [55/100], Step [200/1248], Loss: 0.0014\n",
      "Epoch [55/100], Step [220/1248], Loss: 0.0130\n",
      "Epoch [55/100], Step [240/1248], Loss: 0.0008\n",
      "Epoch [55/100], Step [260/1248], Loss: 0.0093\n",
      "Epoch [55/100], Step [280/1248], Loss: 0.0030\n",
      "Epoch [55/100], Step [300/1248], Loss: 0.0050\n",
      "Epoch [55/100], Step [320/1248], Loss: 0.0078\n",
      "Epoch [55/100], Step [340/1248], Loss: 0.0119\n",
      "Epoch [55/100], Step [360/1248], Loss: 0.0004\n",
      "Epoch [55/100], Step [380/1248], Loss: 0.0616\n",
      "Epoch [55/100], Step [400/1248], Loss: 0.0158\n",
      "Epoch [55/100], Step [420/1248], Loss: 0.0002\n",
      "Epoch [55/100], Step [440/1248], Loss: 0.0247\n",
      "Epoch [55/100], Step [460/1248], Loss: 0.0074\n",
      "Epoch [55/100], Step [480/1248], Loss: 0.0065\n",
      "Epoch [55/100], Step [500/1248], Loss: 0.0004\n",
      "Epoch [55/100], Step [520/1248], Loss: 0.0597\n",
      "Epoch [55/100], Step [540/1248], Loss: 0.0116\n",
      "Epoch [55/100], Step [560/1248], Loss: 0.0014\n",
      "Epoch [55/100], Step [580/1248], Loss: 0.0064\n",
      "Epoch [55/100], Step [600/1248], Loss: 0.0276\n",
      "Epoch [55/100], Step [620/1248], Loss: 0.1627\n",
      "Epoch [55/100], Step [640/1248], Loss: 0.0086\n",
      "Epoch [55/100], Step [660/1248], Loss: 0.1833\n",
      "Epoch [55/100], Step [680/1248], Loss: 0.0402\n",
      "Epoch [55/100], Step [700/1248], Loss: 0.0241\n",
      "Epoch [55/100], Step [720/1248], Loss: 0.0540\n",
      "Epoch [55/100], Step [740/1248], Loss: 0.0014\n",
      "Epoch [55/100], Step [760/1248], Loss: 0.0025\n",
      "Epoch [55/100], Step [780/1248], Loss: 0.0834\n",
      "Epoch [55/100], Step [800/1248], Loss: 0.0283\n",
      "Epoch [55/100], Step [820/1248], Loss: 0.0050\n",
      "Epoch [55/100], Step [840/1248], Loss: 0.0017\n",
      "Epoch [55/100], Step [860/1248], Loss: 0.0006\n",
      "Epoch [55/100], Step [880/1248], Loss: 0.0002\n",
      "Epoch [55/100], Step [900/1248], Loss: 0.0146\n",
      "Epoch [55/100], Step [920/1248], Loss: 0.0090\n",
      "Epoch [55/100], Step [940/1248], Loss: 0.0151\n",
      "Epoch [55/100], Step [960/1248], Loss: 0.0003\n",
      "Epoch [55/100], Step [980/1248], Loss: 0.0040\n",
      "Epoch [55/100], Step [1000/1248], Loss: 0.0475\n",
      "Epoch [55/100], Step [1020/1248], Loss: 0.0066\n",
      "Epoch [55/100], Step [1040/1248], Loss: 0.0016\n",
      "Epoch [55/100], Step [1060/1248], Loss: 0.0094\n",
      "Epoch [55/100], Step [1080/1248], Loss: 0.0015\n",
      "Epoch [55/100], Step [1100/1248], Loss: 0.0009\n",
      "Epoch [55/100], Step [1120/1248], Loss: 0.0280\n",
      "Epoch [55/100], Step [1140/1248], Loss: 0.0413\n",
      "Epoch [55/100], Step [1160/1248], Loss: 0.0951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100], Step [1180/1248], Loss: 0.0037\n",
      "Epoch [55/100], Step [1200/1248], Loss: 0.0027\n",
      "Epoch [55/100], Step [1220/1248], Loss: 0.0019\n",
      "Epoch [55/100], Step [1240/1248], Loss: 0.0206\n",
      "\n",
      "train-loss: 0.1553, train-acc: 99.0882\n",
      "validation loss: 1.6283, validation acc: 69.2843\n",
      "\n",
      "Epoch 56\n",
      "\n",
      "Epoch [56/100], Step [0/1248], Loss: 0.0077\n",
      "Epoch [56/100], Step [20/1248], Loss: 0.0021\n",
      "Epoch [56/100], Step [40/1248], Loss: 0.0006\n",
      "Epoch [56/100], Step [60/1248], Loss: 0.0016\n",
      "Epoch [56/100], Step [80/1248], Loss: 0.0099\n",
      "Epoch [56/100], Step [100/1248], Loss: 0.0014\n",
      "Epoch [56/100], Step [120/1248], Loss: 0.0014\n",
      "Epoch [56/100], Step [140/1248], Loss: 0.0099\n",
      "Epoch [56/100], Step [160/1248], Loss: 0.0012\n",
      "Epoch [56/100], Step [180/1248], Loss: 0.0020\n",
      "Epoch [56/100], Step [200/1248], Loss: 0.0023\n",
      "Epoch [56/100], Step [220/1248], Loss: 0.0409\n",
      "Epoch [56/100], Step [240/1248], Loss: 0.0025\n",
      "Epoch [56/100], Step [260/1248], Loss: 0.0005\n",
      "Epoch [56/100], Step [280/1248], Loss: 0.0020\n",
      "Epoch [56/100], Step [300/1248], Loss: 0.0052\n",
      "Epoch [56/100], Step [320/1248], Loss: 0.0003\n",
      "Epoch [56/100], Step [340/1248], Loss: 0.0987\n",
      "Epoch [56/100], Step [360/1248], Loss: 0.0023\n",
      "Epoch [56/100], Step [380/1248], Loss: 0.0037\n",
      "Epoch [56/100], Step [400/1248], Loss: 0.0009\n",
      "Epoch [56/100], Step [420/1248], Loss: 0.0004\n",
      "Epoch [56/100], Step [440/1248], Loss: 0.0003\n",
      "Epoch [56/100], Step [460/1248], Loss: 0.0011\n",
      "Epoch [56/100], Step [480/1248], Loss: 0.0013\n",
      "Epoch [56/100], Step [500/1248], Loss: 0.0234\n",
      "Epoch [56/100], Step [520/1248], Loss: 0.0123\n",
      "Epoch [56/100], Step [540/1248], Loss: 0.0027\n",
      "Epoch [56/100], Step [560/1248], Loss: 0.3776\n",
      "Epoch [56/100], Step [580/1248], Loss: 0.0251\n",
      "Epoch [56/100], Step [600/1248], Loss: 0.0398\n",
      "Epoch [56/100], Step [620/1248], Loss: 0.0019\n",
      "Epoch [56/100], Step [640/1248], Loss: 0.0882\n",
      "Epoch [56/100], Step [660/1248], Loss: 0.0014\n",
      "Epoch [56/100], Step [680/1248], Loss: 0.0167\n",
      "Epoch [56/100], Step [700/1248], Loss: 0.0067\n",
      "Epoch [56/100], Step [720/1248], Loss: 0.0002\n",
      "Epoch [56/100], Step [740/1248], Loss: 0.0251\n",
      "Epoch [56/100], Step [760/1248], Loss: 0.0376\n",
      "Epoch [56/100], Step [780/1248], Loss: 0.0176\n",
      "Epoch [56/100], Step [800/1248], Loss: 0.1390\n",
      "Epoch [56/100], Step [820/1248], Loss: 0.0076\n",
      "Epoch [56/100], Step [840/1248], Loss: 0.0012\n",
      "Epoch [56/100], Step [860/1248], Loss: 0.0018\n",
      "Epoch [56/100], Step [880/1248], Loss: 0.0180\n",
      "Epoch [56/100], Step [900/1248], Loss: 0.1714\n",
      "Epoch [56/100], Step [920/1248], Loss: 0.0063\n",
      "Epoch [56/100], Step [940/1248], Loss: 0.0040\n",
      "Epoch [56/100], Step [960/1248], Loss: 0.0015\n",
      "Epoch [56/100], Step [980/1248], Loss: 0.0184\n",
      "Epoch [56/100], Step [1000/1248], Loss: 0.0004\n",
      "Epoch [56/100], Step [1020/1248], Loss: 0.0782\n",
      "Epoch [56/100], Step [1040/1248], Loss: 0.0870\n",
      "Epoch [56/100], Step [1060/1248], Loss: 0.0512\n",
      "Epoch [56/100], Step [1080/1248], Loss: 0.0077\n",
      "Epoch [56/100], Step [1100/1248], Loss: 0.0241\n",
      "Epoch [56/100], Step [1120/1248], Loss: 0.0009\n",
      "Epoch [56/100], Step [1140/1248], Loss: 0.0065\n",
      "Epoch [56/100], Step [1160/1248], Loss: 0.1094\n",
      "Epoch [56/100], Step [1180/1248], Loss: 0.0032\n",
      "Epoch [56/100], Step [1200/1248], Loss: 0.0501\n",
      "Epoch [56/100], Step [1220/1248], Loss: 0.0077\n",
      "Epoch [56/100], Step [1240/1248], Loss: 0.0403\n",
      "\n",
      "train-loss: 0.1531, train-acc: 98.8076\n",
      "validation loss: 1.6351, validation acc: 68.4244\n",
      "\n",
      "Epoch 57\n",
      "\n",
      "Epoch [57/100], Step [0/1248], Loss: 0.0931\n",
      "Epoch [57/100], Step [20/1248], Loss: 0.0148\n",
      "Epoch [57/100], Step [40/1248], Loss: 0.0005\n",
      "Epoch [57/100], Step [60/1248], Loss: 0.0007\n",
      "Epoch [57/100], Step [80/1248], Loss: 0.0074\n",
      "Epoch [57/100], Step [100/1248], Loss: 0.0014\n",
      "Epoch [57/100], Step [120/1248], Loss: 0.0133\n",
      "Epoch [57/100], Step [140/1248], Loss: 0.0122\n",
      "Epoch [57/100], Step [160/1248], Loss: 0.0024\n",
      "Epoch [57/100], Step [180/1248], Loss: 0.0243\n",
      "Epoch [57/100], Step [200/1248], Loss: 0.0016\n",
      "Epoch [57/100], Step [220/1248], Loss: 0.0052\n",
      "Epoch [57/100], Step [240/1248], Loss: 0.0040\n",
      "Epoch [57/100], Step [260/1248], Loss: 0.0004\n",
      "Epoch [57/100], Step [280/1248], Loss: 0.0013\n",
      "Epoch [57/100], Step [300/1248], Loss: 0.0008\n",
      "Epoch [57/100], Step [320/1248], Loss: 0.0212\n",
      "Epoch [57/100], Step [340/1248], Loss: 0.0132\n",
      "Epoch [57/100], Step [360/1248], Loss: 0.0214\n",
      "Epoch [57/100], Step [380/1248], Loss: 0.0418\n",
      "Epoch [57/100], Step [400/1248], Loss: 0.0045\n",
      "Epoch [57/100], Step [420/1248], Loss: 0.0082\n",
      "Epoch [57/100], Step [440/1248], Loss: 0.0091\n",
      "Epoch [57/100], Step [460/1248], Loss: 0.0002\n",
      "Epoch [57/100], Step [480/1248], Loss: 0.0312\n",
      "Epoch [57/100], Step [500/1248], Loss: 0.1208\n",
      "Epoch [57/100], Step [520/1248], Loss: 0.0395\n",
      "Epoch [57/100], Step [540/1248], Loss: 0.1185\n",
      "Epoch [57/100], Step [560/1248], Loss: 0.0037\n",
      "Epoch [57/100], Step [580/1248], Loss: 0.0161\n",
      "Epoch [57/100], Step [600/1248], Loss: 0.0118\n",
      "Epoch [57/100], Step [620/1248], Loss: 0.0109\n",
      "Epoch [57/100], Step [640/1248], Loss: 0.0019\n",
      "Epoch [57/100], Step [660/1248], Loss: 0.2278\n",
      "Epoch [57/100], Step [680/1248], Loss: 0.0142\n",
      "Epoch [57/100], Step [700/1248], Loss: 0.0282\n",
      "Epoch [57/100], Step [720/1248], Loss: 0.0082\n",
      "Epoch [57/100], Step [740/1248], Loss: 0.0016\n",
      "Epoch [57/100], Step [760/1248], Loss: 0.0923\n",
      "Epoch [57/100], Step [780/1248], Loss: 0.0160\n",
      "Epoch [57/100], Step [800/1248], Loss: 0.0074\n",
      "Epoch [57/100], Step [820/1248], Loss: 0.0029\n",
      "Epoch [57/100], Step [840/1248], Loss: 0.0056\n",
      "Epoch [57/100], Step [860/1248], Loss: 0.0172\n",
      "Epoch [57/100], Step [880/1248], Loss: 0.0036\n",
      "Epoch [57/100], Step [900/1248], Loss: 0.0168\n",
      "Epoch [57/100], Step [920/1248], Loss: 0.0513\n",
      "Epoch [57/100], Step [940/1248], Loss: 0.0122\n",
      "Epoch [57/100], Step [960/1248], Loss: 0.0761\n",
      "Epoch [57/100], Step [980/1248], Loss: 0.1961\n",
      "Epoch [57/100], Step [1000/1248], Loss: 0.0482\n",
      "Epoch [57/100], Step [1020/1248], Loss: 0.0115\n",
      "Epoch [57/100], Step [1040/1248], Loss: 0.0004\n",
      "Epoch [57/100], Step [1060/1248], Loss: 0.0060\n",
      "Epoch [57/100], Step [1080/1248], Loss: 0.0034\n",
      "Epoch [57/100], Step [1100/1248], Loss: 0.0748\n",
      "Epoch [57/100], Step [1120/1248], Loss: 0.0056\n",
      "Epoch [57/100], Step [1140/1248], Loss: 0.0186\n",
      "Epoch [57/100], Step [1160/1248], Loss: 0.0686\n",
      "Epoch [57/100], Step [1180/1248], Loss: 0.0151\n",
      "Epoch [57/100], Step [1200/1248], Loss: 0.0020\n",
      "Epoch [57/100], Step [1220/1248], Loss: 0.0192\n",
      "Epoch [57/100], Step [1240/1248], Loss: 0.0282\n",
      "\n",
      "train-loss: 0.1510, train-acc: 98.9028\n",
      "validation loss: 1.6388, validation acc: 69.1958\n",
      "\n",
      "Epoch 58\n",
      "\n",
      "Epoch [58/100], Step [0/1248], Loss: 0.0014\n",
      "Epoch [58/100], Step [20/1248], Loss: 0.0052\n",
      "Epoch [58/100], Step [40/1248], Loss: 0.0035\n",
      "Epoch [58/100], Step [60/1248], Loss: 0.0006\n",
      "Epoch [58/100], Step [80/1248], Loss: 0.0030\n",
      "Epoch [58/100], Step [100/1248], Loss: 0.0002\n",
      "Epoch [58/100], Step [120/1248], Loss: 0.0005\n",
      "Epoch [58/100], Step [140/1248], Loss: 0.0003\n",
      "Epoch [58/100], Step [160/1248], Loss: 0.0003\n",
      "Epoch [58/100], Step [180/1248], Loss: 0.0019\n",
      "Epoch [58/100], Step [200/1248], Loss: 0.0019\n",
      "Epoch [58/100], Step [220/1248], Loss: 0.0072\n",
      "Epoch [58/100], Step [240/1248], Loss: 0.0006\n",
      "Epoch [58/100], Step [260/1248], Loss: 0.0270\n",
      "Epoch [58/100], Step [280/1248], Loss: 0.0057\n",
      "Epoch [58/100], Step [300/1248], Loss: 0.1073\n",
      "Epoch [58/100], Step [320/1248], Loss: 0.0141\n",
      "Epoch [58/100], Step [340/1248], Loss: 0.0002\n",
      "Epoch [58/100], Step [360/1248], Loss: 0.0031\n",
      "Epoch [58/100], Step [380/1248], Loss: 0.0020\n",
      "Epoch [58/100], Step [400/1248], Loss: 0.0048\n",
      "Epoch [58/100], Step [420/1248], Loss: 0.0028\n",
      "Epoch [58/100], Step [440/1248], Loss: 0.0002\n",
      "Epoch [58/100], Step [460/1248], Loss: 0.6109\n",
      "Epoch [58/100], Step [480/1248], Loss: 0.0006\n",
      "Epoch [58/100], Step [500/1248], Loss: 0.0048\n",
      "Epoch [58/100], Step [520/1248], Loss: 0.0215\n",
      "Epoch [58/100], Step [540/1248], Loss: 0.0019\n",
      "Epoch [58/100], Step [560/1248], Loss: 0.0470\n",
      "Epoch [58/100], Step [580/1248], Loss: 0.0019\n",
      "Epoch [58/100], Step [600/1248], Loss: 0.0420\n",
      "Epoch [58/100], Step [620/1248], Loss: 0.0009\n",
      "Epoch [58/100], Step [640/1248], Loss: 0.0018\n",
      "Epoch [58/100], Step [660/1248], Loss: 0.0081\n",
      "Epoch [58/100], Step [680/1248], Loss: 0.0367\n",
      "Epoch [58/100], Step [700/1248], Loss: 0.0058\n",
      "Epoch [58/100], Step [720/1248], Loss: 0.0004\n",
      "Epoch [58/100], Step [740/1248], Loss: 0.0003\n",
      "Epoch [58/100], Step [760/1248], Loss: 0.0005\n",
      "Epoch [58/100], Step [780/1248], Loss: 0.0003\n",
      "Epoch [58/100], Step [800/1248], Loss: 0.0293\n",
      "Epoch [58/100], Step [820/1248], Loss: 0.0005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Step [840/1248], Loss: 0.0011\n",
      "Epoch [58/100], Step [860/1248], Loss: 0.1056\n",
      "Epoch [58/100], Step [880/1248], Loss: 0.0006\n",
      "Epoch [58/100], Step [900/1248], Loss: 0.0143\n",
      "Epoch [58/100], Step [920/1248], Loss: 0.0926\n",
      "Epoch [58/100], Step [940/1248], Loss: 0.1465\n",
      "Epoch [58/100], Step [960/1248], Loss: 0.0161\n",
      "Epoch [58/100], Step [980/1248], Loss: 0.0171\n",
      "Epoch [58/100], Step [1000/1248], Loss: 0.0035\n",
      "Epoch [58/100], Step [1020/1248], Loss: 0.0140\n",
      "Epoch [58/100], Step [1040/1248], Loss: 0.1716\n",
      "Epoch [58/100], Step [1060/1248], Loss: 0.0062\n",
      "Epoch [58/100], Step [1080/1248], Loss: 0.3001\n",
      "Epoch [58/100], Step [1100/1248], Loss: 0.1260\n",
      "Epoch [58/100], Step [1120/1248], Loss: 0.0522\n",
      "Epoch [58/100], Step [1140/1248], Loss: 0.0031\n",
      "Epoch [58/100], Step [1160/1248], Loss: 0.0835\n",
      "Epoch [58/100], Step [1180/1248], Loss: 0.0036\n",
      "Epoch [58/100], Step [1200/1248], Loss: 0.0185\n",
      "Epoch [58/100], Step [1220/1248], Loss: 0.0181\n",
      "Epoch [58/100], Step [1240/1248], Loss: 0.0305\n",
      "\n",
      "train-loss: 0.1489, train-acc: 98.9679\n",
      "validation loss: 1.6456, validation acc: 67.0334\n",
      "\n",
      "Epoch 59\n",
      "\n",
      "Epoch [59/100], Step [0/1248], Loss: 0.1967\n",
      "Epoch [59/100], Step [20/1248], Loss: 0.2776\n",
      "Epoch [59/100], Step [40/1248], Loss: 0.0182\n",
      "Epoch [59/100], Step [60/1248], Loss: 0.0160\n",
      "Epoch [59/100], Step [80/1248], Loss: 0.0009\n",
      "Epoch [59/100], Step [100/1248], Loss: 0.0020\n",
      "Epoch [59/100], Step [120/1248], Loss: 0.0292\n",
      "Epoch [59/100], Step [140/1248], Loss: 0.0072\n",
      "Epoch [59/100], Step [160/1248], Loss: 0.3910\n",
      "Epoch [59/100], Step [180/1248], Loss: 0.0165\n",
      "Epoch [59/100], Step [200/1248], Loss: 0.0004\n",
      "Epoch [59/100], Step [220/1248], Loss: 0.0255\n",
      "Epoch [59/100], Step [240/1248], Loss: 0.0003\n",
      "Epoch [59/100], Step [260/1248], Loss: 0.0133\n",
      "Epoch [59/100], Step [280/1248], Loss: 0.0091\n",
      "Epoch [59/100], Step [300/1248], Loss: 0.0036\n",
      "Epoch [59/100], Step [320/1248], Loss: 0.0019\n",
      "Epoch [59/100], Step [340/1248], Loss: 0.0055\n",
      "Epoch [59/100], Step [360/1248], Loss: 0.0352\n",
      "Epoch [59/100], Step [380/1248], Loss: 0.0676\n",
      "Epoch [59/100], Step [400/1248], Loss: 0.0086\n",
      "Epoch [59/100], Step [420/1248], Loss: 0.0205\n",
      "Epoch [59/100], Step [440/1248], Loss: 0.0336\n",
      "Epoch [59/100], Step [460/1248], Loss: 0.0006\n",
      "Epoch [59/100], Step [480/1248], Loss: 0.0147\n",
      "Epoch [59/100], Step [500/1248], Loss: 0.0316\n",
      "Epoch [59/100], Step [520/1248], Loss: 0.0061\n",
      "Epoch [59/100], Step [540/1248], Loss: 0.0052\n",
      "Epoch [59/100], Step [560/1248], Loss: 0.0602\n",
      "Epoch [59/100], Step [580/1248], Loss: 0.0113\n",
      "Epoch [59/100], Step [600/1248], Loss: 0.0320\n",
      "Epoch [59/100], Step [620/1248], Loss: 0.1581\n",
      "Epoch [59/100], Step [640/1248], Loss: 0.0016\n",
      "Epoch [59/100], Step [660/1248], Loss: 0.0036\n",
      "Epoch [59/100], Step [680/1248], Loss: 0.0153\n",
      "Epoch [59/100], Step [700/1248], Loss: 0.0203\n",
      "Epoch [59/100], Step [720/1248], Loss: 0.0036\n",
      "Epoch [59/100], Step [740/1248], Loss: 0.0069\n",
      "Epoch [59/100], Step [760/1248], Loss: 0.0043\n",
      "Epoch [59/100], Step [780/1248], Loss: 0.1331\n",
      "Epoch [59/100], Step [800/1248], Loss: 0.0002\n",
      "Epoch [59/100], Step [820/1248], Loss: 0.0131\n",
      "Epoch [59/100], Step [840/1248], Loss: 0.0033\n",
      "Epoch [59/100], Step [860/1248], Loss: 0.0065\n",
      "Epoch [59/100], Step [880/1248], Loss: 0.0120\n",
      "Epoch [59/100], Step [900/1248], Loss: 0.0022\n",
      "Epoch [59/100], Step [920/1248], Loss: 0.1560\n",
      "Epoch [59/100], Step [940/1248], Loss: 0.0074\n",
      "Epoch [59/100], Step [960/1248], Loss: 0.0055\n",
      "Epoch [59/100], Step [980/1248], Loss: 0.0172\n",
      "Epoch [59/100], Step [1000/1248], Loss: 0.0098\n",
      "Epoch [59/100], Step [1020/1248], Loss: 0.0052\n",
      "Epoch [59/100], Step [1040/1248], Loss: 0.0011\n",
      "Epoch [59/100], Step [1060/1248], Loss: 0.0353\n",
      "Epoch [59/100], Step [1080/1248], Loss: 0.0082\n",
      "Epoch [59/100], Step [1100/1248], Loss: 0.0006\n",
      "Epoch [59/100], Step [1120/1248], Loss: 0.0082\n",
      "Epoch [59/100], Step [1140/1248], Loss: 0.0014\n",
      "Epoch [59/100], Step [1160/1248], Loss: 0.0072\n",
      "Epoch [59/100], Step [1180/1248], Loss: 0.0068\n",
      "Epoch [59/100], Step [1200/1248], Loss: 0.0164\n",
      "Epoch [59/100], Step [1220/1248], Loss: 0.1701\n",
      "Epoch [59/100], Step [1240/1248], Loss: 0.0605\n",
      "\n",
      "train-loss: 0.1469, train-acc: 98.9629\n",
      "validation loss: 1.6511, validation acc: 69.3601\n",
      "\n",
      "Epoch 60\n",
      "\n",
      "Epoch [60/100], Step [0/1248], Loss: 0.0031\n",
      "Epoch [60/100], Step [20/1248], Loss: 0.0050\n",
      "Epoch [60/100], Step [40/1248], Loss: 0.0006\n",
      "Epoch [60/100], Step [60/1248], Loss: 0.0001\n",
      "Epoch [60/100], Step [80/1248], Loss: 0.0015\n",
      "Epoch [60/100], Step [100/1248], Loss: 0.2214\n",
      "Epoch [60/100], Step [120/1248], Loss: 0.0128\n",
      "Epoch [60/100], Step [140/1248], Loss: 0.0044\n",
      "Epoch [60/100], Step [160/1248], Loss: 0.0010\n",
      "Epoch [60/100], Step [180/1248], Loss: 0.0067\n",
      "Epoch [60/100], Step [200/1248], Loss: 0.0006\n",
      "Epoch [60/100], Step [220/1248], Loss: 0.1889\n",
      "Epoch [60/100], Step [240/1248], Loss: 0.1367\n",
      "Epoch [60/100], Step [260/1248], Loss: 0.0006\n",
      "Epoch [60/100], Step [280/1248], Loss: 0.0054\n",
      "Epoch [60/100], Step [300/1248], Loss: 0.0001\n",
      "Epoch [60/100], Step [320/1248], Loss: 0.0008\n",
      "Epoch [60/100], Step [340/1248], Loss: 0.0076\n",
      "Epoch [60/100], Step [360/1248], Loss: 0.1412\n",
      "Epoch [60/100], Step [380/1248], Loss: 0.0562\n",
      "Epoch [60/100], Step [400/1248], Loss: 0.0005\n",
      "Epoch [60/100], Step [420/1248], Loss: 0.0064\n",
      "Epoch [60/100], Step [440/1248], Loss: 0.0015\n",
      "Epoch [60/100], Step [460/1248], Loss: 0.0010\n",
      "Epoch [60/100], Step [480/1248], Loss: 0.0005\n",
      "Epoch [60/100], Step [500/1248], Loss: 0.1141\n",
      "Epoch [60/100], Step [520/1248], Loss: 0.0045\n",
      "Epoch [60/100], Step [540/1248], Loss: 0.0440\n",
      "Epoch [60/100], Step [560/1248], Loss: 0.0043\n",
      "Epoch [60/100], Step [580/1248], Loss: 0.0032\n",
      "Epoch [60/100], Step [600/1248], Loss: 0.0004\n",
      "Epoch [60/100], Step [620/1248], Loss: 0.1993\n",
      "Epoch [60/100], Step [640/1248], Loss: 0.0056\n",
      "Epoch [60/100], Step [660/1248], Loss: 0.0074\n",
      "Epoch [60/100], Step [680/1248], Loss: 0.0312\n",
      "Epoch [60/100], Step [700/1248], Loss: 0.0006\n",
      "Epoch [60/100], Step [720/1248], Loss: 0.0038\n",
      "Epoch [60/100], Step [740/1248], Loss: 0.0201\n",
      "Epoch [60/100], Step [760/1248], Loss: 0.0945\n",
      "Epoch [60/100], Step [780/1248], Loss: 0.0573\n",
      "Epoch [60/100], Step [800/1248], Loss: 0.0105\n",
      "Epoch [60/100], Step [820/1248], Loss: 0.0003\n",
      "Epoch [60/100], Step [840/1248], Loss: 0.0027\n",
      "Epoch [60/100], Step [860/1248], Loss: 0.1942\n",
      "Epoch [60/100], Step [880/1248], Loss: 0.0193\n",
      "Epoch [60/100], Step [900/1248], Loss: 0.0021\n",
      "Epoch [60/100], Step [920/1248], Loss: 0.0011\n",
      "Epoch [60/100], Step [940/1248], Loss: 0.0053\n",
      "Epoch [60/100], Step [960/1248], Loss: 0.0352\n",
      "Epoch [60/100], Step [980/1248], Loss: 0.0011\n",
      "Epoch [60/100], Step [1000/1248], Loss: 0.0775\n",
      "Epoch [60/100], Step [1020/1248], Loss: 0.0054\n",
      "Epoch [60/100], Step [1040/1248], Loss: 0.1728\n",
      "Epoch [60/100], Step [1060/1248], Loss: 0.0147\n",
      "Epoch [60/100], Step [1080/1248], Loss: 0.0015\n",
      "Epoch [60/100], Step [1100/1248], Loss: 0.0007\n",
      "Epoch [60/100], Step [1120/1248], Loss: 0.0357\n",
      "Epoch [60/100], Step [1140/1248], Loss: 0.3837\n",
      "Epoch [60/100], Step [1160/1248], Loss: 0.0075\n",
      "Epoch [60/100], Step [1180/1248], Loss: 0.0003\n",
      "Epoch [60/100], Step [1200/1248], Loss: 0.1645\n",
      "Epoch [60/100], Step [1220/1248], Loss: 0.0008\n",
      "Epoch [60/100], Step [1240/1248], Loss: 0.0007\n",
      "\n",
      "train-loss: 0.1449, train-acc: 99.1082\n",
      "validation loss: 1.6572, validation acc: 68.4876\n",
      "\n",
      "Epoch 61\n",
      "\n",
      "Epoch [61/100], Step [0/1248], Loss: 0.0062\n",
      "Epoch [61/100], Step [20/1248], Loss: 0.0160\n",
      "Epoch [61/100], Step [40/1248], Loss: 0.1625\n",
      "Epoch [61/100], Step [60/1248], Loss: 0.0006\n",
      "Epoch [61/100], Step [80/1248], Loss: 0.3198\n",
      "Epoch [61/100], Step [100/1248], Loss: 0.1694\n",
      "Epoch [61/100], Step [120/1248], Loss: 0.0012\n",
      "Epoch [61/100], Step [140/1248], Loss: 0.1534\n",
      "Epoch [61/100], Step [160/1248], Loss: 0.0094\n",
      "Epoch [61/100], Step [180/1248], Loss: 0.0090\n",
      "Epoch [61/100], Step [200/1248], Loss: 0.0004\n",
      "Epoch [61/100], Step [220/1248], Loss: 0.0240\n",
      "Epoch [61/100], Step [240/1248], Loss: 0.0024\n",
      "Epoch [61/100], Step [260/1248], Loss: 0.0104\n",
      "Epoch [61/100], Step [280/1248], Loss: 0.0153\n",
      "Epoch [61/100], Step [300/1248], Loss: 0.0193\n",
      "Epoch [61/100], Step [320/1248], Loss: 0.1385\n",
      "Epoch [61/100], Step [340/1248], Loss: 0.0001\n",
      "Epoch [61/100], Step [360/1248], Loss: 0.1148\n",
      "Epoch [61/100], Step [380/1248], Loss: 0.0139\n",
      "Epoch [61/100], Step [400/1248], Loss: 0.0055\n",
      "Epoch [61/100], Step [420/1248], Loss: 0.0025\n",
      "Epoch [61/100], Step [440/1248], Loss: 0.0439\n",
      "Epoch [61/100], Step [460/1248], Loss: 0.0054\n",
      "Epoch [61/100], Step [480/1248], Loss: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100], Step [500/1248], Loss: 0.0017\n",
      "Epoch [61/100], Step [520/1248], Loss: 0.0156\n",
      "Epoch [61/100], Step [540/1248], Loss: 0.0015\n",
      "Epoch [61/100], Step [560/1248], Loss: 0.0028\n",
      "Epoch [61/100], Step [580/1248], Loss: 0.2070\n",
      "Epoch [61/100], Step [600/1248], Loss: 0.0631\n",
      "Epoch [61/100], Step [620/1248], Loss: 0.0018\n",
      "Epoch [61/100], Step [640/1248], Loss: 0.0213\n",
      "Epoch [61/100], Step [660/1248], Loss: 0.0009\n",
      "Epoch [61/100], Step [680/1248], Loss: 0.0575\n",
      "Epoch [61/100], Step [700/1248], Loss: 0.0003\n",
      "Epoch [61/100], Step [720/1248], Loss: 0.0195\n",
      "Epoch [61/100], Step [740/1248], Loss: 0.0211\n",
      "Epoch [61/100], Step [760/1248], Loss: 0.5232\n",
      "Epoch [61/100], Step [780/1248], Loss: 0.0074\n",
      "Epoch [61/100], Step [800/1248], Loss: 0.1306\n",
      "Epoch [61/100], Step [820/1248], Loss: 0.0128\n",
      "Epoch [61/100], Step [840/1248], Loss: 0.0033\n",
      "Epoch [61/100], Step [860/1248], Loss: 0.0533\n",
      "Epoch [61/100], Step [880/1248], Loss: 0.0238\n",
      "Epoch [61/100], Step [900/1248], Loss: 0.0035\n",
      "Epoch [61/100], Step [920/1248], Loss: 0.0834\n",
      "Epoch [61/100], Step [940/1248], Loss: 0.0060\n",
      "Epoch [61/100], Step [960/1248], Loss: 0.2203\n",
      "Epoch [61/100], Step [980/1248], Loss: 0.0221\n",
      "Epoch [61/100], Step [1000/1248], Loss: 0.0074\n",
      "Epoch [61/100], Step [1020/1248], Loss: 0.0103\n",
      "Epoch [61/100], Step [1040/1248], Loss: 0.0557\n",
      "Epoch [61/100], Step [1060/1248], Loss: 0.0440\n",
      "Epoch [61/100], Step [1080/1248], Loss: 0.0314\n",
      "Epoch [61/100], Step [1100/1248], Loss: 0.0110\n",
      "Epoch [61/100], Step [1120/1248], Loss: 0.0008\n",
      "Epoch [61/100], Step [1140/1248], Loss: 0.0415\n",
      "Epoch [61/100], Step [1160/1248], Loss: 0.0019\n",
      "Epoch [61/100], Step [1180/1248], Loss: 0.0035\n",
      "Epoch [61/100], Step [1200/1248], Loss: 0.0023\n",
      "Epoch [61/100], Step [1220/1248], Loss: 0.0056\n",
      "Epoch [61/100], Step [1240/1248], Loss: 0.0040\n",
      "\n",
      "train-loss: 0.1432, train-acc: 98.6723\n",
      "validation loss: 1.6637, validation acc: 68.8796\n",
      "\n",
      "Epoch 62\n",
      "\n",
      "Epoch [62/100], Step [0/1248], Loss: 0.0116\n",
      "Epoch [62/100], Step [20/1248], Loss: 0.0030\n",
      "Epoch [62/100], Step [40/1248], Loss: 0.0808\n",
      "Epoch [62/100], Step [60/1248], Loss: 0.0556\n",
      "Epoch [62/100], Step [80/1248], Loss: 0.0074\n",
      "Epoch [62/100], Step [100/1248], Loss: 0.0294\n",
      "Epoch [62/100], Step [120/1248], Loss: 0.0068\n",
      "Epoch [62/100], Step [140/1248], Loss: 0.0375\n",
      "Epoch [62/100], Step [160/1248], Loss: 0.0001\n",
      "Epoch [62/100], Step [180/1248], Loss: 0.0484\n",
      "Epoch [62/100], Step [200/1248], Loss: 0.0007\n",
      "Epoch [62/100], Step [220/1248], Loss: 0.0224\n",
      "Epoch [62/100], Step [240/1248], Loss: 0.0009\n",
      "Epoch [62/100], Step [260/1248], Loss: 0.0016\n",
      "Epoch [62/100], Step [280/1248], Loss: 0.0069\n",
      "Epoch [62/100], Step [300/1248], Loss: 0.0159\n",
      "Epoch [62/100], Step [320/1248], Loss: 0.0577\n",
      "Epoch [62/100], Step [340/1248], Loss: 0.0003\n",
      "Epoch [62/100], Step [360/1248], Loss: 0.0120\n",
      "Epoch [62/100], Step [380/1248], Loss: 0.0012\n",
      "Epoch [62/100], Step [400/1248], Loss: 0.0021\n",
      "Epoch [62/100], Step [420/1248], Loss: 0.0371\n",
      "Epoch [62/100], Step [440/1248], Loss: 0.0003\n",
      "Epoch [62/100], Step [460/1248], Loss: 0.0004\n",
      "Epoch [62/100], Step [480/1248], Loss: 0.0029\n",
      "Epoch [62/100], Step [500/1248], Loss: 0.0047\n",
      "Epoch [62/100], Step [520/1248], Loss: 0.0075\n",
      "Epoch [62/100], Step [540/1248], Loss: 0.0112\n",
      "Epoch [62/100], Step [560/1248], Loss: 0.0141\n",
      "Epoch [62/100], Step [580/1248], Loss: 0.0010\n",
      "Epoch [62/100], Step [600/1248], Loss: 0.0015\n",
      "Epoch [62/100], Step [620/1248], Loss: 0.0123\n",
      "Epoch [62/100], Step [640/1248], Loss: 0.0095\n",
      "Epoch [62/100], Step [660/1248], Loss: 0.0046\n",
      "Epoch [62/100], Step [680/1248], Loss: 0.0390\n",
      "Epoch [62/100], Step [700/1248], Loss: 0.0117\n",
      "Epoch [62/100], Step [720/1248], Loss: 0.0007\n",
      "Epoch [62/100], Step [740/1248], Loss: 0.0008\n",
      "Epoch [62/100], Step [760/1248], Loss: 0.0556\n",
      "Epoch [62/100], Step [780/1248], Loss: 0.0028\n",
      "Epoch [62/100], Step [800/1248], Loss: 0.0020\n",
      "Epoch [62/100], Step [820/1248], Loss: 0.0385\n",
      "Epoch [62/100], Step [840/1248], Loss: 0.0146\n",
      "Epoch [62/100], Step [860/1248], Loss: 0.0111\n",
      "Epoch [62/100], Step [880/1248], Loss: 0.0007\n",
      "Epoch [62/100], Step [900/1248], Loss: 0.0004\n",
      "Epoch [62/100], Step [920/1248], Loss: 0.0720\n",
      "Epoch [62/100], Step [940/1248], Loss: 0.3949\n",
      "Epoch [62/100], Step [960/1248], Loss: 0.0055\n",
      "Epoch [62/100], Step [980/1248], Loss: 0.0229\n",
      "Epoch [62/100], Step [1000/1248], Loss: 0.0003\n",
      "Epoch [62/100], Step [1020/1248], Loss: 0.0002\n",
      "Epoch [62/100], Step [1040/1248], Loss: 0.0159\n",
      "Epoch [62/100], Step [1060/1248], Loss: 0.1535\n",
      "Epoch [62/100], Step [1080/1248], Loss: 0.0003\n",
      "Epoch [62/100], Step [1100/1248], Loss: 0.1772\n",
      "Epoch [62/100], Step [1120/1248], Loss: 0.0031\n",
      "Epoch [62/100], Step [1140/1248], Loss: 0.0009\n",
      "Epoch [62/100], Step [1160/1248], Loss: 0.0072\n",
      "Epoch [62/100], Step [1180/1248], Loss: 0.2540\n",
      "Epoch [62/100], Step [1200/1248], Loss: 0.0010\n",
      "Epoch [62/100], Step [1220/1248], Loss: 0.0075\n",
      "Epoch [62/100], Step [1240/1248], Loss: 0.0172\n",
      "\n",
      "train-loss: 0.1413, train-acc: 99.1834\n",
      "validation loss: 1.6677, validation acc: 69.1578\n",
      "\n",
      "Epoch 63\n",
      "\n",
      "Epoch [63/100], Step [0/1248], Loss: 0.0086\n",
      "Epoch [63/100], Step [20/1248], Loss: 0.0084\n",
      "Epoch [63/100], Step [40/1248], Loss: 0.0065\n",
      "Epoch [63/100], Step [60/1248], Loss: 0.0046\n",
      "Epoch [63/100], Step [80/1248], Loss: 0.0001\n",
      "Epoch [63/100], Step [100/1248], Loss: 0.0028\n",
      "Epoch [63/100], Step [120/1248], Loss: 0.0123\n",
      "Epoch [63/100], Step [140/1248], Loss: 0.0031\n",
      "Epoch [63/100], Step [160/1248], Loss: 0.0021\n",
      "Epoch [63/100], Step [180/1248], Loss: 0.0775\n",
      "Epoch [63/100], Step [200/1248], Loss: 0.0061\n",
      "Epoch [63/100], Step [220/1248], Loss: 0.0018\n",
      "Epoch [63/100], Step [240/1248], Loss: 0.0003\n",
      "Epoch [63/100], Step [260/1248], Loss: 0.0008\n",
      "Epoch [63/100], Step [280/1248], Loss: 0.0314\n",
      "Epoch [63/100], Step [300/1248], Loss: 0.2629\n",
      "Epoch [63/100], Step [320/1248], Loss: 0.0007\n",
      "Epoch [63/100], Step [340/1248], Loss: 0.0121\n",
      "Epoch [63/100], Step [360/1248], Loss: 0.0013\n",
      "Epoch [63/100], Step [380/1248], Loss: 0.0013\n",
      "Epoch [63/100], Step [400/1248], Loss: 0.0004\n",
      "Epoch [63/100], Step [420/1248], Loss: 0.0008\n",
      "Epoch [63/100], Step [440/1248], Loss: 0.0015\n",
      "Epoch [63/100], Step [460/1248], Loss: 0.0076\n",
      "Epoch [63/100], Step [480/1248], Loss: 0.0056\n",
      "Epoch [63/100], Step [500/1248], Loss: 0.0337\n",
      "Epoch [63/100], Step [520/1248], Loss: 0.0002\n",
      "Epoch [63/100], Step [540/1248], Loss: 0.0011\n",
      "Epoch [63/100], Step [560/1248], Loss: 0.0189\n",
      "Epoch [63/100], Step [580/1248], Loss: 0.0096\n",
      "Epoch [63/100], Step [600/1248], Loss: 0.0045\n",
      "Epoch [63/100], Step [620/1248], Loss: 0.0803\n",
      "Epoch [63/100], Step [640/1248], Loss: 0.0016\n",
      "Epoch [63/100], Step [660/1248], Loss: 0.0016\n",
      "Epoch [63/100], Step [680/1248], Loss: 0.0024\n",
      "Epoch [63/100], Step [700/1248], Loss: 0.0082\n",
      "Epoch [63/100], Step [720/1248], Loss: 0.0010\n",
      "Epoch [63/100], Step [740/1248], Loss: 0.0027\n",
      "Epoch [63/100], Step [760/1248], Loss: 0.0003\n",
      "Epoch [63/100], Step [780/1248], Loss: 0.0130\n",
      "Epoch [63/100], Step [800/1248], Loss: 0.0041\n",
      "Epoch [63/100], Step [820/1248], Loss: 0.0026\n",
      "Epoch [63/100], Step [840/1248], Loss: 0.0003\n",
      "Epoch [63/100], Step [860/1248], Loss: 0.0101\n",
      "Epoch [63/100], Step [880/1248], Loss: 0.0059\n",
      "Epoch [63/100], Step [900/1248], Loss: 0.0024\n",
      "Epoch [63/100], Step [920/1248], Loss: 0.0000\n",
      "Epoch [63/100], Step [940/1248], Loss: 0.0018\n",
      "Epoch [63/100], Step [960/1248], Loss: 0.0778\n",
      "Epoch [63/100], Step [980/1248], Loss: 0.0008\n",
      "Epoch [63/100], Step [1000/1248], Loss: 0.0086\n",
      "Epoch [63/100], Step [1020/1248], Loss: 0.0690\n",
      "Epoch [63/100], Step [1040/1248], Loss: 0.0086\n",
      "Epoch [63/100], Step [1060/1248], Loss: 0.0242\n",
      "Epoch [63/100], Step [1080/1248], Loss: 0.0231\n",
      "Epoch [63/100], Step [1100/1248], Loss: 0.1895\n",
      "Epoch [63/100], Step [1120/1248], Loss: 0.0146\n",
      "Epoch [63/100], Step [1140/1248], Loss: 0.0037\n",
      "Epoch [63/100], Step [1160/1248], Loss: 0.0045\n",
      "Epoch [63/100], Step [1180/1248], Loss: 0.0270\n",
      "Epoch [63/100], Step [1200/1248], Loss: 0.0462\n",
      "Epoch [63/100], Step [1220/1248], Loss: 0.0093\n",
      "Epoch [63/100], Step [1240/1248], Loss: 0.0007\n",
      "\n",
      "train-loss: 0.1396, train-acc: 98.8878\n",
      "validation loss: 1.6733, validation acc: 67.9312\n",
      "\n",
      "Epoch 64\n",
      "\n",
      "Epoch [64/100], Step [0/1248], Loss: 0.0060\n",
      "Epoch [64/100], Step [20/1248], Loss: 0.0004\n",
      "Epoch [64/100], Step [40/1248], Loss: 0.0233\n",
      "Epoch [64/100], Step [60/1248], Loss: 0.0019\n",
      "Epoch [64/100], Step [80/1248], Loss: 0.0007\n",
      "Epoch [64/100], Step [100/1248], Loss: 0.0269\n",
      "Epoch [64/100], Step [120/1248], Loss: 0.0029\n",
      "Epoch [64/100], Step [140/1248], Loss: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100], Step [160/1248], Loss: 0.0032\n",
      "Epoch [64/100], Step [180/1248], Loss: 0.0001\n",
      "Epoch [64/100], Step [200/1248], Loss: 0.0036\n",
      "Epoch [64/100], Step [220/1248], Loss: 0.0051\n",
      "Epoch [64/100], Step [240/1248], Loss: 0.0010\n",
      "Epoch [64/100], Step [260/1248], Loss: 0.0148\n",
      "Epoch [64/100], Step [280/1248], Loss: 0.0010\n",
      "Epoch [64/100], Step [300/1248], Loss: 0.0172\n",
      "Epoch [64/100], Step [320/1248], Loss: 0.0031\n",
      "Epoch [64/100], Step [340/1248], Loss: 0.0104\n",
      "Epoch [64/100], Step [360/1248], Loss: 0.0019\n",
      "Epoch [64/100], Step [380/1248], Loss: 0.0132\n",
      "Epoch [64/100], Step [400/1248], Loss: 0.0094\n",
      "Epoch [64/100], Step [420/1248], Loss: 0.0004\n",
      "Epoch [64/100], Step [440/1248], Loss: 0.0113\n",
      "Epoch [64/100], Step [460/1248], Loss: 0.0021\n",
      "Epoch [64/100], Step [480/1248], Loss: 0.0031\n",
      "Epoch [64/100], Step [500/1248], Loss: 0.0289\n",
      "Epoch [64/100], Step [520/1248], Loss: 0.0020\n",
      "Epoch [64/100], Step [540/1248], Loss: 0.0049\n",
      "Epoch [64/100], Step [560/1248], Loss: 0.0001\n",
      "Epoch [64/100], Step [580/1248], Loss: 0.0014\n",
      "Epoch [64/100], Step [600/1248], Loss: 0.0081\n",
      "Epoch [64/100], Step [620/1248], Loss: 0.4363\n",
      "Epoch [64/100], Step [640/1248], Loss: 0.1472\n",
      "Epoch [64/100], Step [660/1248], Loss: 0.1974\n",
      "Epoch [64/100], Step [680/1248], Loss: 0.0015\n",
      "Epoch [64/100], Step [700/1248], Loss: 0.0059\n",
      "Epoch [64/100], Step [720/1248], Loss: 0.0092\n",
      "Epoch [64/100], Step [740/1248], Loss: 0.0012\n",
      "Epoch [64/100], Step [760/1248], Loss: 0.1152\n",
      "Epoch [64/100], Step [780/1248], Loss: 0.0071\n",
      "Epoch [64/100], Step [800/1248], Loss: 0.0047\n",
      "Epoch [64/100], Step [820/1248], Loss: 0.0186\n",
      "Epoch [64/100], Step [840/1248], Loss: 0.0056\n",
      "Epoch [64/100], Step [860/1248], Loss: 0.0007\n",
      "Epoch [64/100], Step [880/1248], Loss: 0.0002\n",
      "Epoch [64/100], Step [900/1248], Loss: 0.0364\n",
      "Epoch [64/100], Step [920/1248], Loss: 0.0022\n",
      "Epoch [64/100], Step [940/1248], Loss: 0.0008\n",
      "Epoch [64/100], Step [960/1248], Loss: 0.0020\n",
      "Epoch [64/100], Step [980/1248], Loss: 0.2550\n",
      "Epoch [64/100], Step [1000/1248], Loss: 0.0067\n",
      "Epoch [64/100], Step [1020/1248], Loss: 0.0016\n",
      "Epoch [64/100], Step [1040/1248], Loss: 0.0004\n",
      "Epoch [64/100], Step [1060/1248], Loss: 0.0006\n",
      "Epoch [64/100], Step [1080/1248], Loss: 0.0650\n",
      "Epoch [64/100], Step [1100/1248], Loss: 0.0011\n",
      "Epoch [64/100], Step [1120/1248], Loss: 0.0047\n",
      "Epoch [64/100], Step [1140/1248], Loss: 0.0028\n",
      "Epoch [64/100], Step [1160/1248], Loss: 0.0019\n",
      "Epoch [64/100], Step [1180/1248], Loss: 0.0329\n",
      "Epoch [64/100], Step [1200/1248], Loss: 0.0018\n",
      "Epoch [64/100], Step [1220/1248], Loss: 0.0004\n",
      "Epoch [64/100], Step [1240/1248], Loss: 0.0419\n",
      "\n",
      "train-loss: 0.1379, train-acc: 98.9279\n",
      "validation loss: 1.6767, validation acc: 69.2210\n",
      "\n",
      "Epoch 65\n",
      "\n",
      "Epoch [65/100], Step [0/1248], Loss: 0.0013\n",
      "Epoch [65/100], Step [20/1248], Loss: 0.0011\n",
      "Epoch [65/100], Step [40/1248], Loss: 0.0118\n",
      "Epoch [65/100], Step [60/1248], Loss: 0.0001\n",
      "Epoch [65/100], Step [80/1248], Loss: 0.0003\n",
      "Epoch [65/100], Step [100/1248], Loss: 0.0001\n",
      "Epoch [65/100], Step [120/1248], Loss: 0.1414\n",
      "Epoch [65/100], Step [140/1248], Loss: 0.0013\n",
      "Epoch [65/100], Step [160/1248], Loss: 0.0147\n",
      "Epoch [65/100], Step [180/1248], Loss: 0.0006\n",
      "Epoch [65/100], Step [200/1248], Loss: 0.0021\n",
      "Epoch [65/100], Step [220/1248], Loss: 0.0029\n",
      "Epoch [65/100], Step [240/1248], Loss: 0.0021\n",
      "Epoch [65/100], Step [260/1248], Loss: 0.0264\n",
      "Epoch [65/100], Step [280/1248], Loss: 0.0001\n",
      "Epoch [65/100], Step [300/1248], Loss: 0.0709\n",
      "Epoch [65/100], Step [320/1248], Loss: 0.0367\n",
      "Epoch [65/100], Step [340/1248], Loss: 0.0008\n",
      "Epoch [65/100], Step [360/1248], Loss: 0.0011\n",
      "Epoch [65/100], Step [380/1248], Loss: 0.0219\n",
      "Epoch [65/100], Step [400/1248], Loss: 0.0561\n",
      "Epoch [65/100], Step [420/1248], Loss: 0.0008\n",
      "Epoch [65/100], Step [440/1248], Loss: 0.0003\n",
      "Epoch [65/100], Step [460/1248], Loss: 0.0009\n",
      "Epoch [65/100], Step [480/1248], Loss: 0.0066\n",
      "Epoch [65/100], Step [500/1248], Loss: 0.0005\n",
      "Epoch [65/100], Step [520/1248], Loss: 0.0332\n",
      "Epoch [65/100], Step [540/1248], Loss: 0.0020\n",
      "Epoch [65/100], Step [560/1248], Loss: 0.0015\n",
      "Epoch [65/100], Step [580/1248], Loss: 0.0484\n",
      "Epoch [65/100], Step [600/1248], Loss: 0.0020\n",
      "Epoch [65/100], Step [620/1248], Loss: 0.0089\n",
      "Epoch [65/100], Step [640/1248], Loss: 0.0018\n",
      "Epoch [65/100], Step [660/1248], Loss: 0.0006\n",
      "Epoch [65/100], Step [680/1248], Loss: 0.0010\n",
      "Epoch [65/100], Step [700/1248], Loss: 0.0317\n",
      "Epoch [65/100], Step [720/1248], Loss: 0.0032\n",
      "Epoch [65/100], Step [740/1248], Loss: 0.0058\n",
      "Epoch [65/100], Step [760/1248], Loss: 0.0030\n",
      "Epoch [65/100], Step [780/1248], Loss: 0.0034\n",
      "Epoch [65/100], Step [800/1248], Loss: 0.0171\n",
      "Epoch [65/100], Step [820/1248], Loss: 0.0010\n",
      "Epoch [65/100], Step [840/1248], Loss: 0.0003\n",
      "Epoch [65/100], Step [860/1248], Loss: 0.0120\n",
      "Epoch [65/100], Step [880/1248], Loss: 0.0007\n",
      "Epoch [65/100], Step [900/1248], Loss: 0.0134\n",
      "Epoch [65/100], Step [920/1248], Loss: 0.0034\n",
      "Epoch [65/100], Step [940/1248], Loss: 0.0022\n",
      "Epoch [65/100], Step [960/1248], Loss: 0.0006\n",
      "Epoch [65/100], Step [980/1248], Loss: 0.1379\n",
      "Epoch [65/100], Step [1000/1248], Loss: 0.0008\n",
      "Epoch [65/100], Step [1020/1248], Loss: 0.0247\n",
      "Epoch [65/100], Step [1040/1248], Loss: 0.0035\n",
      "Epoch [65/100], Step [1060/1248], Loss: 0.0069\n",
      "Epoch [65/100], Step [1080/1248], Loss: 0.0171\n",
      "Epoch [65/100], Step [1100/1248], Loss: 0.0125\n",
      "Epoch [65/100], Step [1120/1248], Loss: 0.0364\n",
      "Epoch [65/100], Step [1140/1248], Loss: 0.1326\n",
      "Epoch [65/100], Step [1160/1248], Loss: 0.0012\n",
      "Epoch [65/100], Step [1180/1248], Loss: 0.0029\n",
      "Epoch [65/100], Step [1200/1248], Loss: 0.0425\n",
      "Epoch [65/100], Step [1220/1248], Loss: 0.1958\n",
      "Epoch [65/100], Step [1240/1248], Loss: 0.0498\n",
      "\n",
      "train-loss: 0.1362, train-acc: 99.0180\n",
      "validation loss: 1.6810, validation acc: 68.1588\n",
      "\n",
      "Epoch 66\n",
      "\n",
      "Epoch [66/100], Step [0/1248], Loss: 0.0053\n",
      "Epoch [66/100], Step [20/1248], Loss: 0.0003\n",
      "Epoch [66/100], Step [40/1248], Loss: 0.0010\n",
      "Epoch [66/100], Step [60/1248], Loss: 0.0024\n",
      "Epoch [66/100], Step [80/1248], Loss: 0.0167\n",
      "Epoch [66/100], Step [100/1248], Loss: 0.2047\n",
      "Epoch [66/100], Step [120/1248], Loss: 0.0020\n",
      "Epoch [66/100], Step [140/1248], Loss: 0.0042\n",
      "Epoch [66/100], Step [160/1248], Loss: 0.0385\n",
      "Epoch [66/100], Step [180/1248], Loss: 0.0268\n",
      "Epoch [66/100], Step [200/1248], Loss: 0.0055\n",
      "Epoch [66/100], Step [220/1248], Loss: 0.0004\n",
      "Epoch [66/100], Step [240/1248], Loss: 0.0808\n",
      "Epoch [66/100], Step [260/1248], Loss: 0.2056\n",
      "Epoch [66/100], Step [280/1248], Loss: 0.0016\n",
      "Epoch [66/100], Step [300/1248], Loss: 0.0062\n",
      "Epoch [66/100], Step [320/1248], Loss: 0.0136\n",
      "Epoch [66/100], Step [340/1248], Loss: 0.0133\n",
      "Epoch [66/100], Step [360/1248], Loss: 0.0492\n",
      "Epoch [66/100], Step [380/1248], Loss: 0.0001\n",
      "Epoch [66/100], Step [400/1248], Loss: 0.0010\n",
      "Epoch [66/100], Step [420/1248], Loss: 0.1934\n",
      "Epoch [66/100], Step [440/1248], Loss: 0.0027\n",
      "Epoch [66/100], Step [460/1248], Loss: 0.0641\n",
      "Epoch [66/100], Step [480/1248], Loss: 0.0008\n",
      "Epoch [66/100], Step [500/1248], Loss: 0.0010\n",
      "Epoch [66/100], Step [520/1248], Loss: 0.0012\n",
      "Epoch [66/100], Step [540/1248], Loss: 0.0178\n",
      "Epoch [66/100], Step [560/1248], Loss: 0.0036\n",
      "Epoch [66/100], Step [580/1248], Loss: 0.0040\n",
      "Epoch [66/100], Step [600/1248], Loss: 0.0027\n",
      "Epoch [66/100], Step [620/1248], Loss: 0.0126\n",
      "Epoch [66/100], Step [640/1248], Loss: 0.0115\n",
      "Epoch [66/100], Step [660/1248], Loss: 0.0003\n",
      "Epoch [66/100], Step [680/1248], Loss: 0.0020\n",
      "Epoch [66/100], Step [700/1248], Loss: 0.0006\n",
      "Epoch [66/100], Step [720/1248], Loss: 0.0010\n",
      "Epoch [66/100], Step [740/1248], Loss: 0.0008\n",
      "Epoch [66/100], Step [760/1248], Loss: 0.0015\n",
      "Epoch [66/100], Step [780/1248], Loss: 0.0003\n",
      "Epoch [66/100], Step [800/1248], Loss: 0.0410\n",
      "Epoch [66/100], Step [820/1248], Loss: 0.0001\n",
      "Epoch [66/100], Step [840/1248], Loss: 0.0012\n",
      "Epoch [66/100], Step [860/1248], Loss: 0.0011\n",
      "Epoch [66/100], Step [880/1248], Loss: 0.0155\n",
      "Epoch [66/100], Step [900/1248], Loss: 0.0094\n",
      "Epoch [66/100], Step [920/1248], Loss: 0.0424\n",
      "Epoch [66/100], Step [940/1248], Loss: 0.0002\n",
      "Epoch [66/100], Step [960/1248], Loss: 0.0020\n",
      "Epoch [66/100], Step [980/1248], Loss: 0.0001\n",
      "Epoch [66/100], Step [1000/1248], Loss: 0.0012\n",
      "Epoch [66/100], Step [1020/1248], Loss: 0.0029\n",
      "Epoch [66/100], Step [1040/1248], Loss: 0.0456\n",
      "Epoch [66/100], Step [1060/1248], Loss: 0.0187\n",
      "Epoch [66/100], Step [1080/1248], Loss: 0.0020\n",
      "Epoch [66/100], Step [1100/1248], Loss: 0.0009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100], Step [1120/1248], Loss: 0.0003\n",
      "Epoch [66/100], Step [1140/1248], Loss: 0.0177\n",
      "Epoch [66/100], Step [1160/1248], Loss: 0.0714\n",
      "Epoch [66/100], Step [1180/1248], Loss: 0.0006\n",
      "Epoch [66/100], Step [1200/1248], Loss: 0.0001\n",
      "Epoch [66/100], Step [1220/1248], Loss: 0.0018\n",
      "Epoch [66/100], Step [1240/1248], Loss: 0.0001\n",
      "\n",
      "train-loss: 0.1345, train-acc: 99.0581\n",
      "validation loss: 1.6844, validation acc: 68.4244\n",
      "\n",
      "Epoch 67\n",
      "\n",
      "Epoch [67/100], Step [0/1248], Loss: 0.0002\n",
      "Epoch [67/100], Step [20/1248], Loss: 0.0047\n",
      "Epoch [67/100], Step [40/1248], Loss: 0.0159\n",
      "Epoch [67/100], Step [60/1248], Loss: 0.0017\n",
      "Epoch [67/100], Step [80/1248], Loss: 0.0008\n",
      "Epoch [67/100], Step [100/1248], Loss: 0.4273\n",
      "Epoch [67/100], Step [120/1248], Loss: 0.0003\n",
      "Epoch [67/100], Step [140/1248], Loss: 0.0045\n",
      "Epoch [67/100], Step [160/1248], Loss: 0.0023\n",
      "Epoch [67/100], Step [180/1248], Loss: 0.0015\n",
      "Epoch [67/100], Step [200/1248], Loss: 0.0008\n",
      "Epoch [67/100], Step [220/1248], Loss: 0.0021\n",
      "Epoch [67/100], Step [240/1248], Loss: 0.0003\n",
      "Epoch [67/100], Step [260/1248], Loss: 0.0037\n",
      "Epoch [67/100], Step [280/1248], Loss: 0.0006\n",
      "Epoch [67/100], Step [300/1248], Loss: 0.0002\n",
      "Epoch [67/100], Step [320/1248], Loss: 0.0027\n",
      "Epoch [67/100], Step [340/1248], Loss: 0.0027\n",
      "Epoch [67/100], Step [360/1248], Loss: 0.0046\n",
      "Epoch [67/100], Step [380/1248], Loss: 0.0032\n",
      "Epoch [67/100], Step [400/1248], Loss: 0.0036\n",
      "Epoch [67/100], Step [420/1248], Loss: 0.0014\n",
      "Epoch [67/100], Step [440/1248], Loss: 0.0328\n",
      "Epoch [67/100], Step [460/1248], Loss: 0.0069\n",
      "Epoch [67/100], Step [480/1248], Loss: 0.0004\n",
      "Epoch [67/100], Step [500/1248], Loss: 0.0015\n",
      "Epoch [67/100], Step [520/1248], Loss: 0.0003\n",
      "Epoch [67/100], Step [540/1248], Loss: 0.0060\n",
      "Epoch [67/100], Step [560/1248], Loss: 0.0080\n",
      "Epoch [67/100], Step [580/1248], Loss: 0.0017\n",
      "Epoch [67/100], Step [600/1248], Loss: 0.0058\n",
      "Epoch [67/100], Step [620/1248], Loss: 0.0007\n",
      "Epoch [67/100], Step [640/1248], Loss: 0.0007\n",
      "Epoch [67/100], Step [660/1248], Loss: 0.0004\n",
      "Epoch [67/100], Step [680/1248], Loss: 0.0070\n",
      "Epoch [67/100], Step [700/1248], Loss: 0.0027\n",
      "Epoch [67/100], Step [720/1248], Loss: 0.0027\n",
      "Epoch [67/100], Step [740/1248], Loss: 0.0058\n",
      "Epoch [67/100], Step [760/1248], Loss: 0.0002\n",
      "Epoch [67/100], Step [780/1248], Loss: 0.0020\n",
      "Epoch [67/100], Step [800/1248], Loss: 0.0055\n",
      "Epoch [67/100], Step [820/1248], Loss: 0.0289\n",
      "Epoch [67/100], Step [840/1248], Loss: 0.0072\n",
      "Epoch [67/100], Step [860/1248], Loss: 0.0522\n",
      "Epoch [67/100], Step [880/1248], Loss: 0.0978\n",
      "Epoch [67/100], Step [900/1248], Loss: 0.0343\n",
      "Epoch [67/100], Step [920/1248], Loss: 0.0301\n",
      "Epoch [67/100], Step [940/1248], Loss: 0.0063\n",
      "Epoch [67/100], Step [960/1248], Loss: 0.0020\n",
      "Epoch [67/100], Step [980/1248], Loss: 0.0127\n",
      "Epoch [67/100], Step [1000/1248], Loss: 0.0165\n",
      "Epoch [67/100], Step [1020/1248], Loss: 0.1545\n",
      "Epoch [67/100], Step [1040/1248], Loss: 0.0150\n",
      "Epoch [67/100], Step [1060/1248], Loss: 0.0017\n",
      "Epoch [67/100], Step [1080/1248], Loss: 0.0019\n",
      "Epoch [67/100], Step [1100/1248], Loss: 0.0020\n",
      "Epoch [67/100], Step [1120/1248], Loss: 0.0050\n",
      "Epoch [67/100], Step [1140/1248], Loss: 0.0033\n",
      "Epoch [67/100], Step [1160/1248], Loss: 0.0338\n",
      "Epoch [67/100], Step [1180/1248], Loss: 0.0004\n",
      "Epoch [67/100], Step [1200/1248], Loss: 0.0008\n",
      "Epoch [67/100], Step [1220/1248], Loss: 0.0079\n",
      "Epoch [67/100], Step [1240/1248], Loss: 0.0021\n",
      "\n",
      "train-loss: 0.1329, train-acc: 99.1884\n",
      "validation loss: 1.6882, validation acc: 68.2979\n",
      "\n",
      "Epoch 68\n",
      "\n",
      "Epoch [68/100], Step [0/1248], Loss: 0.0030\n",
      "Epoch [68/100], Step [20/1248], Loss: 0.0073\n",
      "Epoch [68/100], Step [40/1248], Loss: 0.0154\n",
      "Epoch [68/100], Step [60/1248], Loss: 0.0004\n",
      "Epoch [68/100], Step [80/1248], Loss: 0.1201\n",
      "Epoch [68/100], Step [100/1248], Loss: 0.6137\n",
      "Epoch [68/100], Step [120/1248], Loss: 0.0097\n",
      "Epoch [68/100], Step [140/1248], Loss: 0.0007\n",
      "Epoch [68/100], Step [160/1248], Loss: 0.0082\n",
      "Epoch [68/100], Step [180/1248], Loss: 0.3228\n",
      "Epoch [68/100], Step [200/1248], Loss: 0.0448\n",
      "Epoch [68/100], Step [220/1248], Loss: 0.0092\n",
      "Epoch [68/100], Step [240/1248], Loss: 0.0149\n",
      "Epoch [68/100], Step [260/1248], Loss: 0.0031\n",
      "Epoch [68/100], Step [280/1248], Loss: 0.0025\n",
      "Epoch [68/100], Step [300/1248], Loss: 0.0045\n",
      "Epoch [68/100], Step [320/1248], Loss: 0.0004\n",
      "Epoch [68/100], Step [340/1248], Loss: 0.0510\n",
      "Epoch [68/100], Step [360/1248], Loss: 0.0020\n",
      "Epoch [68/100], Step [380/1248], Loss: 0.0012\n",
      "Epoch [68/100], Step [400/1248], Loss: 0.1827\n",
      "Epoch [68/100], Step [420/1248], Loss: 0.0101\n",
      "Epoch [68/100], Step [440/1248], Loss: 0.0009\n",
      "Epoch [68/100], Step [460/1248], Loss: 0.1768\n",
      "Epoch [68/100], Step [480/1248], Loss: 0.0215\n",
      "Epoch [68/100], Step [500/1248], Loss: 0.0887\n",
      "Epoch [68/100], Step [520/1248], Loss: 0.0006\n",
      "Epoch [68/100], Step [540/1248], Loss: 0.0024\n",
      "Epoch [68/100], Step [560/1248], Loss: 0.0175\n",
      "Epoch [68/100], Step [580/1248], Loss: 0.3573\n",
      "Epoch [68/100], Step [600/1248], Loss: 0.0993\n",
      "Epoch [68/100], Step [620/1248], Loss: 0.0171\n",
      "Epoch [68/100], Step [640/1248], Loss: 0.0014\n",
      "Epoch [68/100], Step [660/1248], Loss: 0.0979\n",
      "Epoch [68/100], Step [680/1248], Loss: 0.0825\n",
      "Epoch [68/100], Step [700/1248], Loss: 0.1711\n",
      "Epoch [68/100], Step [720/1248], Loss: 0.1399\n",
      "Epoch [68/100], Step [740/1248], Loss: 0.0128\n",
      "Epoch [68/100], Step [760/1248], Loss: 0.0434\n",
      "Epoch [68/100], Step [780/1248], Loss: 0.0042\n",
      "Epoch [68/100], Step [800/1248], Loss: 0.0517\n",
      "Epoch [68/100], Step [820/1248], Loss: 0.0072\n",
      "Epoch [68/100], Step [840/1248], Loss: 0.0103\n",
      "Epoch [68/100], Step [860/1248], Loss: 0.0012\n",
      "Epoch [68/100], Step [880/1248], Loss: 0.0087\n",
      "Epoch [68/100], Step [900/1248], Loss: 0.0025\n",
      "Epoch [68/100], Step [920/1248], Loss: 0.0007\n",
      "Epoch [68/100], Step [940/1248], Loss: 0.0622\n",
      "Epoch [68/100], Step [960/1248], Loss: 0.0024\n",
      "Epoch [68/100], Step [980/1248], Loss: 0.0179\n",
      "Epoch [68/100], Step [1000/1248], Loss: 0.0004\n",
      "Epoch [68/100], Step [1020/1248], Loss: 0.0011\n",
      "Epoch [68/100], Step [1040/1248], Loss: 0.0198\n",
      "Epoch [68/100], Step [1060/1248], Loss: 0.0197\n",
      "Epoch [68/100], Step [1080/1248], Loss: 0.0227\n",
      "Epoch [68/100], Step [1100/1248], Loss: 0.0000\n",
      "Epoch [68/100], Step [1120/1248], Loss: 0.1916\n",
      "Epoch [68/100], Step [1140/1248], Loss: 0.0029\n",
      "Epoch [68/100], Step [1160/1248], Loss: 0.0038\n",
      "Epoch [68/100], Step [1180/1248], Loss: 0.0183\n",
      "Epoch [68/100], Step [1200/1248], Loss: 0.0035\n",
      "Epoch [68/100], Step [1220/1248], Loss: 0.0137\n",
      "Epoch [68/100], Step [1240/1248], Loss: 0.0013\n",
      "\n",
      "train-loss: 0.1314, train-acc: 98.9629\n",
      "validation loss: 1.6915, validation acc: 68.4117\n",
      "\n",
      "Epoch 69\n",
      "\n",
      "Epoch [69/100], Step [0/1248], Loss: 0.0288\n",
      "Epoch [69/100], Step [20/1248], Loss: 0.0121\n",
      "Epoch [69/100], Step [40/1248], Loss: 0.0014\n",
      "Epoch [69/100], Step [60/1248], Loss: 0.0233\n",
      "Epoch [69/100], Step [80/1248], Loss: 0.0137\n",
      "Epoch [69/100], Step [100/1248], Loss: 0.0202\n",
      "Epoch [69/100], Step [120/1248], Loss: 0.0010\n",
      "Epoch [69/100], Step [140/1248], Loss: 0.0000\n",
      "Epoch [69/100], Step [160/1248], Loss: 0.0009\n",
      "Epoch [69/100], Step [180/1248], Loss: 0.0011\n",
      "Epoch [69/100], Step [200/1248], Loss: 0.0005\n",
      "Epoch [69/100], Step [220/1248], Loss: 0.0002\n",
      "Epoch [69/100], Step [240/1248], Loss: 0.0009\n",
      "Epoch [69/100], Step [260/1248], Loss: 0.0031\n",
      "Epoch [69/100], Step [280/1248], Loss: 0.0613\n",
      "Epoch [69/100], Step [300/1248], Loss: 0.0289\n",
      "Epoch [69/100], Step [320/1248], Loss: 0.0020\n",
      "Epoch [69/100], Step [340/1248], Loss: 0.0005\n",
      "Epoch [69/100], Step [360/1248], Loss: 0.0243\n",
      "Epoch [69/100], Step [380/1248], Loss: 0.0029\n",
      "Epoch [69/100], Step [400/1248], Loss: 0.0009\n",
      "Epoch [69/100], Step [420/1248], Loss: 0.0007\n",
      "Epoch [69/100], Step [440/1248], Loss: 0.0179\n",
      "Epoch [69/100], Step [460/1248], Loss: 0.0023\n",
      "Epoch [69/100], Step [480/1248], Loss: 0.0005\n",
      "Epoch [69/100], Step [500/1248], Loss: 0.0016\n",
      "Epoch [69/100], Step [520/1248], Loss: 0.0074\n",
      "Epoch [69/100], Step [540/1248], Loss: 0.0021\n",
      "Epoch [69/100], Step [560/1248], Loss: 0.0398\n",
      "Epoch [69/100], Step [580/1248], Loss: 0.0013\n",
      "Epoch [69/100], Step [600/1248], Loss: 0.0472\n",
      "Epoch [69/100], Step [620/1248], Loss: 0.0017\n",
      "Epoch [69/100], Step [640/1248], Loss: 0.0145\n",
      "Epoch [69/100], Step [660/1248], Loss: 0.0025\n",
      "Epoch [69/100], Step [680/1248], Loss: 0.0000\n",
      "Epoch [69/100], Step [700/1248], Loss: 0.0073\n",
      "Epoch [69/100], Step [720/1248], Loss: 0.0182\n",
      "Epoch [69/100], Step [740/1248], Loss: 0.0052\n",
      "Epoch [69/100], Step [760/1248], Loss: 0.1536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/100], Step [780/1248], Loss: 0.0069\n",
      "Epoch [69/100], Step [800/1248], Loss: 0.0027\n",
      "Epoch [69/100], Step [820/1248], Loss: 0.0002\n",
      "Epoch [69/100], Step [840/1248], Loss: 0.0125\n",
      "Epoch [69/100], Step [860/1248], Loss: 0.0013\n",
      "Epoch [69/100], Step [880/1248], Loss: 0.0066\n",
      "Epoch [69/100], Step [900/1248], Loss: 0.0098\n",
      "Epoch [69/100], Step [920/1248], Loss: 0.0005\n",
      "Epoch [69/100], Step [940/1248], Loss: 0.0042\n",
      "Epoch [69/100], Step [960/1248], Loss: 0.0749\n",
      "Epoch [69/100], Step [980/1248], Loss: 0.0060\n",
      "Epoch [69/100], Step [1000/1248], Loss: 0.0863\n",
      "Epoch [69/100], Step [1020/1248], Loss: 0.0011\n",
      "Epoch [69/100], Step [1040/1248], Loss: 0.0047\n",
      "Epoch [69/100], Step [1060/1248], Loss: 0.0146\n",
      "Epoch [69/100], Step [1080/1248], Loss: 0.0074\n",
      "Epoch [69/100], Step [1100/1248], Loss: 0.0037\n",
      "Epoch [69/100], Step [1120/1248], Loss: 0.0007\n",
      "Epoch [69/100], Step [1140/1248], Loss: 0.0040\n",
      "Epoch [69/100], Step [1160/1248], Loss: 0.0037\n",
      "Epoch [69/100], Step [1180/1248], Loss: 0.0188\n",
      "Epoch [69/100], Step [1200/1248], Loss: 0.0049\n",
      "Epoch [69/100], Step [1220/1248], Loss: 0.0026\n",
      "Epoch [69/100], Step [1240/1248], Loss: 0.3480\n",
      "\n",
      "train-loss: 0.1298, train-acc: 99.2836\n",
      "validation loss: 1.6963, validation acc: 67.6783\n",
      "\n",
      "Epoch 70\n",
      "\n",
      "Epoch [70/100], Step [0/1248], Loss: 0.0031\n",
      "Epoch [70/100], Step [20/1248], Loss: 0.0019\n",
      "Epoch [70/100], Step [40/1248], Loss: 0.0385\n",
      "Epoch [70/100], Step [60/1248], Loss: 0.0942\n",
      "Epoch [70/100], Step [80/1248], Loss: 0.5651\n",
      "Epoch [70/100], Step [100/1248], Loss: 0.0780\n",
      "Epoch [70/100], Step [120/1248], Loss: 0.0271\n",
      "Epoch [70/100], Step [140/1248], Loss: 0.0004\n",
      "Epoch [70/100], Step [160/1248], Loss: 0.0023\n",
      "Epoch [70/100], Step [180/1248], Loss: 0.0142\n",
      "Epoch [70/100], Step [200/1248], Loss: 0.1381\n",
      "Epoch [70/100], Step [220/1248], Loss: 0.0021\n",
      "Epoch [70/100], Step [240/1248], Loss: 0.0017\n",
      "Epoch [70/100], Step [260/1248], Loss: 0.0025\n",
      "Epoch [70/100], Step [280/1248], Loss: 0.0732\n",
      "Epoch [70/100], Step [300/1248], Loss: 0.0112\n",
      "Epoch [70/100], Step [320/1248], Loss: 0.0538\n",
      "Epoch [70/100], Step [340/1248], Loss: 0.0007\n",
      "Epoch [70/100], Step [360/1248], Loss: 0.0367\n",
      "Epoch [70/100], Step [380/1248], Loss: 0.0122\n",
      "Epoch [70/100], Step [400/1248], Loss: 0.0082\n",
      "Epoch [70/100], Step [420/1248], Loss: 0.3546\n",
      "Epoch [70/100], Step [440/1248], Loss: 0.0297\n",
      "Epoch [70/100], Step [460/1248], Loss: 0.0164\n",
      "Epoch [70/100], Step [480/1248], Loss: 0.1430\n",
      "Epoch [70/100], Step [500/1248], Loss: 0.0031\n",
      "Epoch [70/100], Step [520/1248], Loss: 0.0094\n",
      "Epoch [70/100], Step [540/1248], Loss: 0.0011\n",
      "Epoch [70/100], Step [560/1248], Loss: 0.0034\n",
      "Epoch [70/100], Step [580/1248], Loss: 0.0215\n",
      "Epoch [70/100], Step [600/1248], Loss: 0.0004\n",
      "Epoch [70/100], Step [620/1248], Loss: 0.0036\n",
      "Epoch [70/100], Step [640/1248], Loss: 0.0026\n",
      "Epoch [70/100], Step [660/1248], Loss: 0.1026\n",
      "Epoch [70/100], Step [680/1248], Loss: 0.0163\n",
      "Epoch [70/100], Step [700/1248], Loss: 0.0095\n",
      "Epoch [70/100], Step [720/1248], Loss: 0.3969\n",
      "Epoch [70/100], Step [740/1248], Loss: 0.0065\n",
      "Epoch [70/100], Step [760/1248], Loss: 0.0012\n",
      "Epoch [70/100], Step [780/1248], Loss: 0.0034\n",
      "Epoch [70/100], Step [800/1248], Loss: 0.0003\n",
      "Epoch [70/100], Step [820/1248], Loss: 0.0020\n",
      "Epoch [70/100], Step [840/1248], Loss: 0.2548\n",
      "Epoch [70/100], Step [860/1248], Loss: 0.0205\n",
      "Epoch [70/100], Step [880/1248], Loss: 0.0071\n",
      "Epoch [70/100], Step [900/1248], Loss: 0.0005\n",
      "Epoch [70/100], Step [920/1248], Loss: 0.0406\n",
      "Epoch [70/100], Step [940/1248], Loss: 0.0853\n",
      "Epoch [70/100], Step [960/1248], Loss: 0.0326\n",
      "Epoch [70/100], Step [980/1248], Loss: 0.0254\n",
      "Epoch [70/100], Step [1000/1248], Loss: 0.0540\n",
      "Epoch [70/100], Step [1020/1248], Loss: 0.0035\n",
      "Epoch [70/100], Step [1040/1248], Loss: 0.0013\n",
      "Epoch [70/100], Step [1060/1248], Loss: 0.0529\n",
      "Epoch [70/100], Step [1080/1248], Loss: 0.0057\n",
      "Epoch [70/100], Step [1100/1248], Loss: 0.0268\n",
      "Epoch [70/100], Step [1120/1248], Loss: 0.0005\n",
      "Epoch [70/100], Step [1140/1248], Loss: 0.0030\n",
      "Epoch [70/100], Step [1160/1248], Loss: 0.0916\n",
      "Epoch [70/100], Step [1180/1248], Loss: 0.0336\n",
      "Epoch [70/100], Step [1200/1248], Loss: 0.0095\n",
      "Epoch [70/100], Step [1220/1248], Loss: 0.0402\n",
      "Epoch [70/100], Step [1240/1248], Loss: 0.0030\n",
      "\n",
      "train-loss: 0.1285, train-acc: 98.8427\n",
      "validation loss: 1.7007, validation acc: 68.7152\n",
      "\n",
      "Epoch 71\n",
      "\n",
      "Epoch [71/100], Step [0/1248], Loss: 0.0009\n",
      "Epoch [71/100], Step [20/1248], Loss: 0.0041\n",
      "Epoch [71/100], Step [40/1248], Loss: 0.0066\n",
      "Epoch [71/100], Step [60/1248], Loss: 0.0006\n",
      "Epoch [71/100], Step [80/1248], Loss: 0.0061\n",
      "Epoch [71/100], Step [100/1248], Loss: 0.0018\n",
      "Epoch [71/100], Step [120/1248], Loss: 0.0018\n",
      "Epoch [71/100], Step [140/1248], Loss: 0.0004\n",
      "Epoch [71/100], Step [160/1248], Loss: 0.0104\n",
      "Epoch [71/100], Step [180/1248], Loss: 0.0869\n",
      "Epoch [71/100], Step [200/1248], Loss: 0.0045\n",
      "Epoch [71/100], Step [220/1248], Loss: 0.0016\n",
      "Epoch [71/100], Step [240/1248], Loss: 0.0276\n",
      "Epoch [71/100], Step [260/1248], Loss: 0.0001\n",
      "Epoch [71/100], Step [280/1248], Loss: 0.0053\n",
      "Epoch [71/100], Step [300/1248], Loss: 0.0066\n",
      "Epoch [71/100], Step [320/1248], Loss: 0.0032\n",
      "Epoch [71/100], Step [340/1248], Loss: 0.0698\n",
      "Epoch [71/100], Step [360/1248], Loss: 0.0008\n",
      "Epoch [71/100], Step [380/1248], Loss: 0.0003\n",
      "Epoch [71/100], Step [400/1248], Loss: 0.0030\n",
      "Epoch [71/100], Step [420/1248], Loss: 0.0025\n",
      "Epoch [71/100], Step [440/1248], Loss: 0.0005\n",
      "Epoch [71/100], Step [460/1248], Loss: 0.0110\n",
      "Epoch [71/100], Step [480/1248], Loss: 0.0020\n",
      "Epoch [71/100], Step [500/1248], Loss: 0.2204\n",
      "Epoch [71/100], Step [520/1248], Loss: 0.0066\n",
      "Epoch [71/100], Step [540/1248], Loss: 0.0004\n",
      "Epoch [71/100], Step [560/1248], Loss: 0.0051\n",
      "Epoch [71/100], Step [580/1248], Loss: 0.0262\n",
      "Epoch [71/100], Step [600/1248], Loss: 0.0007\n",
      "Epoch [71/100], Step [620/1248], Loss: 0.0025\n",
      "Epoch [71/100], Step [640/1248], Loss: 0.0004\n",
      "Epoch [71/100], Step [660/1248], Loss: 0.0019\n",
      "Epoch [71/100], Step [680/1248], Loss: 0.0184\n",
      "Epoch [71/100], Step [700/1248], Loss: 0.0054\n",
      "Epoch [71/100], Step [720/1248], Loss: 0.0251\n",
      "Epoch [71/100], Step [740/1248], Loss: 0.0008\n",
      "Epoch [71/100], Step [760/1248], Loss: 0.0022\n",
      "Epoch [71/100], Step [780/1248], Loss: 0.0032\n",
      "Epoch [71/100], Step [800/1248], Loss: 0.0030\n",
      "Epoch [71/100], Step [820/1248], Loss: 0.0043\n",
      "Epoch [71/100], Step [840/1248], Loss: 0.2123\n",
      "Epoch [71/100], Step [860/1248], Loss: 0.0012\n",
      "Epoch [71/100], Step [880/1248], Loss: 0.0004\n",
      "Epoch [71/100], Step [900/1248], Loss: 0.0270\n",
      "Epoch [71/100], Step [920/1248], Loss: 0.0038\n",
      "Epoch [71/100], Step [940/1248], Loss: 0.0021\n",
      "Epoch [71/100], Step [960/1248], Loss: 0.0293\n",
      "Epoch [71/100], Step [980/1248], Loss: 0.0035\n",
      "Epoch [71/100], Step [1000/1248], Loss: 0.0003\n",
      "Epoch [71/100], Step [1020/1248], Loss: 0.0186\n",
      "Epoch [71/100], Step [1040/1248], Loss: 0.0028\n",
      "Epoch [71/100], Step [1060/1248], Loss: 0.1782\n",
      "Epoch [71/100], Step [1080/1248], Loss: 0.0012\n",
      "Epoch [71/100], Step [1100/1248], Loss: 0.0258\n",
      "Epoch [71/100], Step [1120/1248], Loss: 0.0007\n",
      "Epoch [71/100], Step [1140/1248], Loss: 0.0055\n",
      "Epoch [71/100], Step [1160/1248], Loss: 0.0099\n",
      "Epoch [71/100], Step [1180/1248], Loss: 0.0003\n",
      "Epoch [71/100], Step [1200/1248], Loss: 0.0031\n",
      "Epoch [71/100], Step [1220/1248], Loss: 0.0031\n",
      "Epoch [71/100], Step [1240/1248], Loss: 0.0516\n",
      "\n",
      "train-loss: 0.1271, train-acc: 99.0381\n",
      "validation loss: 1.7037, validation acc: 68.3991\n",
      "\n",
      "Epoch 72\n",
      "\n",
      "Epoch [72/100], Step [0/1248], Loss: 0.0353\n",
      "Epoch [72/100], Step [20/1248], Loss: 0.0331\n",
      "Epoch [72/100], Step [40/1248], Loss: 0.0317\n",
      "Epoch [72/100], Step [60/1248], Loss: 0.0204\n",
      "Epoch [72/100], Step [80/1248], Loss: 0.0226\n",
      "Epoch [72/100], Step [100/1248], Loss: 0.0012\n",
      "Epoch [72/100], Step [120/1248], Loss: 0.0065\n",
      "Epoch [72/100], Step [140/1248], Loss: 0.0024\n",
      "Epoch [72/100], Step [160/1248], Loss: 0.0331\n",
      "Epoch [72/100], Step [180/1248], Loss: 0.0227\n",
      "Epoch [72/100], Step [200/1248], Loss: 0.0098\n",
      "Epoch [72/100], Step [220/1248], Loss: 0.0001\n",
      "Epoch [72/100], Step [240/1248], Loss: 0.0320\n",
      "Epoch [72/100], Step [260/1248], Loss: 0.0039\n",
      "Epoch [72/100], Step [280/1248], Loss: 0.0054\n",
      "Epoch [72/100], Step [300/1248], Loss: 0.4721\n",
      "Epoch [72/100], Step [320/1248], Loss: 0.0625\n",
      "Epoch [72/100], Step [340/1248], Loss: 0.0014\n",
      "Epoch [72/100], Step [360/1248], Loss: 0.0539\n",
      "Epoch [72/100], Step [380/1248], Loss: 0.0156\n",
      "Epoch [72/100], Step [400/1248], Loss: 0.0003\n",
      "Epoch [72/100], Step [420/1248], Loss: 0.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100], Step [440/1248], Loss: 0.0126\n",
      "Epoch [72/100], Step [460/1248], Loss: 0.0003\n",
      "Epoch [72/100], Step [480/1248], Loss: 0.0037\n",
      "Epoch [72/100], Step [500/1248], Loss: 0.0096\n",
      "Epoch [72/100], Step [520/1248], Loss: 0.0126\n",
      "Epoch [72/100], Step [540/1248], Loss: 0.0158\n",
      "Epoch [72/100], Step [560/1248], Loss: 0.0329\n",
      "Epoch [72/100], Step [580/1248], Loss: 0.0063\n",
      "Epoch [72/100], Step [600/1248], Loss: 0.0803\n",
      "Epoch [72/100], Step [620/1248], Loss: 0.0038\n",
      "Epoch [72/100], Step [640/1248], Loss: 0.0019\n",
      "Epoch [72/100], Step [660/1248], Loss: 0.0022\n",
      "Epoch [72/100], Step [680/1248], Loss: 0.0153\n",
      "Epoch [72/100], Step [700/1248], Loss: 0.0152\n",
      "Epoch [72/100], Step [720/1248], Loss: 0.0020\n",
      "Epoch [72/100], Step [740/1248], Loss: 0.0045\n",
      "Epoch [72/100], Step [760/1248], Loss: 0.0009\n",
      "Epoch [72/100], Step [780/1248], Loss: 0.0011\n",
      "Epoch [72/100], Step [800/1248], Loss: 0.0003\n",
      "Epoch [72/100], Step [820/1248], Loss: 0.0021\n",
      "Epoch [72/100], Step [840/1248], Loss: 0.0106\n",
      "Epoch [72/100], Step [860/1248], Loss: 0.0223\n",
      "Epoch [72/100], Step [880/1248], Loss: 0.0004\n",
      "Epoch [72/100], Step [900/1248], Loss: 0.0017\n",
      "Epoch [72/100], Step [920/1248], Loss: 0.0024\n",
      "Epoch [72/100], Step [940/1248], Loss: 0.0128\n",
      "Epoch [72/100], Step [960/1248], Loss: 0.0024\n",
      "Epoch [72/100], Step [980/1248], Loss: 0.0278\n",
      "Epoch [72/100], Step [1000/1248], Loss: 0.0071\n",
      "Epoch [72/100], Step [1020/1248], Loss: 0.0020\n",
      "Epoch [72/100], Step [1040/1248], Loss: 0.0032\n",
      "Epoch [72/100], Step [1060/1248], Loss: 0.0004\n",
      "Epoch [72/100], Step [1080/1248], Loss: 0.0021\n",
      "Epoch [72/100], Step [1100/1248], Loss: 0.0013\n",
      "Epoch [72/100], Step [1120/1248], Loss: 0.0256\n",
      "Epoch [72/100], Step [1140/1248], Loss: 0.0011\n",
      "Epoch [72/100], Step [1160/1248], Loss: 0.0039\n",
      "Epoch [72/100], Step [1180/1248], Loss: 0.0367\n",
      "Epoch [72/100], Step [1200/1248], Loss: 0.1688\n",
      "Epoch [72/100], Step [1220/1248], Loss: 0.1479\n",
      "Epoch [72/100], Step [1240/1248], Loss: 0.0020\n",
      "\n",
      "train-loss: 0.1257, train-acc: 99.0481\n",
      "validation loss: 1.7111, validation acc: 67.8048\n",
      "\n",
      "Epoch 73\n",
      "\n",
      "Epoch [73/100], Step [0/1248], Loss: 0.0067\n",
      "Epoch [73/100], Step [20/1248], Loss: 0.0001\n",
      "Epoch [73/100], Step [40/1248], Loss: 0.0013\n",
      "Epoch [73/100], Step [60/1248], Loss: 0.0002\n",
      "Epoch [73/100], Step [80/1248], Loss: 0.0205\n",
      "Epoch [73/100], Step [100/1248], Loss: 0.0007\n",
      "Epoch [73/100], Step [120/1248], Loss: 0.0062\n",
      "Epoch [73/100], Step [140/1248], Loss: 0.0308\n",
      "Epoch [73/100], Step [160/1248], Loss: 0.0392\n",
      "Epoch [73/100], Step [180/1248], Loss: 0.0030\n",
      "Epoch [73/100], Step [200/1248], Loss: 0.0076\n",
      "Epoch [73/100], Step [220/1248], Loss: 0.0005\n",
      "Epoch [73/100], Step [240/1248], Loss: 0.0032\n",
      "Epoch [73/100], Step [260/1248], Loss: 0.0005\n",
      "Epoch [73/100], Step [280/1248], Loss: 0.0233\n",
      "Epoch [73/100], Step [300/1248], Loss: 0.0011\n",
      "Epoch [73/100], Step [320/1248], Loss: 0.0602\n",
      "Epoch [73/100], Step [340/1248], Loss: 0.0008\n",
      "Epoch [73/100], Step [360/1248], Loss: 0.0005\n",
      "Epoch [73/100], Step [380/1248], Loss: 0.0575\n",
      "Epoch [73/100], Step [400/1248], Loss: 0.0047\n",
      "Epoch [73/100], Step [420/1248], Loss: 0.0152\n",
      "Epoch [73/100], Step [440/1248], Loss: 0.0020\n",
      "Epoch [73/100], Step [460/1248], Loss: 0.0582\n",
      "Epoch [73/100], Step [480/1248], Loss: 0.0217\n",
      "Epoch [73/100], Step [500/1248], Loss: 0.0049\n",
      "Epoch [73/100], Step [520/1248], Loss: 0.0025\n",
      "Epoch [73/100], Step [540/1248], Loss: 0.0019\n",
      "Epoch [73/100], Step [560/1248], Loss: 0.0004\n",
      "Epoch [73/100], Step [580/1248], Loss: 0.0020\n",
      "Epoch [73/100], Step [600/1248], Loss: 0.0004\n",
      "Epoch [73/100], Step [620/1248], Loss: 0.0055\n",
      "Epoch [73/100], Step [640/1248], Loss: 0.0078\n",
      "Epoch [73/100], Step [660/1248], Loss: 0.0011\n",
      "Epoch [73/100], Step [680/1248], Loss: 0.0013\n",
      "Epoch [73/100], Step [700/1248], Loss: 0.0031\n",
      "Epoch [73/100], Step [720/1248], Loss: 0.1989\n",
      "Epoch [73/100], Step [740/1248], Loss: 0.0323\n",
      "Epoch [73/100], Step [760/1248], Loss: 0.1442\n",
      "Epoch [73/100], Step [780/1248], Loss: 0.0965\n",
      "Epoch [73/100], Step [800/1248], Loss: 0.0192\n",
      "Epoch [73/100], Step [820/1248], Loss: 0.0007\n",
      "Epoch [73/100], Step [840/1248], Loss: 0.0019\n",
      "Epoch [73/100], Step [860/1248], Loss: 0.0063\n",
      "Epoch [73/100], Step [880/1248], Loss: 0.1237\n",
      "Epoch [73/100], Step [900/1248], Loss: 0.0014\n",
      "Epoch [73/100], Step [920/1248], Loss: 0.0001\n",
      "Epoch [73/100], Step [940/1248], Loss: 0.0145\n",
      "Epoch [73/100], Step [960/1248], Loss: 0.0036\n",
      "Epoch [73/100], Step [980/1248], Loss: 0.0026\n",
      "Epoch [73/100], Step [1000/1248], Loss: 0.0243\n",
      "Epoch [73/100], Step [1020/1248], Loss: 0.0445\n",
      "Epoch [73/100], Step [1040/1248], Loss: 0.0012\n",
      "Epoch [73/100], Step [1060/1248], Loss: 0.0097\n",
      "Epoch [73/100], Step [1080/1248], Loss: 0.0139\n",
      "Epoch [73/100], Step [1100/1248], Loss: 0.0421\n",
      "Epoch [73/100], Step [1120/1248], Loss: 0.0016\n",
      "Epoch [73/100], Step [1140/1248], Loss: 0.0016\n",
      "Epoch [73/100], Step [1160/1248], Loss: 0.0041\n",
      "Epoch [73/100], Step [1180/1248], Loss: 0.2359\n",
      "Epoch [73/100], Step [1200/1248], Loss: 0.0008\n",
      "Epoch [73/100], Step [1220/1248], Loss: 0.0006\n",
      "Epoch [73/100], Step [1240/1248], Loss: 0.0658\n",
      "\n",
      "train-loss: 0.1243, train-acc: 99.1583\n",
      "validation loss: 1.7166, validation acc: 67.2484\n",
      "\n",
      "Epoch 74\n",
      "\n",
      "Epoch [74/100], Step [0/1248], Loss: 0.0021\n",
      "Epoch [74/100], Step [20/1248], Loss: 0.0004\n",
      "Epoch [74/100], Step [40/1248], Loss: 0.0008\n",
      "Epoch [74/100], Step [60/1248], Loss: 0.0048\n",
      "Epoch [74/100], Step [80/1248], Loss: 0.0034\n",
      "Epoch [74/100], Step [100/1248], Loss: 0.0022\n",
      "Epoch [74/100], Step [120/1248], Loss: 0.0120\n",
      "Epoch [74/100], Step [140/1248], Loss: 0.0179\n",
      "Epoch [74/100], Step [160/1248], Loss: 0.0011\n",
      "Epoch [74/100], Step [180/1248], Loss: 0.0024\n",
      "Epoch [74/100], Step [200/1248], Loss: 0.0011\n",
      "Epoch [74/100], Step [220/1248], Loss: 0.0028\n",
      "Epoch [74/100], Step [240/1248], Loss: 0.0016\n",
      "Epoch [74/100], Step [260/1248], Loss: 0.0014\n",
      "Epoch [74/100], Step [280/1248], Loss: 0.0158\n",
      "Epoch [74/100], Step [300/1248], Loss: 0.0003\n",
      "Epoch [74/100], Step [320/1248], Loss: 0.0073\n",
      "Epoch [74/100], Step [340/1248], Loss: 0.0003\n",
      "Epoch [74/100], Step [360/1248], Loss: 0.0016\n",
      "Epoch [74/100], Step [380/1248], Loss: 0.0013\n",
      "Epoch [74/100], Step [400/1248], Loss: 0.0073\n",
      "Epoch [74/100], Step [420/1248], Loss: 0.0106\n",
      "Epoch [74/100], Step [440/1248], Loss: 0.0040\n",
      "Epoch [74/100], Step [460/1248], Loss: 0.0374\n",
      "Epoch [74/100], Step [480/1248], Loss: 0.0030\n",
      "Epoch [74/100], Step [500/1248], Loss: 0.0199\n",
      "Epoch [74/100], Step [520/1248], Loss: 0.0369\n",
      "Epoch [74/100], Step [540/1248], Loss: 0.0053\n",
      "Epoch [74/100], Step [560/1248], Loss: 0.0421\n",
      "Epoch [74/100], Step [580/1248], Loss: 0.0007\n",
      "Epoch [74/100], Step [600/1248], Loss: 0.0907\n",
      "Epoch [74/100], Step [620/1248], Loss: 0.0155\n",
      "Epoch [74/100], Step [640/1248], Loss: 0.0183\n",
      "Epoch [74/100], Step [660/1248], Loss: 0.0005\n",
      "Epoch [74/100], Step [680/1248], Loss: 0.0114\n",
      "Epoch [74/100], Step [700/1248], Loss: 0.0004\n",
      "Epoch [74/100], Step [720/1248], Loss: 0.0085\n",
      "Epoch [74/100], Step [740/1248], Loss: 0.0010\n",
      "Epoch [74/100], Step [760/1248], Loss: 0.0034\n",
      "Epoch [74/100], Step [780/1248], Loss: 0.0032\n",
      "Epoch [74/100], Step [800/1248], Loss: 0.0005\n",
      "Epoch [74/100], Step [820/1248], Loss: 0.0006\n",
      "Epoch [74/100], Step [840/1248], Loss: 0.0005\n",
      "Epoch [74/100], Step [860/1248], Loss: 0.0662\n",
      "Epoch [74/100], Step [880/1248], Loss: 0.0006\n",
      "Epoch [74/100], Step [900/1248], Loss: 0.0029\n",
      "Epoch [74/100], Step [920/1248], Loss: 0.0026\n",
      "Epoch [74/100], Step [940/1248], Loss: 0.0013\n",
      "Epoch [74/100], Step [960/1248], Loss: 0.0001\n",
      "Epoch [74/100], Step [980/1248], Loss: 0.0010\n",
      "Epoch [74/100], Step [1000/1248], Loss: 0.0011\n",
      "Epoch [74/100], Step [1020/1248], Loss: 0.0003\n",
      "Epoch [74/100], Step [1040/1248], Loss: 0.0154\n",
      "Epoch [74/100], Step [1060/1248], Loss: 0.0087\n",
      "Epoch [74/100], Step [1080/1248], Loss: 0.0120\n",
      "Epoch [74/100], Step [1100/1248], Loss: 0.0008\n",
      "Epoch [74/100], Step [1120/1248], Loss: 0.0048\n",
      "Epoch [74/100], Step [1140/1248], Loss: 0.0679\n",
      "Epoch [74/100], Step [1160/1248], Loss: 0.0064\n",
      "Epoch [74/100], Step [1180/1248], Loss: 0.0934\n",
      "Epoch [74/100], Step [1200/1248], Loss: 0.0449\n",
      "Epoch [74/100], Step [1220/1248], Loss: 0.0941\n",
      "Epoch [74/100], Step [1240/1248], Loss: 0.0006\n",
      "\n",
      "train-loss: 0.1229, train-acc: 99.2285\n",
      "validation loss: 1.7218, validation acc: 68.0577\n",
      "\n",
      "Epoch 75\n",
      "\n",
      "Epoch [75/100], Step [0/1248], Loss: 0.0073\n",
      "Epoch [75/100], Step [20/1248], Loss: 0.0005\n",
      "Epoch [75/100], Step [40/1248], Loss: 0.0383\n",
      "Epoch [75/100], Step [60/1248], Loss: 0.0007\n",
      "Epoch [75/100], Step [80/1248], Loss: 0.4756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/100], Step [100/1248], Loss: 0.0282\n",
      "Epoch [75/100], Step [120/1248], Loss: 0.0002\n",
      "Epoch [75/100], Step [140/1248], Loss: 0.0026\n",
      "Epoch [75/100], Step [160/1248], Loss: 0.0011\n",
      "Epoch [75/100], Step [180/1248], Loss: 0.0103\n",
      "Epoch [75/100], Step [200/1248], Loss: 0.0025\n",
      "Epoch [75/100], Step [220/1248], Loss: 0.0007\n",
      "Epoch [75/100], Step [240/1248], Loss: 0.0148\n",
      "Epoch [75/100], Step [260/1248], Loss: 0.0001\n",
      "Epoch [75/100], Step [280/1248], Loss: 0.0132\n",
      "Epoch [75/100], Step [300/1248], Loss: 0.0005\n",
      "Epoch [75/100], Step [320/1248], Loss: 0.0076\n",
      "Epoch [75/100], Step [340/1248], Loss: 0.0001\n",
      "Epoch [75/100], Step [360/1248], Loss: 0.0013\n",
      "Epoch [75/100], Step [380/1248], Loss: 0.0012\n",
      "Epoch [75/100], Step [400/1248], Loss: 0.0003\n",
      "Epoch [75/100], Step [420/1248], Loss: 0.0533\n",
      "Epoch [75/100], Step [440/1248], Loss: 0.0039\n",
      "Epoch [75/100], Step [460/1248], Loss: 0.0643\n",
      "Epoch [75/100], Step [480/1248], Loss: 0.0002\n",
      "Epoch [75/100], Step [500/1248], Loss: 0.0140\n",
      "Epoch [75/100], Step [520/1248], Loss: 0.0016\n",
      "Epoch [75/100], Step [540/1248], Loss: 0.0056\n",
      "Epoch [75/100], Step [560/1248], Loss: 0.0017\n",
      "Epoch [75/100], Step [580/1248], Loss: 0.0009\n",
      "Epoch [75/100], Step [600/1248], Loss: 0.0016\n",
      "Epoch [75/100], Step [620/1248], Loss: 0.0056\n",
      "Epoch [75/100], Step [640/1248], Loss: 0.0004\n",
      "Epoch [75/100], Step [660/1248], Loss: 0.0010\n",
      "Epoch [75/100], Step [680/1248], Loss: 0.0023\n",
      "Epoch [75/100], Step [700/1248], Loss: 0.0549\n",
      "Epoch [75/100], Step [720/1248], Loss: 0.0008\n",
      "Epoch [75/100], Step [740/1248], Loss: 0.0496\n",
      "Epoch [75/100], Step [760/1248], Loss: 0.0004\n",
      "Epoch [75/100], Step [780/1248], Loss: 0.2860\n",
      "Epoch [75/100], Step [800/1248], Loss: 0.0708\n",
      "Epoch [75/100], Step [820/1248], Loss: 0.0016\n",
      "Epoch [75/100], Step [840/1248], Loss: 0.0001\n",
      "Epoch [75/100], Step [860/1248], Loss: 0.0013\n",
      "Epoch [75/100], Step [880/1248], Loss: 0.0182\n",
      "Epoch [75/100], Step [900/1248], Loss: 0.0061\n",
      "Epoch [75/100], Step [920/1248], Loss: 0.0082\n",
      "Epoch [75/100], Step [940/1248], Loss: 0.0050\n",
      "Epoch [75/100], Step [960/1248], Loss: 0.0004\n",
      "Epoch [75/100], Step [980/1248], Loss: 0.0059\n",
      "Epoch [75/100], Step [1000/1248], Loss: 0.0146\n",
      "Epoch [75/100], Step [1020/1248], Loss: 0.0631\n",
      "Epoch [75/100], Step [1040/1248], Loss: 0.0001\n",
      "Epoch [75/100], Step [1060/1248], Loss: 0.0250\n",
      "Epoch [75/100], Step [1080/1248], Loss: 0.0015\n",
      "Epoch [75/100], Step [1100/1248], Loss: 0.1485\n",
      "Epoch [75/100], Step [1120/1248], Loss: 0.0041\n",
      "Epoch [75/100], Step [1140/1248], Loss: 0.0001\n",
      "Epoch [75/100], Step [1160/1248], Loss: 0.0005\n",
      "Epoch [75/100], Step [1180/1248], Loss: 0.0132\n",
      "Epoch [75/100], Step [1200/1248], Loss: 0.0062\n",
      "Epoch [75/100], Step [1220/1248], Loss: 0.0010\n",
      "Epoch [75/100], Step [1240/1248], Loss: 0.0023\n",
      "\n",
      "train-loss: 0.1217, train-acc: 99.0581\n",
      "validation loss: 1.7264, validation acc: 68.3738\n",
      "\n",
      "Epoch 76\n",
      "\n",
      "Epoch [76/100], Step [0/1248], Loss: 0.0061\n",
      "Epoch [76/100], Step [20/1248], Loss: 0.0813\n",
      "Epoch [76/100], Step [40/1248], Loss: 0.0022\n",
      "Epoch [76/100], Step [60/1248], Loss: 0.1056\n",
      "Epoch [76/100], Step [80/1248], Loss: 0.0057\n",
      "Epoch [76/100], Step [100/1248], Loss: 0.1181\n",
      "Epoch [76/100], Step [120/1248], Loss: 0.1056\n",
      "Epoch [76/100], Step [140/1248], Loss: 0.0056\n",
      "Epoch [76/100], Step [160/1248], Loss: 0.0082\n",
      "Epoch [76/100], Step [180/1248], Loss: 0.0079\n",
      "Epoch [76/100], Step [200/1248], Loss: 0.0129\n",
      "Epoch [76/100], Step [220/1248], Loss: 0.1105\n",
      "Epoch [76/100], Step [240/1248], Loss: 0.0028\n",
      "Epoch [76/100], Step [260/1248], Loss: 0.1053\n",
      "Epoch [76/100], Step [280/1248], Loss: 0.0182\n",
      "Epoch [76/100], Step [300/1248], Loss: 0.0439\n",
      "Epoch [76/100], Step [320/1248], Loss: 0.0002\n",
      "Epoch [76/100], Step [340/1248], Loss: 0.0092\n",
      "Epoch [76/100], Step [360/1248], Loss: 0.0106\n",
      "Epoch [76/100], Step [380/1248], Loss: 0.0010\n",
      "Epoch [76/100], Step [400/1248], Loss: 0.0012\n",
      "Epoch [76/100], Step [420/1248], Loss: 0.0033\n",
      "Epoch [76/100], Step [440/1248], Loss: 0.0235\n",
      "Epoch [76/100], Step [460/1248], Loss: 0.0012\n",
      "Epoch [76/100], Step [480/1248], Loss: 0.0046\n",
      "Epoch [76/100], Step [500/1248], Loss: 0.0013\n",
      "Epoch [76/100], Step [520/1248], Loss: 0.0575\n",
      "Epoch [76/100], Step [540/1248], Loss: 0.0007\n",
      "Epoch [76/100], Step [560/1248], Loss: 0.0003\n",
      "Epoch [76/100], Step [580/1248], Loss: 0.0024\n",
      "Epoch [76/100], Step [600/1248], Loss: 0.0016\n",
      "Epoch [76/100], Step [620/1248], Loss: 0.0350\n",
      "Epoch [76/100], Step [640/1248], Loss: 0.0239\n",
      "Epoch [76/100], Step [660/1248], Loss: 0.0020\n",
      "Epoch [76/100], Step [680/1248], Loss: 0.0003\n",
      "Epoch [76/100], Step [700/1248], Loss: 0.0002\n",
      "Epoch [76/100], Step [720/1248], Loss: 0.0016\n",
      "Epoch [76/100], Step [740/1248], Loss: 0.0329\n",
      "Epoch [76/100], Step [760/1248], Loss: 0.0072\n",
      "Epoch [76/100], Step [780/1248], Loss: 0.0067\n",
      "Epoch [76/100], Step [800/1248], Loss: 0.0110\n",
      "Epoch [76/100], Step [820/1248], Loss: 0.0037\n",
      "Epoch [76/100], Step [840/1248], Loss: 0.0006\n",
      "Epoch [76/100], Step [860/1248], Loss: 0.0012\n",
      "Epoch [76/100], Step [880/1248], Loss: 0.0002\n",
      "Epoch [76/100], Step [900/1248], Loss: 0.0011\n",
      "Epoch [76/100], Step [920/1248], Loss: 0.0211\n",
      "Epoch [76/100], Step [940/1248], Loss: 0.0003\n",
      "Epoch [76/100], Step [960/1248], Loss: 0.0025\n",
      "Epoch [76/100], Step [980/1248], Loss: 0.0024\n",
      "Epoch [76/100], Step [1000/1248], Loss: 0.0077\n",
      "Epoch [76/100], Step [1020/1248], Loss: 0.0462\n",
      "Epoch [76/100], Step [1040/1248], Loss: 0.0444\n",
      "Epoch [76/100], Step [1060/1248], Loss: 0.0053\n",
      "Epoch [76/100], Step [1080/1248], Loss: 0.0111\n",
      "Epoch [76/100], Step [1100/1248], Loss: 0.2593\n",
      "Epoch [76/100], Step [1120/1248], Loss: 0.0004\n",
      "Epoch [76/100], Step [1140/1248], Loss: 0.0056\n",
      "Epoch [76/100], Step [1160/1248], Loss: 0.0050\n",
      "Epoch [76/100], Step [1180/1248], Loss: 0.0002\n",
      "Epoch [76/100], Step [1200/1248], Loss: 0.0065\n",
      "Epoch [76/100], Step [1220/1248], Loss: 0.2130\n",
      "Epoch [76/100], Step [1240/1248], Loss: 0.0019\n",
      "\n",
      "train-loss: 0.1204, train-acc: 99.0681\n",
      "validation loss: 1.7309, validation acc: 68.4750\n",
      "\n",
      "Epoch 77\n",
      "\n",
      "Epoch [77/100], Step [0/1248], Loss: 0.0237\n",
      "Epoch [77/100], Step [20/1248], Loss: 0.1067\n",
      "Epoch [77/100], Step [40/1248], Loss: 0.0002\n",
      "Epoch [77/100], Step [60/1248], Loss: 0.0003\n",
      "Epoch [77/100], Step [80/1248], Loss: 0.1468\n",
      "Epoch [77/100], Step [100/1248], Loss: 0.0001\n",
      "Epoch [77/100], Step [120/1248], Loss: 0.0530\n",
      "Epoch [77/100], Step [140/1248], Loss: 0.0000\n",
      "Epoch [77/100], Step [160/1248], Loss: 0.0074\n",
      "Epoch [77/100], Step [180/1248], Loss: 0.0126\n",
      "Epoch [77/100], Step [200/1248], Loss: 0.0010\n",
      "Epoch [77/100], Step [220/1248], Loss: 0.0004\n",
      "Epoch [77/100], Step [240/1248], Loss: 0.0007\n",
      "Epoch [77/100], Step [260/1248], Loss: 0.0081\n",
      "Epoch [77/100], Step [280/1248], Loss: 0.0046\n",
      "Epoch [77/100], Step [300/1248], Loss: 0.0039\n",
      "Epoch [77/100], Step [320/1248], Loss: 0.0164\n",
      "Epoch [77/100], Step [340/1248], Loss: 0.0003\n",
      "Epoch [77/100], Step [360/1248], Loss: 0.0014\n",
      "Epoch [77/100], Step [380/1248], Loss: 0.0045\n",
      "Epoch [77/100], Step [400/1248], Loss: 0.0206\n",
      "Epoch [77/100], Step [420/1248], Loss: 0.0003\n",
      "Epoch [77/100], Step [440/1248], Loss: 0.0080\n",
      "Epoch [77/100], Step [460/1248], Loss: 0.0003\n",
      "Epoch [77/100], Step [480/1248], Loss: 0.0253\n",
      "Epoch [77/100], Step [500/1248], Loss: 0.0003\n",
      "Epoch [77/100], Step [520/1248], Loss: 0.0039\n",
      "Epoch [77/100], Step [540/1248], Loss: 0.0002\n",
      "Epoch [77/100], Step [560/1248], Loss: 0.0644\n",
      "Epoch [77/100], Step [580/1248], Loss: 0.0037\n",
      "Epoch [77/100], Step [600/1248], Loss: 0.0053\n",
      "Epoch [77/100], Step [620/1248], Loss: 0.1022\n",
      "Epoch [77/100], Step [640/1248], Loss: 0.0007\n",
      "Epoch [77/100], Step [660/1248], Loss: 0.1363\n",
      "Epoch [77/100], Step [680/1248], Loss: 0.0106\n",
      "Epoch [77/100], Step [700/1248], Loss: 0.0120\n",
      "Epoch [77/100], Step [720/1248], Loss: 0.0001\n",
      "Epoch [77/100], Step [740/1248], Loss: 0.0005\n",
      "Epoch [77/100], Step [760/1248], Loss: 0.0018\n",
      "Epoch [77/100], Step [780/1248], Loss: 0.0001\n",
      "Epoch [77/100], Step [800/1248], Loss: 0.0343\n",
      "Epoch [77/100], Step [820/1248], Loss: 0.0255\n",
      "Epoch [77/100], Step [840/1248], Loss: 0.0007\n",
      "Epoch [77/100], Step [860/1248], Loss: 0.0001\n",
      "Epoch [77/100], Step [880/1248], Loss: 0.0010\n",
      "Epoch [77/100], Step [900/1248], Loss: 0.0020\n",
      "Epoch [77/100], Step [920/1248], Loss: 0.0141\n",
      "Epoch [77/100], Step [940/1248], Loss: 0.0001\n",
      "Epoch [77/100], Step [960/1248], Loss: 0.0197\n",
      "Epoch [77/100], Step [980/1248], Loss: 0.0002\n",
      "Epoch [77/100], Step [1000/1248], Loss: 0.0001\n",
      "Epoch [77/100], Step [1020/1248], Loss: 0.0081\n",
      "Epoch [77/100], Step [1040/1248], Loss: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100], Step [1060/1248], Loss: 0.0011\n",
      "Epoch [77/100], Step [1080/1248], Loss: 0.0017\n",
      "Epoch [77/100], Step [1100/1248], Loss: 0.0008\n",
      "Epoch [77/100], Step [1120/1248], Loss: 0.0007\n",
      "Epoch [77/100], Step [1140/1248], Loss: 0.0122\n",
      "Epoch [77/100], Step [1160/1248], Loss: 0.0024\n",
      "Epoch [77/100], Step [1180/1248], Loss: 0.0056\n",
      "Epoch [77/100], Step [1200/1248], Loss: 0.0006\n",
      "Epoch [77/100], Step [1220/1248], Loss: 0.0033\n",
      "Epoch [77/100], Step [1240/1248], Loss: 0.0006\n",
      "\n",
      "train-loss: 0.1191, train-acc: 99.3687\n",
      "validation loss: 1.7355, validation acc: 68.0956\n",
      "\n",
      "Epoch 78\n",
      "\n",
      "Epoch [78/100], Step [0/1248], Loss: 0.0339\n",
      "Epoch [78/100], Step [20/1248], Loss: 0.5708\n",
      "Epoch [78/100], Step [40/1248], Loss: 0.0215\n",
      "Epoch [78/100], Step [60/1248], Loss: 0.0442\n",
      "Epoch [78/100], Step [80/1248], Loss: 0.0015\n",
      "Epoch [78/100], Step [100/1248], Loss: 0.0019\n",
      "Epoch [78/100], Step [120/1248], Loss: 0.0046\n",
      "Epoch [78/100], Step [140/1248], Loss: 0.0161\n",
      "Epoch [78/100], Step [160/1248], Loss: 0.0057\n",
      "Epoch [78/100], Step [180/1248], Loss: 0.0032\n",
      "Epoch [78/100], Step [200/1248], Loss: 0.0010\n",
      "Epoch [78/100], Step [220/1248], Loss: 0.0060\n",
      "Epoch [78/100], Step [240/1248], Loss: 0.0001\n",
      "Epoch [78/100], Step [260/1248], Loss: 0.0180\n",
      "Epoch [78/100], Step [280/1248], Loss: 0.0004\n",
      "Epoch [78/100], Step [300/1248], Loss: 0.0529\n",
      "Epoch [78/100], Step [320/1248], Loss: 0.0017\n",
      "Epoch [78/100], Step [340/1248], Loss: 0.0002\n",
      "Epoch [78/100], Step [360/1248], Loss: 0.2040\n",
      "Epoch [78/100], Step [380/1248], Loss: 0.0062\n",
      "Epoch [78/100], Step [400/1248], Loss: 0.0002\n",
      "Epoch [78/100], Step [420/1248], Loss: 0.0141\n",
      "Epoch [78/100], Step [440/1248], Loss: 0.0727\n",
      "Epoch [78/100], Step [460/1248], Loss: 0.0029\n",
      "Epoch [78/100], Step [480/1248], Loss: 0.0022\n",
      "Epoch [78/100], Step [500/1248], Loss: 0.0572\n",
      "Epoch [78/100], Step [520/1248], Loss: 0.0056\n",
      "Epoch [78/100], Step [540/1248], Loss: 0.0019\n",
      "Epoch [78/100], Step [560/1248], Loss: 0.0132\n",
      "Epoch [78/100], Step [580/1248], Loss: 0.0025\n",
      "Epoch [78/100], Step [600/1248], Loss: 0.0091\n",
      "Epoch [78/100], Step [620/1248], Loss: 0.0004\n",
      "Epoch [78/100], Step [640/1248], Loss: 0.0106\n",
      "Epoch [78/100], Step [660/1248], Loss: 0.0025\n",
      "Epoch [78/100], Step [680/1248], Loss: 0.0479\n",
      "Epoch [78/100], Step [700/1248], Loss: 0.0020\n",
      "Epoch [78/100], Step [720/1248], Loss: 0.0018\n",
      "Epoch [78/100], Step [740/1248], Loss: 0.0044\n",
      "Epoch [78/100], Step [760/1248], Loss: 0.2544\n",
      "Epoch [78/100], Step [780/1248], Loss: 0.0122\n",
      "Epoch [78/100], Step [800/1248], Loss: 0.0063\n",
      "Epoch [78/100], Step [820/1248], Loss: 0.0205\n",
      "Epoch [78/100], Step [840/1248], Loss: 0.0148\n",
      "Epoch [78/100], Step [860/1248], Loss: 0.0012\n",
      "Epoch [78/100], Step [880/1248], Loss: 0.0967\n",
      "Epoch [78/100], Step [900/1248], Loss: 0.0020\n",
      "Epoch [78/100], Step [920/1248], Loss: 0.0031\n",
      "Epoch [78/100], Step [940/1248], Loss: 0.0034\n",
      "Epoch [78/100], Step [960/1248], Loss: 0.0277\n",
      "Epoch [78/100], Step [980/1248], Loss: 0.0076\n",
      "Epoch [78/100], Step [1000/1248], Loss: 0.0041\n",
      "Epoch [78/100], Step [1020/1248], Loss: 0.0012\n",
      "Epoch [78/100], Step [1040/1248], Loss: 0.0119\n",
      "Epoch [78/100], Step [1060/1248], Loss: 0.0031\n",
      "Epoch [78/100], Step [1080/1248], Loss: 0.0070\n",
      "Epoch [78/100], Step [1100/1248], Loss: 0.1121\n",
      "Epoch [78/100], Step [1120/1248], Loss: 0.0012\n",
      "Epoch [78/100], Step [1140/1248], Loss: 0.0084\n",
      "Epoch [78/100], Step [1160/1248], Loss: 0.0052\n",
      "Epoch [78/100], Step [1180/1248], Loss: 0.0173\n",
      "Epoch [78/100], Step [1200/1248], Loss: 0.0974\n",
      "Epoch [78/100], Step [1220/1248], Loss: 0.0028\n",
      "Epoch [78/100], Step [1240/1248], Loss: 0.0347\n",
      "\n",
      "train-loss: 0.1180, train-acc: 98.8477\n",
      "validation loss: 1.7391, validation acc: 68.1588\n",
      "\n",
      "Epoch 79\n",
      "\n",
      "Epoch [79/100], Step [0/1248], Loss: 0.0014\n",
      "Epoch [79/100], Step [20/1248], Loss: 0.4886\n",
      "Epoch [79/100], Step [40/1248], Loss: 0.0030\n",
      "Epoch [79/100], Step [60/1248], Loss: 0.0033\n",
      "Epoch [79/100], Step [80/1248], Loss: 0.0734\n",
      "Epoch [79/100], Step [100/1248], Loss: 0.0089\n",
      "Epoch [79/100], Step [120/1248], Loss: 0.0081\n",
      "Epoch [79/100], Step [140/1248], Loss: 0.0178\n",
      "Epoch [79/100], Step [160/1248], Loss: 0.0025\n",
      "Epoch [79/100], Step [180/1248], Loss: 0.1146\n",
      "Epoch [79/100], Step [200/1248], Loss: 0.0007\n",
      "Epoch [79/100], Step [220/1248], Loss: 0.0006\n",
      "Epoch [79/100], Step [240/1248], Loss: 0.0154\n",
      "Epoch [79/100], Step [260/1248], Loss: 0.0017\n",
      "Epoch [79/100], Step [280/1248], Loss: 0.0026\n",
      "Epoch [79/100], Step [300/1248], Loss: 0.0014\n",
      "Epoch [79/100], Step [320/1248], Loss: 0.3666\n",
      "Epoch [79/100], Step [340/1248], Loss: 0.0031\n",
      "Epoch [79/100], Step [360/1248], Loss: 0.0007\n",
      "Epoch [79/100], Step [380/1248], Loss: 0.0051\n",
      "Epoch [79/100], Step [400/1248], Loss: 0.0006\n",
      "Epoch [79/100], Step [420/1248], Loss: 0.0094\n",
      "Epoch [79/100], Step [440/1248], Loss: 0.0010\n",
      "Epoch [79/100], Step [460/1248], Loss: 0.0037\n",
      "Epoch [79/100], Step [480/1248], Loss: 0.0022\n",
      "Epoch [79/100], Step [500/1248], Loss: 0.0029\n",
      "Epoch [79/100], Step [520/1248], Loss: 0.0021\n",
      "Epoch [79/100], Step [540/1248], Loss: 0.0013\n",
      "Epoch [79/100], Step [560/1248], Loss: 0.0010\n",
      "Epoch [79/100], Step [580/1248], Loss: 0.0006\n",
      "Epoch [79/100], Step [600/1248], Loss: 0.0014\n",
      "Epoch [79/100], Step [620/1248], Loss: 0.0246\n",
      "Epoch [79/100], Step [640/1248], Loss: 0.0007\n",
      "Epoch [79/100], Step [660/1248], Loss: 0.0039\n",
      "Epoch [79/100], Step [680/1248], Loss: 0.0006\n",
      "Epoch [79/100], Step [700/1248], Loss: 0.0017\n",
      "Epoch [79/100], Step [720/1248], Loss: 0.0066\n",
      "Epoch [79/100], Step [740/1248], Loss: 0.0003\n",
      "Epoch [79/100], Step [760/1248], Loss: 0.0122\n",
      "Epoch [79/100], Step [780/1248], Loss: 0.0018\n",
      "Epoch [79/100], Step [800/1248], Loss: 0.0020\n",
      "Epoch [79/100], Step [820/1248], Loss: 0.0039\n",
      "Epoch [79/100], Step [840/1248], Loss: 0.3678\n",
      "Epoch [79/100], Step [860/1248], Loss: 0.0416\n",
      "Epoch [79/100], Step [880/1248], Loss: 0.0016\n",
      "Epoch [79/100], Step [900/1248], Loss: 0.0529\n",
      "Epoch [79/100], Step [920/1248], Loss: 0.0003\n",
      "Epoch [79/100], Step [940/1248], Loss: 0.0037\n",
      "Epoch [79/100], Step [960/1248], Loss: 0.0201\n",
      "Epoch [79/100], Step [980/1248], Loss: 0.0005\n",
      "Epoch [79/100], Step [1000/1248], Loss: 0.0658\n",
      "Epoch [79/100], Step [1020/1248], Loss: 0.0009\n",
      "Epoch [79/100], Step [1040/1248], Loss: 0.0064\n",
      "Epoch [79/100], Step [1060/1248], Loss: 0.0003\n",
      "Epoch [79/100], Step [1080/1248], Loss: 0.1901\n",
      "Epoch [79/100], Step [1100/1248], Loss: 0.0017\n",
      "Epoch [79/100], Step [1120/1248], Loss: 0.0009\n",
      "Epoch [79/100], Step [1140/1248], Loss: 0.0071\n",
      "Epoch [79/100], Step [1160/1248], Loss: 0.0552\n",
      "Epoch [79/100], Step [1180/1248], Loss: 0.0003\n",
      "Epoch [79/100], Step [1200/1248], Loss: 0.0430\n",
      "Epoch [79/100], Step [1220/1248], Loss: 0.0012\n",
      "Epoch [79/100], Step [1240/1248], Loss: 0.0244\n",
      "\n",
      "train-loss: 0.1168, train-acc: 99.1633\n",
      "validation loss: 1.7429, validation acc: 68.7785\n",
      "\n",
      "Epoch 80\n",
      "\n",
      "Epoch [80/100], Step [0/1248], Loss: 0.0003\n",
      "Epoch [80/100], Step [20/1248], Loss: 0.0930\n",
      "Epoch [80/100], Step [40/1248], Loss: 0.0004\n",
      "Epoch [80/100], Step [60/1248], Loss: 0.0005\n",
      "Epoch [80/100], Step [80/1248], Loss: 0.0009\n",
      "Epoch [80/100], Step [100/1248], Loss: 0.0015\n",
      "Epoch [80/100], Step [120/1248], Loss: 0.0009\n",
      "Epoch [80/100], Step [140/1248], Loss: 0.0405\n",
      "Epoch [80/100], Step [160/1248], Loss: 0.0143\n",
      "Epoch [80/100], Step [180/1248], Loss: 0.0012\n",
      "Epoch [80/100], Step [200/1248], Loss: 0.1197\n",
      "Epoch [80/100], Step [220/1248], Loss: 0.0049\n",
      "Epoch [80/100], Step [240/1248], Loss: 0.0004\n",
      "Epoch [80/100], Step [260/1248], Loss: 0.0016\n",
      "Epoch [80/100], Step [280/1248], Loss: 0.0074\n",
      "Epoch [80/100], Step [300/1248], Loss: 0.0097\n",
      "Epoch [80/100], Step [320/1248], Loss: 0.0010\n",
      "Epoch [80/100], Step [340/1248], Loss: 0.0036\n",
      "Epoch [80/100], Step [360/1248], Loss: 0.0001\n",
      "Epoch [80/100], Step [380/1248], Loss: 0.0004\n",
      "Epoch [80/100], Step [400/1248], Loss: 0.0004\n",
      "Epoch [80/100], Step [420/1248], Loss: 0.0343\n",
      "Epoch [80/100], Step [440/1248], Loss: 0.0113\n",
      "Epoch [80/100], Step [460/1248], Loss: 0.0009\n",
      "Epoch [80/100], Step [480/1248], Loss: 0.0004\n",
      "Epoch [80/100], Step [500/1248], Loss: 0.1423\n",
      "Epoch [80/100], Step [520/1248], Loss: 0.0050\n",
      "Epoch [80/100], Step [540/1248], Loss: 0.0002\n",
      "Epoch [80/100], Step [560/1248], Loss: 0.1200\n",
      "Epoch [80/100], Step [580/1248], Loss: 0.0376\n",
      "Epoch [80/100], Step [600/1248], Loss: 0.0059\n",
      "Epoch [80/100], Step [620/1248], Loss: 0.0020\n",
      "Epoch [80/100], Step [640/1248], Loss: 0.0004\n",
      "Epoch [80/100], Step [660/1248], Loss: 0.0028\n",
      "Epoch [80/100], Step [680/1248], Loss: 0.0021\n",
      "Epoch [80/100], Step [700/1248], Loss: 0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Step [720/1248], Loss: 0.0003\n",
      "Epoch [80/100], Step [740/1248], Loss: 0.0006\n",
      "Epoch [80/100], Step [760/1248], Loss: 0.2613\n",
      "Epoch [80/100], Step [780/1248], Loss: 0.0603\n",
      "Epoch [80/100], Step [800/1248], Loss: 0.0009\n",
      "Epoch [80/100], Step [820/1248], Loss: 0.0009\n",
      "Epoch [80/100], Step [840/1248], Loss: 0.0201\n",
      "Epoch [80/100], Step [860/1248], Loss: 0.0902\n",
      "Epoch [80/100], Step [880/1248], Loss: 0.0424\n",
      "Epoch [80/100], Step [900/1248], Loss: 0.0029\n",
      "Epoch [80/100], Step [920/1248], Loss: 0.0005\n",
      "Epoch [80/100], Step [940/1248], Loss: 0.0009\n",
      "Epoch [80/100], Step [960/1248], Loss: 0.0011\n",
      "Epoch [80/100], Step [980/1248], Loss: 0.0004\n",
      "Epoch [80/100], Step [1000/1248], Loss: 0.0001\n",
      "Epoch [80/100], Step [1020/1248], Loss: 0.0011\n",
      "Epoch [80/100], Step [1040/1248], Loss: 0.0003\n",
      "Epoch [80/100], Step [1060/1248], Loss: 0.0002\n",
      "Epoch [80/100], Step [1080/1248], Loss: 0.0008\n",
      "Epoch [80/100], Step [1100/1248], Loss: 0.0003\n",
      "Epoch [80/100], Step [1120/1248], Loss: 0.0136\n",
      "Epoch [80/100], Step [1140/1248], Loss: 0.0062\n",
      "Epoch [80/100], Step [1160/1248], Loss: 0.0015\n",
      "Epoch [80/100], Step [1180/1248], Loss: 0.0073\n",
      "Epoch [80/100], Step [1200/1248], Loss: 0.0037\n",
      "Epoch [80/100], Step [1220/1248], Loss: 0.0032\n",
      "Epoch [80/100], Step [1240/1248], Loss: 0.0002\n",
      "\n",
      "train-loss: 0.1156, train-acc: 99.3888\n",
      "validation loss: 1.7476, validation acc: 68.7911\n",
      "\n",
      "Epoch 81\n",
      "\n",
      "Epoch [81/100], Step [0/1248], Loss: 0.0006\n",
      "Epoch [81/100], Step [20/1248], Loss: 0.0060\n",
      "Epoch [81/100], Step [40/1248], Loss: 0.0001\n",
      "Epoch [81/100], Step [60/1248], Loss: 0.0002\n",
      "Epoch [81/100], Step [80/1248], Loss: 0.0005\n",
      "Epoch [81/100], Step [100/1248], Loss: 0.0006\n",
      "Epoch [81/100], Step [120/1248], Loss: 0.0013\n",
      "Epoch [81/100], Step [140/1248], Loss: 0.0015\n",
      "Epoch [81/100], Step [160/1248], Loss: 0.0188\n",
      "Epoch [81/100], Step [180/1248], Loss: 0.0080\n",
      "Epoch [81/100], Step [200/1248], Loss: 0.0005\n",
      "Epoch [81/100], Step [220/1248], Loss: 0.0127\n",
      "Epoch [81/100], Step [240/1248], Loss: 0.0011\n",
      "Epoch [81/100], Step [260/1248], Loss: 0.0018\n",
      "Epoch [81/100], Step [280/1248], Loss: 0.0037\n",
      "Epoch [81/100], Step [300/1248], Loss: 0.0064\n",
      "Epoch [81/100], Step [320/1248], Loss: 0.0002\n",
      "Epoch [81/100], Step [340/1248], Loss: 0.0001\n",
      "Epoch [81/100], Step [360/1248], Loss: 0.1092\n",
      "Epoch [81/100], Step [380/1248], Loss: 0.0001\n",
      "Epoch [81/100], Step [400/1248], Loss: 0.0000\n",
      "Epoch [81/100], Step [420/1248], Loss: 0.0018\n",
      "Epoch [81/100], Step [440/1248], Loss: 0.0008\n",
      "Epoch [81/100], Step [460/1248], Loss: 0.0003\n",
      "Epoch [81/100], Step [480/1248], Loss: 0.0258\n",
      "Epoch [81/100], Step [500/1248], Loss: 0.0005\n",
      "Epoch [81/100], Step [520/1248], Loss: 0.0005\n",
      "Epoch [81/100], Step [540/1248], Loss: 0.0005\n",
      "Epoch [81/100], Step [560/1248], Loss: 0.0002\n",
      "Epoch [81/100], Step [580/1248], Loss: 0.1230\n",
      "Epoch [81/100], Step [600/1248], Loss: 0.0063\n",
      "Epoch [81/100], Step [620/1248], Loss: 0.3052\n",
      "Epoch [81/100], Step [640/1248], Loss: 0.0006\n",
      "Epoch [81/100], Step [660/1248], Loss: 0.0052\n",
      "Epoch [81/100], Step [680/1248], Loss: 0.0001\n",
      "Epoch [81/100], Step [700/1248], Loss: 0.0009\n",
      "Epoch [81/100], Step [720/1248], Loss: 0.0001\n",
      "Epoch [81/100], Step [740/1248], Loss: 0.0258\n",
      "Epoch [81/100], Step [760/1248], Loss: 0.0006\n",
      "Epoch [81/100], Step [780/1248], Loss: 0.0015\n",
      "Epoch [81/100], Step [800/1248], Loss: 0.0745\n",
      "Epoch [81/100], Step [820/1248], Loss: 0.0485\n",
      "Epoch [81/100], Step [840/1248], Loss: 0.0540\n",
      "Epoch [81/100], Step [860/1248], Loss: 0.0656\n",
      "Epoch [81/100], Step [880/1248], Loss: 0.0119\n",
      "Epoch [81/100], Step [900/1248], Loss: 0.0414\n",
      "Epoch [81/100], Step [920/1248], Loss: 0.0186\n",
      "Epoch [81/100], Step [940/1248], Loss: 0.0002\n",
      "Epoch [81/100], Step [960/1248], Loss: 0.0001\n",
      "Epoch [81/100], Step [980/1248], Loss: 0.0049\n",
      "Epoch [81/100], Step [1000/1248], Loss: 0.1079\n",
      "Epoch [81/100], Step [1020/1248], Loss: 0.0009\n",
      "Epoch [81/100], Step [1040/1248], Loss: 0.0025\n",
      "Epoch [81/100], Step [1060/1248], Loss: 0.0004\n",
      "Epoch [81/100], Step [1080/1248], Loss: 0.0597\n",
      "Epoch [81/100], Step [1100/1248], Loss: 0.9447\n",
      "Epoch [81/100], Step [1120/1248], Loss: 0.0174\n",
      "Epoch [81/100], Step [1140/1248], Loss: 0.0125\n",
      "Epoch [81/100], Step [1160/1248], Loss: 0.0283\n",
      "Epoch [81/100], Step [1180/1248], Loss: 0.0081\n",
      "Epoch [81/100], Step [1200/1248], Loss: 0.0061\n",
      "Epoch [81/100], Step [1220/1248], Loss: 0.1939\n",
      "Epoch [81/100], Step [1240/1248], Loss: 0.0006\n",
      "\n",
      "train-loss: 0.1145, train-acc: 99.0531\n",
      "validation loss: 1.7528, validation acc: 68.1715\n",
      "\n",
      "Epoch 82\n",
      "\n",
      "Epoch [82/100], Step [0/1248], Loss: 0.0006\n",
      "Epoch [82/100], Step [20/1248], Loss: 0.0465\n",
      "Epoch [82/100], Step [40/1248], Loss: 0.0017\n",
      "Epoch [82/100], Step [60/1248], Loss: 0.0007\n",
      "Epoch [82/100], Step [80/1248], Loss: 0.2275\n",
      "Epoch [82/100], Step [100/1248], Loss: 0.0082\n",
      "Epoch [82/100], Step [120/1248], Loss: 0.0064\n",
      "Epoch [82/100], Step [140/1248], Loss: 0.0362\n",
      "Epoch [82/100], Step [160/1248], Loss: 0.0010\n",
      "Epoch [82/100], Step [180/1248], Loss: 0.0039\n",
      "Epoch [82/100], Step [200/1248], Loss: 0.0083\n",
      "Epoch [82/100], Step [220/1248], Loss: 0.0003\n",
      "Epoch [82/100], Step [240/1248], Loss: 0.0028\n",
      "Epoch [82/100], Step [260/1248], Loss: 0.0020\n",
      "Epoch [82/100], Step [280/1248], Loss: 0.0010\n",
      "Epoch [82/100], Step [300/1248], Loss: 0.0240\n",
      "Epoch [82/100], Step [320/1248], Loss: 0.0031\n",
      "Epoch [82/100], Step [340/1248], Loss: 0.0015\n",
      "Epoch [82/100], Step [360/1248], Loss: 0.1155\n",
      "Epoch [82/100], Step [380/1248], Loss: 0.0041\n",
      "Epoch [82/100], Step [400/1248], Loss: 0.3621\n",
      "Epoch [82/100], Step [420/1248], Loss: 0.0011\n",
      "Epoch [82/100], Step [440/1248], Loss: 0.0020\n",
      "Epoch [82/100], Step [460/1248], Loss: 0.0097\n",
      "Epoch [82/100], Step [480/1248], Loss: 0.0003\n",
      "Epoch [82/100], Step [500/1248], Loss: 0.0055\n",
      "Epoch [82/100], Step [520/1248], Loss: 0.0019\n",
      "Epoch [82/100], Step [540/1248], Loss: 0.0397\n",
      "Epoch [82/100], Step [560/1248], Loss: 0.0013\n",
      "Epoch [82/100], Step [580/1248], Loss: 0.0391\n",
      "Epoch [82/100], Step [600/1248], Loss: 0.0101\n",
      "Epoch [82/100], Step [620/1248], Loss: 0.0047\n",
      "Epoch [82/100], Step [640/1248], Loss: 0.0037\n",
      "Epoch [82/100], Step [660/1248], Loss: 0.0069\n",
      "Epoch [82/100], Step [680/1248], Loss: 0.0017\n",
      "Epoch [82/100], Step [700/1248], Loss: 0.0414\n",
      "Epoch [82/100], Step [720/1248], Loss: 0.0039\n",
      "Epoch [82/100], Step [740/1248], Loss: 0.0556\n",
      "Epoch [82/100], Step [760/1248], Loss: 0.0035\n",
      "Epoch [82/100], Step [780/1248], Loss: 0.0312\n",
      "Epoch [82/100], Step [800/1248], Loss: 0.0030\n",
      "Epoch [82/100], Step [820/1248], Loss: 0.0027\n",
      "Epoch [82/100], Step [840/1248], Loss: 0.0120\n",
      "Epoch [82/100], Step [860/1248], Loss: 0.0007\n",
      "Epoch [82/100], Step [880/1248], Loss: 0.0001\n",
      "Epoch [82/100], Step [900/1248], Loss: 0.0015\n",
      "Epoch [82/100], Step [920/1248], Loss: 0.0003\n",
      "Epoch [82/100], Step [940/1248], Loss: 0.0062\n",
      "Epoch [82/100], Step [960/1248], Loss: 0.0115\n",
      "Epoch [82/100], Step [980/1248], Loss: 0.0395\n",
      "Epoch [82/100], Step [1000/1248], Loss: 0.0016\n",
      "Epoch [82/100], Step [1020/1248], Loss: 0.0001\n",
      "Epoch [82/100], Step [1040/1248], Loss: 0.0044\n",
      "Epoch [82/100], Step [1060/1248], Loss: 0.0367\n",
      "Epoch [82/100], Step [1080/1248], Loss: 0.0017\n",
      "Epoch [82/100], Step [1100/1248], Loss: 0.0095\n",
      "Epoch [82/100], Step [1120/1248], Loss: 0.0027\n",
      "Epoch [82/100], Step [1140/1248], Loss: 0.0021\n",
      "Epoch [82/100], Step [1160/1248], Loss: 0.0017\n",
      "Epoch [82/100], Step [1180/1248], Loss: 0.0019\n",
      "Epoch [82/100], Step [1200/1248], Loss: 0.0013\n",
      "Epoch [82/100], Step [1220/1248], Loss: 0.0047\n",
      "Epoch [82/100], Step [1240/1248], Loss: 0.1226\n",
      "\n",
      "train-loss: 0.1134, train-acc: 99.0481\n",
      "validation loss: 1.7580, validation acc: 67.3242\n",
      "\n",
      "Epoch 83\n",
      "\n",
      "Epoch [83/100], Step [0/1248], Loss: 0.0202\n",
      "Epoch [83/100], Step [20/1248], Loss: 0.1406\n",
      "Epoch [83/100], Step [40/1248], Loss: 0.0008\n",
      "Epoch [83/100], Step [60/1248], Loss: 0.0025\n",
      "Epoch [83/100], Step [80/1248], Loss: 0.0163\n",
      "Epoch [83/100], Step [100/1248], Loss: 0.0040\n",
      "Epoch [83/100], Step [120/1248], Loss: 0.0003\n",
      "Epoch [83/100], Step [140/1248], Loss: 0.0170\n",
      "Epoch [83/100], Step [160/1248], Loss: 0.0001\n",
      "Epoch [83/100], Step [180/1248], Loss: 0.0011\n",
      "Epoch [83/100], Step [200/1248], Loss: 0.0030\n",
      "Epoch [83/100], Step [220/1248], Loss: 0.0002\n",
      "Epoch [83/100], Step [240/1248], Loss: 0.0203\n",
      "Epoch [83/100], Step [260/1248], Loss: 0.0006\n",
      "Epoch [83/100], Step [280/1248], Loss: 0.0010\n",
      "Epoch [83/100], Step [300/1248], Loss: 0.0018\n",
      "Epoch [83/100], Step [320/1248], Loss: 0.0070\n",
      "Epoch [83/100], Step [340/1248], Loss: 0.0005\n",
      "Epoch [83/100], Step [360/1248], Loss: 0.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/100], Step [380/1248], Loss: 0.0003\n",
      "Epoch [83/100], Step [400/1248], Loss: 0.0002\n",
      "Epoch [83/100], Step [420/1248], Loss: 0.0008\n",
      "Epoch [83/100], Step [440/1248], Loss: 0.0002\n",
      "Epoch [83/100], Step [460/1248], Loss: 0.0027\n",
      "Epoch [83/100], Step [480/1248], Loss: 0.0004\n",
      "Epoch [83/100], Step [500/1248], Loss: 0.0005\n",
      "Epoch [83/100], Step [520/1248], Loss: 0.0015\n",
      "Epoch [83/100], Step [540/1248], Loss: 0.0006\n",
      "Epoch [83/100], Step [560/1248], Loss: 0.0403\n",
      "Epoch [83/100], Step [580/1248], Loss: 0.0071\n",
      "Epoch [83/100], Step [600/1248], Loss: 0.0070\n",
      "Epoch [83/100], Step [620/1248], Loss: 0.0021\n",
      "Epoch [83/100], Step [640/1248], Loss: 0.0292\n",
      "Epoch [83/100], Step [660/1248], Loss: 0.0007\n",
      "Epoch [83/100], Step [680/1248], Loss: 0.0005\n",
      "Epoch [83/100], Step [700/1248], Loss: 0.0012\n",
      "Epoch [83/100], Step [720/1248], Loss: 0.0073\n",
      "Epoch [83/100], Step [740/1248], Loss: 0.0051\n",
      "Epoch [83/100], Step [760/1248], Loss: 0.0021\n",
      "Epoch [83/100], Step [780/1248], Loss: 0.0002\n",
      "Epoch [83/100], Step [800/1248], Loss: 0.0002\n",
      "Epoch [83/100], Step [820/1248], Loss: 0.0003\n",
      "Epoch [83/100], Step [840/1248], Loss: 0.0004\n",
      "Epoch [83/100], Step [860/1248], Loss: 0.0035\n",
      "Epoch [83/100], Step [880/1248], Loss: 0.0064\n",
      "Epoch [83/100], Step [900/1248], Loss: 0.0028\n",
      "Epoch [83/100], Step [920/1248], Loss: 0.0001\n",
      "Epoch [83/100], Step [940/1248], Loss: 0.0079\n",
      "Epoch [83/100], Step [960/1248], Loss: 0.0009\n",
      "Epoch [83/100], Step [980/1248], Loss: 0.0117\n",
      "Epoch [83/100], Step [1000/1248], Loss: 0.0178\n",
      "Epoch [83/100], Step [1020/1248], Loss: 0.0072\n",
      "Epoch [83/100], Step [1040/1248], Loss: 0.1387\n",
      "Epoch [83/100], Step [1060/1248], Loss: 0.0010\n",
      "Epoch [83/100], Step [1080/1248], Loss: 0.0183\n",
      "Epoch [83/100], Step [1100/1248], Loss: 0.0015\n",
      "Epoch [83/100], Step [1120/1248], Loss: 0.0713\n",
      "Epoch [83/100], Step [1140/1248], Loss: 0.0008\n",
      "Epoch [83/100], Step [1160/1248], Loss: 0.0060\n",
      "Epoch [83/100], Step [1180/1248], Loss: 0.0008\n",
      "Epoch [83/100], Step [1200/1248], Loss: 0.0063\n",
      "Epoch [83/100], Step [1220/1248], Loss: 0.0015\n",
      "Epoch [83/100], Step [1240/1248], Loss: 0.0827\n",
      "\n",
      "train-loss: 0.1123, train-acc: 99.4689\n",
      "validation loss: 1.7631, validation acc: 68.1968\n",
      "\n",
      "Epoch 84\n",
      "\n",
      "Epoch [84/100], Step [0/1248], Loss: 0.0904\n",
      "Epoch [84/100], Step [20/1248], Loss: 0.0002\n",
      "Epoch [84/100], Step [40/1248], Loss: 0.0015\n",
      "Epoch [84/100], Step [60/1248], Loss: 0.1178\n",
      "Epoch [84/100], Step [80/1248], Loss: 0.0422\n",
      "Epoch [84/100], Step [100/1248], Loss: 0.0004\n",
      "Epoch [84/100], Step [120/1248], Loss: 0.0000\n",
      "Epoch [84/100], Step [140/1248], Loss: 0.0030\n",
      "Epoch [84/100], Step [160/1248], Loss: 0.0006\n",
      "Epoch [84/100], Step [180/1248], Loss: 0.0011\n",
      "Epoch [84/100], Step [200/1248], Loss: 0.0071\n",
      "Epoch [84/100], Step [220/1248], Loss: 0.0005\n",
      "Epoch [84/100], Step [240/1248], Loss: 0.0009\n",
      "Epoch [84/100], Step [260/1248], Loss: 0.0739\n",
      "Epoch [84/100], Step [280/1248], Loss: 0.1039\n",
      "Epoch [84/100], Step [300/1248], Loss: 0.0001\n",
      "Epoch [84/100], Step [320/1248], Loss: 0.0045\n",
      "Epoch [84/100], Step [340/1248], Loss: 0.0019\n",
      "Epoch [84/100], Step [360/1248], Loss: 0.0003\n",
      "Epoch [84/100], Step [380/1248], Loss: 0.0046\n",
      "Epoch [84/100], Step [400/1248], Loss: 0.0002\n",
      "Epoch [84/100], Step [420/1248], Loss: 0.0110\n",
      "Epoch [84/100], Step [440/1248], Loss: 0.0001\n",
      "Epoch [84/100], Step [460/1248], Loss: 0.0474\n",
      "Epoch [84/100], Step [480/1248], Loss: 0.0006\n",
      "Epoch [84/100], Step [500/1248], Loss: 0.0014\n",
      "Epoch [84/100], Step [520/1248], Loss: 0.0029\n",
      "Epoch [84/100], Step [540/1248], Loss: 0.0070\n",
      "Epoch [84/100], Step [560/1248], Loss: 0.0012\n",
      "Epoch [84/100], Step [580/1248], Loss: 0.0049\n",
      "Epoch [84/100], Step [600/1248], Loss: 0.0024\n",
      "Epoch [84/100], Step [620/1248], Loss: 0.0073\n",
      "Epoch [84/100], Step [640/1248], Loss: 0.0020\n",
      "Epoch [84/100], Step [660/1248], Loss: 0.4921\n",
      "Epoch [84/100], Step [680/1248], Loss: 0.5662\n",
      "Epoch [84/100], Step [700/1248], Loss: 0.0177\n",
      "Epoch [84/100], Step [720/1248], Loss: 0.0347\n",
      "Epoch [84/100], Step [740/1248], Loss: 0.0009\n",
      "Epoch [84/100], Step [760/1248], Loss: 0.1296\n",
      "Epoch [84/100], Step [780/1248], Loss: 0.1717\n",
      "Epoch [84/100], Step [800/1248], Loss: 0.0014\n",
      "Epoch [84/100], Step [820/1248], Loss: 0.0350\n",
      "Epoch [84/100], Step [840/1248], Loss: 0.0012\n",
      "Epoch [84/100], Step [860/1248], Loss: 0.0068\n",
      "Epoch [84/100], Step [880/1248], Loss: 0.0027\n",
      "Epoch [84/100], Step [900/1248], Loss: 0.0259\n",
      "Epoch [84/100], Step [920/1248], Loss: 0.0044\n",
      "Epoch [84/100], Step [940/1248], Loss: 0.0217\n",
      "Epoch [84/100], Step [960/1248], Loss: 0.0066\n",
      "Epoch [84/100], Step [980/1248], Loss: 0.0062\n",
      "Epoch [84/100], Step [1000/1248], Loss: 0.0010\n",
      "Epoch [84/100], Step [1020/1248], Loss: 0.0164\n",
      "Epoch [84/100], Step [1040/1248], Loss: 0.0010\n",
      "Epoch [84/100], Step [1060/1248], Loss: 0.0010\n",
      "Epoch [84/100], Step [1080/1248], Loss: 0.0005\n",
      "Epoch [84/100], Step [1100/1248], Loss: 0.0819\n",
      "Epoch [84/100], Step [1120/1248], Loss: 0.0325\n",
      "Epoch [84/100], Step [1140/1248], Loss: 0.0353\n",
      "Epoch [84/100], Step [1160/1248], Loss: 0.0003\n",
      "Epoch [84/100], Step [1180/1248], Loss: 0.0025\n",
      "Epoch [84/100], Step [1200/1248], Loss: 0.0025\n",
      "Epoch [84/100], Step [1220/1248], Loss: 0.0012\n",
      "Epoch [84/100], Step [1240/1248], Loss: 0.0205\n",
      "\n",
      "train-loss: 0.1113, train-acc: 98.9780\n",
      "validation loss: 1.7665, validation acc: 67.9691\n",
      "\n",
      "Epoch 85\n",
      "\n",
      "Epoch [85/100], Step [0/1248], Loss: 0.0078\n",
      "Epoch [85/100], Step [20/1248], Loss: 0.0027\n",
      "Epoch [85/100], Step [40/1248], Loss: 0.0132\n",
      "Epoch [85/100], Step [60/1248], Loss: 0.0096\n",
      "Epoch [85/100], Step [80/1248], Loss: 0.0007\n",
      "Epoch [85/100], Step [100/1248], Loss: 0.0207\n",
      "Epoch [85/100], Step [120/1248], Loss: 0.0013\n",
      "Epoch [85/100], Step [140/1248], Loss: 0.0564\n",
      "Epoch [85/100], Step [160/1248], Loss: 0.0025\n",
      "Epoch [85/100], Step [180/1248], Loss: 0.0084\n",
      "Epoch [85/100], Step [200/1248], Loss: 0.0008\n",
      "Epoch [85/100], Step [220/1248], Loss: 0.0043\n",
      "Epoch [85/100], Step [240/1248], Loss: 0.0021\n",
      "Epoch [85/100], Step [260/1248], Loss: 0.0002\n",
      "Epoch [85/100], Step [280/1248], Loss: 0.0016\n",
      "Epoch [85/100], Step [300/1248], Loss: 0.0005\n",
      "Epoch [85/100], Step [320/1248], Loss: 0.0000\n",
      "Epoch [85/100], Step [340/1248], Loss: 0.0058\n",
      "Epoch [85/100], Step [360/1248], Loss: 0.0003\n",
      "Epoch [85/100], Step [380/1248], Loss: 0.0009\n",
      "Epoch [85/100], Step [400/1248], Loss: 0.0002\n",
      "Epoch [85/100], Step [420/1248], Loss: 0.0006\n",
      "Epoch [85/100], Step [440/1248], Loss: 0.0023\n",
      "Epoch [85/100], Step [460/1248], Loss: 0.0005\n",
      "Epoch [85/100], Step [480/1248], Loss: 0.0016\n",
      "Epoch [85/100], Step [500/1248], Loss: 0.0019\n",
      "Epoch [85/100], Step [520/1248], Loss: 0.0022\n",
      "Epoch [85/100], Step [540/1248], Loss: 0.0010\n",
      "Epoch [85/100], Step [560/1248], Loss: 0.0374\n",
      "Epoch [85/100], Step [580/1248], Loss: 0.0028\n",
      "Epoch [85/100], Step [600/1248], Loss: 0.0011\n",
      "Epoch [85/100], Step [620/1248], Loss: 0.0146\n",
      "Epoch [85/100], Step [640/1248], Loss: 0.0025\n",
      "Epoch [85/100], Step [660/1248], Loss: 0.0045\n",
      "Epoch [85/100], Step [680/1248], Loss: 0.0039\n",
      "Epoch [85/100], Step [700/1248], Loss: 0.0001\n",
      "Epoch [85/100], Step [720/1248], Loss: 0.0028\n",
      "Epoch [85/100], Step [740/1248], Loss: 0.1039\n",
      "Epoch [85/100], Step [760/1248], Loss: 0.0029\n",
      "Epoch [85/100], Step [780/1248], Loss: 0.0737\n",
      "Epoch [85/100], Step [800/1248], Loss: 0.0002\n",
      "Epoch [85/100], Step [820/1248], Loss: 0.0015\n",
      "Epoch [85/100], Step [840/1248], Loss: 0.0237\n",
      "Epoch [85/100], Step [860/1248], Loss: 0.0422\n",
      "Epoch [85/100], Step [880/1248], Loss: 0.0156\n",
      "Epoch [85/100], Step [900/1248], Loss: 0.0011\n",
      "Epoch [85/100], Step [920/1248], Loss: 0.0018\n",
      "Epoch [85/100], Step [940/1248], Loss: 0.0017\n",
      "Epoch [85/100], Step [960/1248], Loss: 0.0010\n",
      "Epoch [85/100], Step [980/1248], Loss: 0.0212\n",
      "Epoch [85/100], Step [1000/1248], Loss: 0.0011\n",
      "Epoch [85/100], Step [1020/1248], Loss: 0.1089\n",
      "Epoch [85/100], Step [1040/1248], Loss: 0.0012\n",
      "Epoch [85/100], Step [1060/1248], Loss: 0.0012\n",
      "Epoch [85/100], Step [1080/1248], Loss: 0.0078\n",
      "Epoch [85/100], Step [1100/1248], Loss: 0.0016\n",
      "Epoch [85/100], Step [1120/1248], Loss: 0.0003\n",
      "Epoch [85/100], Step [1140/1248], Loss: 0.0775\n",
      "Epoch [85/100], Step [1160/1248], Loss: 0.0000\n",
      "Epoch [85/100], Step [1180/1248], Loss: 0.0066\n",
      "Epoch [85/100], Step [1200/1248], Loss: 0.0013\n",
      "Epoch [85/100], Step [1220/1248], Loss: 0.0002\n",
      "Epoch [85/100], Step [1240/1248], Loss: 0.0035\n",
      "\n",
      "train-loss: 0.1102, train-acc: 99.2936\n",
      "validation loss: 1.7720, validation acc: 68.1462\n",
      "\n",
      "Epoch 86\n",
      "\n",
      "Epoch [86/100], Step [0/1248], Loss: 0.0254\n",
      "Epoch [86/100], Step [20/1248], Loss: 0.0043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100], Step [40/1248], Loss: 0.0002\n",
      "Epoch [86/100], Step [60/1248], Loss: 0.0361\n",
      "Epoch [86/100], Step [80/1248], Loss: 0.0000\n",
      "Epoch [86/100], Step [100/1248], Loss: 0.0019\n",
      "Epoch [86/100], Step [120/1248], Loss: 0.0037\n",
      "Epoch [86/100], Step [140/1248], Loss: 0.0002\n",
      "Epoch [86/100], Step [160/1248], Loss: 0.1166\n",
      "Epoch [86/100], Step [180/1248], Loss: 0.1647\n",
      "Epoch [86/100], Step [200/1248], Loss: 0.0008\n",
      "Epoch [86/100], Step [220/1248], Loss: 0.0031\n",
      "Epoch [86/100], Step [240/1248], Loss: 0.0019\n",
      "Epoch [86/100], Step [260/1248], Loss: 0.0531\n",
      "Epoch [86/100], Step [280/1248], Loss: 0.0171\n",
      "Epoch [86/100], Step [300/1248], Loss: 0.0003\n",
      "Epoch [86/100], Step [320/1248], Loss: 0.0001\n",
      "Epoch [86/100], Step [340/1248], Loss: 0.0791\n",
      "Epoch [86/100], Step [360/1248], Loss: 0.0428\n",
      "Epoch [86/100], Step [380/1248], Loss: 0.0253\n",
      "Epoch [86/100], Step [400/1248], Loss: 0.0987\n",
      "Epoch [86/100], Step [420/1248], Loss: 0.0012\n",
      "Epoch [86/100], Step [440/1248], Loss: 0.0004\n",
      "Epoch [86/100], Step [460/1248], Loss: 0.0034\n",
      "Epoch [86/100], Step [480/1248], Loss: 0.0214\n",
      "Epoch [86/100], Step [500/1248], Loss: 0.0010\n",
      "Epoch [86/100], Step [520/1248], Loss: 0.0007\n",
      "Epoch [86/100], Step [540/1248], Loss: 0.0001\n",
      "Epoch [86/100], Step [560/1248], Loss: 0.5702\n",
      "Epoch [86/100], Step [580/1248], Loss: 0.0085\n",
      "Epoch [86/100], Step [600/1248], Loss: 0.0034\n",
      "Epoch [86/100], Step [620/1248], Loss: 0.1766\n",
      "Epoch [86/100], Step [640/1248], Loss: 0.0003\n",
      "Epoch [86/100], Step [660/1248], Loss: 0.0705\n",
      "Epoch [86/100], Step [680/1248], Loss: 0.0563\n",
      "Epoch [86/100], Step [700/1248], Loss: 0.0053\n",
      "Epoch [86/100], Step [720/1248], Loss: 0.0002\n",
      "Epoch [86/100], Step [740/1248], Loss: 0.0001\n",
      "Epoch [86/100], Step [760/1248], Loss: 0.0001\n",
      "Epoch [86/100], Step [780/1248], Loss: 0.0020\n",
      "Epoch [86/100], Step [800/1248], Loss: 0.0084\n",
      "Epoch [86/100], Step [820/1248], Loss: 0.0172\n",
      "Epoch [86/100], Step [840/1248], Loss: 0.0114\n",
      "Epoch [86/100], Step [860/1248], Loss: 0.0077\n",
      "Epoch [86/100], Step [880/1248], Loss: 0.0003\n",
      "Epoch [86/100], Step [900/1248], Loss: 0.0058\n",
      "Epoch [86/100], Step [920/1248], Loss: 0.0047\n",
      "Epoch [86/100], Step [940/1248], Loss: 0.0031\n",
      "Epoch [86/100], Step [960/1248], Loss: 0.0002\n",
      "Epoch [86/100], Step [980/1248], Loss: 0.0003\n",
      "Epoch [86/100], Step [1000/1248], Loss: 0.0001\n",
      "Epoch [86/100], Step [1020/1248], Loss: 0.0225\n",
      "Epoch [86/100], Step [1040/1248], Loss: 0.0077\n",
      "Epoch [86/100], Step [1060/1248], Loss: 0.0022\n",
      "Epoch [86/100], Step [1080/1248], Loss: 0.0876\n",
      "Epoch [86/100], Step [1100/1248], Loss: 0.0113\n",
      "Epoch [86/100], Step [1120/1248], Loss: 0.0386\n",
      "Epoch [86/100], Step [1140/1248], Loss: 0.0042\n",
      "Epoch [86/100], Step [1160/1248], Loss: 0.0316\n",
      "Epoch [86/100], Step [1180/1248], Loss: 0.0270\n",
      "Epoch [86/100], Step [1200/1248], Loss: 0.0004\n",
      "Epoch [86/100], Step [1220/1248], Loss: 0.0462\n",
      "Epoch [86/100], Step [1240/1248], Loss: 0.0642\n",
      "\n",
      "train-loss: 0.1092, train-acc: 99.1483\n",
      "validation loss: 1.7744, validation acc: 68.8037\n",
      "\n",
      "Epoch 87\n",
      "\n",
      "Epoch [87/100], Step [0/1248], Loss: 0.0146\n",
      "Epoch [87/100], Step [20/1248], Loss: 0.0025\n",
      "Epoch [87/100], Step [40/1248], Loss: 0.0745\n",
      "Epoch [87/100], Step [60/1248], Loss: 0.0001\n",
      "Epoch [87/100], Step [80/1248], Loss: 0.0004\n",
      "Epoch [87/100], Step [100/1248], Loss: 0.0178\n",
      "Epoch [87/100], Step [120/1248], Loss: 0.0003\n",
      "Epoch [87/100], Step [140/1248], Loss: 0.0028\n",
      "Epoch [87/100], Step [160/1248], Loss: 0.0009\n",
      "Epoch [87/100], Step [180/1248], Loss: 0.0034\n",
      "Epoch [87/100], Step [200/1248], Loss: 0.1824\n",
      "Epoch [87/100], Step [220/1248], Loss: 0.0013\n",
      "Epoch [87/100], Step [240/1248], Loss: 0.0089\n",
      "Epoch [87/100], Step [260/1248], Loss: 0.0014\n",
      "Epoch [87/100], Step [280/1248], Loss: 0.0002\n",
      "Epoch [87/100], Step [300/1248], Loss: 0.0003\n",
      "Epoch [87/100], Step [320/1248], Loss: 0.0014\n",
      "Epoch [87/100], Step [340/1248], Loss: 0.0051\n",
      "Epoch [87/100], Step [360/1248], Loss: 0.0100\n",
      "Epoch [87/100], Step [380/1248], Loss: 0.0008\n",
      "Epoch [87/100], Step [400/1248], Loss: 0.0002\n",
      "Epoch [87/100], Step [420/1248], Loss: 0.0001\n",
      "Epoch [87/100], Step [440/1248], Loss: 0.0001\n",
      "Epoch [87/100], Step [460/1248], Loss: 0.0054\n",
      "Epoch [87/100], Step [480/1248], Loss: 0.0000\n",
      "Epoch [87/100], Step [500/1248], Loss: 0.0011\n",
      "Epoch [87/100], Step [520/1248], Loss: 0.0029\n",
      "Epoch [87/100], Step [540/1248], Loss: 0.0007\n",
      "Epoch [87/100], Step [560/1248], Loss: 0.0002\n",
      "Epoch [87/100], Step [580/1248], Loss: 0.1533\n",
      "Epoch [87/100], Step [600/1248], Loss: 0.0279\n",
      "Epoch [87/100], Step [620/1248], Loss: 0.0025\n",
      "Epoch [87/100], Step [640/1248], Loss: 0.0021\n",
      "Epoch [87/100], Step [660/1248], Loss: 0.0007\n",
      "Epoch [87/100], Step [680/1248], Loss: 0.0865\n",
      "Epoch [87/100], Step [700/1248], Loss: 0.0015\n",
      "Epoch [87/100], Step [720/1248], Loss: 0.0002\n",
      "Epoch [87/100], Step [740/1248], Loss: 0.0013\n",
      "Epoch [87/100], Step [760/1248], Loss: 0.0004\n",
      "Epoch [87/100], Step [780/1248], Loss: 0.0015\n",
      "Epoch [87/100], Step [800/1248], Loss: 0.0023\n",
      "Epoch [87/100], Step [820/1248], Loss: 0.0005\n",
      "Epoch [87/100], Step [840/1248], Loss: 0.0001\n",
      "Epoch [87/100], Step [860/1248], Loss: 0.0016\n",
      "Epoch [87/100], Step [880/1248], Loss: 0.0079\n",
      "Epoch [87/100], Step [900/1248], Loss: 0.0001\n",
      "Epoch [87/100], Step [920/1248], Loss: 0.0006\n",
      "Epoch [87/100], Step [940/1248], Loss: 0.0003\n",
      "Epoch [87/100], Step [960/1248], Loss: 0.0025\n",
      "Epoch [87/100], Step [980/1248], Loss: 0.0046\n",
      "Epoch [87/100], Step [1000/1248], Loss: 0.0016\n",
      "Epoch [87/100], Step [1020/1248], Loss: 0.0112\n",
      "Epoch [87/100], Step [1040/1248], Loss: 0.0005\n",
      "Epoch [87/100], Step [1060/1248], Loss: 0.1942\n",
      "Epoch [87/100], Step [1080/1248], Loss: 0.0015\n",
      "Epoch [87/100], Step [1100/1248], Loss: 0.0040\n",
      "Epoch [87/100], Step [1120/1248], Loss: 0.1142\n",
      "Epoch [87/100], Step [1140/1248], Loss: 0.0002\n",
      "Epoch [87/100], Step [1160/1248], Loss: 0.0050\n",
      "Epoch [87/100], Step [1180/1248], Loss: 0.0052\n",
      "Epoch [87/100], Step [1200/1248], Loss: 0.0142\n",
      "Epoch [87/100], Step [1220/1248], Loss: 0.0084\n",
      "Epoch [87/100], Step [1240/1248], Loss: 0.0032\n",
      "\n",
      "train-loss: 0.1082, train-acc: 99.4238\n",
      "validation loss: 1.7795, validation acc: 67.9059\n",
      "\n",
      "Epoch 88\n",
      "\n",
      "Epoch [88/100], Step [0/1248], Loss: 0.0015\n",
      "Epoch [88/100], Step [20/1248], Loss: 0.1715\n",
      "Epoch [88/100], Step [40/1248], Loss: 0.0063\n",
      "Epoch [88/100], Step [60/1248], Loss: 0.0543\n",
      "Epoch [88/100], Step [80/1248], Loss: 0.0074\n",
      "Epoch [88/100], Step [100/1248], Loss: 0.0002\n",
      "Epoch [88/100], Step [120/1248], Loss: 0.0002\n",
      "Epoch [88/100], Step [140/1248], Loss: 0.0210\n",
      "Epoch [88/100], Step [160/1248], Loss: 0.0008\n",
      "Epoch [88/100], Step [180/1248], Loss: 0.0003\n",
      "Epoch [88/100], Step [200/1248], Loss: 0.0006\n",
      "Epoch [88/100], Step [220/1248], Loss: 0.0040\n",
      "Epoch [88/100], Step [240/1248], Loss: 0.0017\n",
      "Epoch [88/100], Step [260/1248], Loss: 0.0012\n",
      "Epoch [88/100], Step [280/1248], Loss: 0.0016\n",
      "Epoch [88/100], Step [300/1248], Loss: 0.0017\n",
      "Epoch [88/100], Step [320/1248], Loss: 0.0010\n",
      "Epoch [88/100], Step [340/1248], Loss: 0.0011\n",
      "Epoch [88/100], Step [360/1248], Loss: 0.0003\n",
      "Epoch [88/100], Step [380/1248], Loss: 0.0009\n",
      "Epoch [88/100], Step [400/1248], Loss: 0.0147\n",
      "Epoch [88/100], Step [420/1248], Loss: 0.0011\n",
      "Epoch [88/100], Step [440/1248], Loss: 0.0034\n",
      "Epoch [88/100], Step [460/1248], Loss: 0.0001\n",
      "Epoch [88/100], Step [480/1248], Loss: 0.0252\n",
      "Epoch [88/100], Step [500/1248], Loss: 0.1015\n",
      "Epoch [88/100], Step [520/1248], Loss: 0.0015\n",
      "Epoch [88/100], Step [540/1248], Loss: 0.0006\n",
      "Epoch [88/100], Step [560/1248], Loss: 0.0002\n",
      "Epoch [88/100], Step [580/1248], Loss: 0.0015\n",
      "Epoch [88/100], Step [600/1248], Loss: 0.0020\n",
      "Epoch [88/100], Step [620/1248], Loss: 0.0021\n",
      "Epoch [88/100], Step [640/1248], Loss: 0.0030\n",
      "Epoch [88/100], Step [660/1248], Loss: 0.0002\n",
      "Epoch [88/100], Step [680/1248], Loss: 0.0336\n",
      "Epoch [88/100], Step [700/1248], Loss: 0.0084\n",
      "Epoch [88/100], Step [720/1248], Loss: 0.0006\n",
      "Epoch [88/100], Step [740/1248], Loss: 0.0340\n",
      "Epoch [88/100], Step [760/1248], Loss: 0.0015\n",
      "Epoch [88/100], Step [780/1248], Loss: 0.0075\n",
      "Epoch [88/100], Step [800/1248], Loss: 0.0653\n",
      "Epoch [88/100], Step [820/1248], Loss: 0.0020\n",
      "Epoch [88/100], Step [840/1248], Loss: 0.0032\n",
      "Epoch [88/100], Step [860/1248], Loss: 0.0002\n",
      "Epoch [88/100], Step [880/1248], Loss: 0.0005\n",
      "Epoch [88/100], Step [900/1248], Loss: 0.0007\n",
      "Epoch [88/100], Step [920/1248], Loss: 0.0605\n",
      "Epoch [88/100], Step [940/1248], Loss: 0.0002\n",
      "Epoch [88/100], Step [960/1248], Loss: 0.0029\n",
      "Epoch [88/100], Step [980/1248], Loss: 0.0045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100], Step [1000/1248], Loss: 0.0018\n",
      "Epoch [88/100], Step [1020/1248], Loss: 0.0006\n",
      "Epoch [88/100], Step [1040/1248], Loss: 0.0009\n",
      "Epoch [88/100], Step [1060/1248], Loss: 0.0013\n",
      "Epoch [88/100], Step [1080/1248], Loss: 0.0042\n",
      "Epoch [88/100], Step [1100/1248], Loss: 0.0798\n",
      "Epoch [88/100], Step [1120/1248], Loss: 0.0246\n",
      "Epoch [88/100], Step [1140/1248], Loss: 0.0004\n",
      "Epoch [88/100], Step [1160/1248], Loss: 0.0022\n",
      "Epoch [88/100], Step [1180/1248], Loss: 0.0043\n",
      "Epoch [88/100], Step [1200/1248], Loss: 0.1120\n",
      "Epoch [88/100], Step [1220/1248], Loss: 0.2025\n",
      "Epoch [88/100], Step [1240/1248], Loss: 0.0004\n",
      "\n",
      "train-loss: 0.1073, train-acc: 99.0080\n",
      "validation loss: 1.7844, validation acc: 67.6277\n",
      "\n",
      "Epoch 89\n",
      "\n",
      "Epoch [89/100], Step [0/1248], Loss: 0.0053\n",
      "Epoch [89/100], Step [20/1248], Loss: 0.0492\n",
      "Epoch [89/100], Step [40/1248], Loss: 0.1032\n",
      "Epoch [89/100], Step [60/1248], Loss: 0.0004\n",
      "Epoch [89/100], Step [80/1248], Loss: 0.0004\n",
      "Epoch [89/100], Step [100/1248], Loss: 0.0070\n",
      "Epoch [89/100], Step [120/1248], Loss: 0.0038\n",
      "Epoch [89/100], Step [140/1248], Loss: 0.0217\n",
      "Epoch [89/100], Step [160/1248], Loss: 0.0016\n",
      "Epoch [89/100], Step [180/1248], Loss: 0.0027\n",
      "Epoch [89/100], Step [200/1248], Loss: 0.1957\n",
      "Epoch [89/100], Step [220/1248], Loss: 0.0037\n",
      "Epoch [89/100], Step [240/1248], Loss: 0.0008\n",
      "Epoch [89/100], Step [260/1248], Loss: 0.0052\n",
      "Epoch [89/100], Step [280/1248], Loss: 0.0003\n",
      "Epoch [89/100], Step [300/1248], Loss: 0.0016\n",
      "Epoch [89/100], Step [320/1248], Loss: 0.0008\n",
      "Epoch [89/100], Step [340/1248], Loss: 0.0001\n",
      "Epoch [89/100], Step [360/1248], Loss: 0.0018\n",
      "Epoch [89/100], Step [380/1248], Loss: 0.0002\n",
      "Epoch [89/100], Step [400/1248], Loss: 0.0003\n",
      "Epoch [89/100], Step [420/1248], Loss: 0.0023\n",
      "Epoch [89/100], Step [440/1248], Loss: 0.0338\n",
      "Epoch [89/100], Step [460/1248], Loss: 0.0026\n",
      "Epoch [89/100], Step [480/1248], Loss: 0.0043\n",
      "Epoch [89/100], Step [500/1248], Loss: 0.0003\n",
      "Epoch [89/100], Step [520/1248], Loss: 0.0003\n",
      "Epoch [89/100], Step [540/1248], Loss: 0.0026\n",
      "Epoch [89/100], Step [560/1248], Loss: 0.0099\n",
      "Epoch [89/100], Step [580/1248], Loss: 0.0005\n",
      "Epoch [89/100], Step [600/1248], Loss: 0.0001\n",
      "Epoch [89/100], Step [620/1248], Loss: 0.0001\n",
      "Epoch [89/100], Step [640/1248], Loss: 0.0002\n",
      "Epoch [89/100], Step [660/1248], Loss: 0.1312\n",
      "Epoch [89/100], Step [680/1248], Loss: 0.0005\n",
      "Epoch [89/100], Step [700/1248], Loss: 0.0182\n",
      "Epoch [89/100], Step [720/1248], Loss: 0.0064\n",
      "Epoch [89/100], Step [740/1248], Loss: 0.0070\n",
      "Epoch [89/100], Step [760/1248], Loss: 0.0111\n",
      "Epoch [89/100], Step [780/1248], Loss: 0.0044\n",
      "Epoch [89/100], Step [800/1248], Loss: 0.0018\n",
      "Epoch [89/100], Step [820/1248], Loss: 0.0009\n",
      "Epoch [89/100], Step [840/1248], Loss: 0.0032\n",
      "Epoch [89/100], Step [860/1248], Loss: 0.0003\n",
      "Epoch [89/100], Step [880/1248], Loss: 0.0016\n",
      "Epoch [89/100], Step [900/1248], Loss: 0.0009\n",
      "Epoch [89/100], Step [920/1248], Loss: 0.0616\n",
      "Epoch [89/100], Step [940/1248], Loss: 0.0118\n",
      "Epoch [89/100], Step [960/1248], Loss: 0.0055\n",
      "Epoch [89/100], Step [980/1248], Loss: 0.0087\n",
      "Epoch [89/100], Step [1000/1248], Loss: 0.0015\n",
      "Epoch [89/100], Step [1020/1248], Loss: 0.0074\n",
      "Epoch [89/100], Step [1040/1248], Loss: 0.0010\n",
      "Epoch [89/100], Step [1060/1248], Loss: 0.0008\n",
      "Epoch [89/100], Step [1080/1248], Loss: 0.0005\n",
      "Epoch [89/100], Step [1100/1248], Loss: 0.0017\n",
      "Epoch [89/100], Step [1120/1248], Loss: 0.0004\n",
      "Epoch [89/100], Step [1140/1248], Loss: 0.0116\n",
      "Epoch [89/100], Step [1160/1248], Loss: 0.3113\n",
      "Epoch [89/100], Step [1180/1248], Loss: 0.1165\n",
      "Epoch [89/100], Step [1200/1248], Loss: 0.0066\n",
      "Epoch [89/100], Step [1220/1248], Loss: 0.0003\n",
      "Epoch [89/100], Step [1240/1248], Loss: 1.7141\n",
      "\n",
      "train-loss: 0.1063, train-acc: 99.4088\n",
      "validation loss: 1.7875, validation acc: 68.3738\n",
      "\n",
      "Epoch 90\n",
      "\n",
      "Epoch [90/100], Step [0/1248], Loss: 0.0003\n",
      "Epoch [90/100], Step [20/1248], Loss: 0.0006\n",
      "Epoch [90/100], Step [40/1248], Loss: 0.0103\n",
      "Epoch [90/100], Step [60/1248], Loss: 0.0074\n",
      "Epoch [90/100], Step [80/1248], Loss: 0.0048\n",
      "Epoch [90/100], Step [100/1248], Loss: 0.0003\n",
      "Epoch [90/100], Step [120/1248], Loss: 0.0003\n",
      "Epoch [90/100], Step [140/1248], Loss: 0.0006\n",
      "Epoch [90/100], Step [160/1248], Loss: 0.0004\n",
      "Epoch [90/100], Step [180/1248], Loss: 0.0032\n",
      "Epoch [90/100], Step [200/1248], Loss: 0.0009\n",
      "Epoch [90/100], Step [220/1248], Loss: 0.0033\n",
      "Epoch [90/100], Step [240/1248], Loss: 0.0010\n",
      "Epoch [90/100], Step [260/1248], Loss: 0.0267\n",
      "Epoch [90/100], Step [280/1248], Loss: 0.0067\n",
      "Epoch [90/100], Step [300/1248], Loss: 0.0017\n",
      "Epoch [90/100], Step [320/1248], Loss: 0.0024\n",
      "Epoch [90/100], Step [340/1248], Loss: 0.0005\n",
      "Epoch [90/100], Step [360/1248], Loss: 0.0084\n",
      "Epoch [90/100], Step [380/1248], Loss: 0.0028\n",
      "Epoch [90/100], Step [400/1248], Loss: 0.0205\n",
      "Epoch [90/100], Step [420/1248], Loss: 0.0220\n",
      "Epoch [90/100], Step [440/1248], Loss: 0.0002\n",
      "Epoch [90/100], Step [460/1248], Loss: 0.0043\n",
      "Epoch [90/100], Step [480/1248], Loss: 0.0063\n",
      "Epoch [90/100], Step [500/1248], Loss: 0.0227\n",
      "Epoch [90/100], Step [520/1248], Loss: 0.0073\n",
      "Epoch [90/100], Step [540/1248], Loss: 0.0015\n",
      "Epoch [90/100], Step [560/1248], Loss: 0.0004\n",
      "Epoch [90/100], Step [580/1248], Loss: 0.0080\n",
      "Epoch [90/100], Step [600/1248], Loss: 0.0638\n",
      "Epoch [90/100], Step [620/1248], Loss: 0.0050\n",
      "Epoch [90/100], Step [640/1248], Loss: 0.0015\n",
      "Epoch [90/100], Step [660/1248], Loss: 0.0004\n",
      "Epoch [90/100], Step [680/1248], Loss: 0.0003\n",
      "Epoch [90/100], Step [700/1248], Loss: 0.0008\n",
      "Epoch [90/100], Step [720/1248], Loss: 0.0059\n",
      "Epoch [90/100], Step [740/1248], Loss: 0.0148\n",
      "Epoch [90/100], Step [760/1248], Loss: 0.0014\n",
      "Epoch [90/100], Step [780/1248], Loss: 0.2788\n",
      "Epoch [90/100], Step [800/1248], Loss: 0.0005\n",
      "Epoch [90/100], Step [820/1248], Loss: 0.0003\n",
      "Epoch [90/100], Step [840/1248], Loss: 0.0071\n",
      "Epoch [90/100], Step [860/1248], Loss: 0.0457\n",
      "Epoch [90/100], Step [880/1248], Loss: 0.0771\n",
      "Epoch [90/100], Step [900/1248], Loss: 0.0003\n",
      "Epoch [90/100], Step [920/1248], Loss: 0.0000\n",
      "Epoch [90/100], Step [940/1248], Loss: 0.0020\n",
      "Epoch [90/100], Step [960/1248], Loss: 0.0722\n",
      "Epoch [90/100], Step [980/1248], Loss: 0.0396\n",
      "Epoch [90/100], Step [1000/1248], Loss: 0.0405\n",
      "Epoch [90/100], Step [1020/1248], Loss: 0.0809\n",
      "Epoch [90/100], Step [1040/1248], Loss: 0.0057\n",
      "Epoch [90/100], Step [1060/1248], Loss: 0.0627\n",
      "Epoch [90/100], Step [1080/1248], Loss: 0.0027\n",
      "Epoch [90/100], Step [1100/1248], Loss: 0.0073\n",
      "Epoch [90/100], Step [1120/1248], Loss: 0.0061\n",
      "Epoch [90/100], Step [1140/1248], Loss: 0.1764\n",
      "Epoch [90/100], Step [1160/1248], Loss: 0.1183\n",
      "Epoch [90/100], Step [1180/1248], Loss: 0.0030\n",
      "Epoch [90/100], Step [1200/1248], Loss: 0.2180\n",
      "Epoch [90/100], Step [1220/1248], Loss: 0.0000\n",
      "Epoch [90/100], Step [1240/1248], Loss: 0.0024\n",
      "\n",
      "train-loss: 0.1053, train-acc: 99.2184\n",
      "validation loss: 1.7901, validation acc: 67.6909\n",
      "\n",
      "Epoch 91\n",
      "\n",
      "Epoch [91/100], Step [0/1248], Loss: 0.0033\n",
      "Epoch [91/100], Step [20/1248], Loss: 0.0002\n",
      "Epoch [91/100], Step [40/1248], Loss: 0.0012\n",
      "Epoch [91/100], Step [60/1248], Loss: 0.0018\n",
      "Epoch [91/100], Step [80/1248], Loss: 0.1314\n",
      "Epoch [91/100], Step [100/1248], Loss: 0.0001\n",
      "Epoch [91/100], Step [120/1248], Loss: 0.0017\n",
      "Epoch [91/100], Step [140/1248], Loss: 0.0013\n",
      "Epoch [91/100], Step [160/1248], Loss: 0.0005\n",
      "Epoch [91/100], Step [180/1248], Loss: 0.0012\n",
      "Epoch [91/100], Step [200/1248], Loss: 0.0284\n",
      "Epoch [91/100], Step [220/1248], Loss: 0.0005\n",
      "Epoch [91/100], Step [240/1248], Loss: 0.0002\n",
      "Epoch [91/100], Step [260/1248], Loss: 0.0014\n",
      "Epoch [91/100], Step [280/1248], Loss: 0.0003\n",
      "Epoch [91/100], Step [300/1248], Loss: 0.0001\n",
      "Epoch [91/100], Step [320/1248], Loss: 0.0093\n",
      "Epoch [91/100], Step [340/1248], Loss: 0.0003\n",
      "Epoch [91/100], Step [360/1248], Loss: 0.0001\n",
      "Epoch [91/100], Step [380/1248], Loss: 0.0008\n",
      "Epoch [91/100], Step [400/1248], Loss: 0.0168\n",
      "Epoch [91/100], Step [420/1248], Loss: 0.0267\n",
      "Epoch [91/100], Step [440/1248], Loss: 0.0006\n",
      "Epoch [91/100], Step [460/1248], Loss: 0.0291\n",
      "Epoch [91/100], Step [480/1248], Loss: 0.1701\n",
      "Epoch [91/100], Step [500/1248], Loss: 0.0017\n",
      "Epoch [91/100], Step [520/1248], Loss: 0.0171\n",
      "Epoch [91/100], Step [540/1248], Loss: 0.1061\n",
      "Epoch [91/100], Step [560/1248], Loss: 0.0009\n",
      "Epoch [91/100], Step [580/1248], Loss: 0.0726\n",
      "Epoch [91/100], Step [600/1248], Loss: 0.0009\n",
      "Epoch [91/100], Step [620/1248], Loss: 0.0323\n",
      "Epoch [91/100], Step [640/1248], Loss: 0.0045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100], Step [660/1248], Loss: 0.0007\n",
      "Epoch [91/100], Step [680/1248], Loss: 0.0004\n",
      "Epoch [91/100], Step [700/1248], Loss: 0.0014\n",
      "Epoch [91/100], Step [720/1248], Loss: 0.0020\n",
      "Epoch [91/100], Step [740/1248], Loss: 0.0006\n",
      "Epoch [91/100], Step [760/1248], Loss: 0.0195\n",
      "Epoch [91/100], Step [780/1248], Loss: 0.0002\n",
      "Epoch [91/100], Step [800/1248], Loss: 0.0005\n",
      "Epoch [91/100], Step [820/1248], Loss: 0.0409\n",
      "Epoch [91/100], Step [840/1248], Loss: 0.0722\n",
      "Epoch [91/100], Step [860/1248], Loss: 1.3626\n",
      "Epoch [91/100], Step [880/1248], Loss: 0.0034\n",
      "Epoch [91/100], Step [900/1248], Loss: 0.0000\n",
      "Epoch [91/100], Step [920/1248], Loss: 0.0007\n",
      "Epoch [91/100], Step [940/1248], Loss: 0.0005\n",
      "Epoch [91/100], Step [960/1248], Loss: 0.0027\n",
      "Epoch [91/100], Step [980/1248], Loss: 0.0020\n",
      "Epoch [91/100], Step [1000/1248], Loss: 0.1578\n",
      "Epoch [91/100], Step [1020/1248], Loss: 0.0006\n",
      "Epoch [91/100], Step [1040/1248], Loss: 0.0014\n",
      "Epoch [91/100], Step [1060/1248], Loss: 0.1398\n",
      "Epoch [91/100], Step [1080/1248], Loss: 0.0005\n",
      "Epoch [91/100], Step [1100/1248], Loss: 0.1921\n",
      "Epoch [91/100], Step [1120/1248], Loss: 0.0006\n",
      "Epoch [91/100], Step [1140/1248], Loss: 0.0004\n",
      "Epoch [91/100], Step [1160/1248], Loss: 0.0016\n",
      "Epoch [91/100], Step [1180/1248], Loss: 0.0047\n",
      "Epoch [91/100], Step [1200/1248], Loss: 0.0102\n",
      "Epoch [91/100], Step [1220/1248], Loss: 0.0002\n",
      "Epoch [91/100], Step [1240/1248], Loss: 0.0183\n",
      "\n",
      "train-loss: 0.1044, train-acc: 99.3587\n",
      "validation loss: 1.7952, validation acc: 68.1462\n",
      "\n",
      "Epoch 92\n",
      "\n",
      "Epoch [92/100], Step [0/1248], Loss: 0.0319\n",
      "Epoch [92/100], Step [20/1248], Loss: 0.0001\n",
      "Epoch [92/100], Step [40/1248], Loss: 0.0015\n",
      "Epoch [92/100], Step [60/1248], Loss: 0.0142\n",
      "Epoch [92/100], Step [80/1248], Loss: 0.0003\n",
      "Epoch [92/100], Step [100/1248], Loss: 0.0230\n",
      "Epoch [92/100], Step [120/1248], Loss: 0.0001\n",
      "Epoch [92/100], Step [140/1248], Loss: 0.0036\n",
      "Epoch [92/100], Step [160/1248], Loss: 0.0002\n",
      "Epoch [92/100], Step [180/1248], Loss: 0.0049\n",
      "Epoch [92/100], Step [200/1248], Loss: 0.0002\n",
      "Epoch [92/100], Step [220/1248], Loss: 0.0066\n",
      "Epoch [92/100], Step [240/1248], Loss: 0.0005\n",
      "Epoch [92/100], Step [260/1248], Loss: 0.0002\n",
      "Epoch [92/100], Step [280/1248], Loss: 0.0008\n",
      "Epoch [92/100], Step [300/1248], Loss: 0.0006\n",
      "Epoch [92/100], Step [320/1248], Loss: 0.0002\n",
      "Epoch [92/100], Step [340/1248], Loss: 0.0669\n",
      "Epoch [92/100], Step [360/1248], Loss: 0.0049\n",
      "Epoch [92/100], Step [380/1248], Loss: 0.0003\n",
      "Epoch [92/100], Step [400/1248], Loss: 0.0004\n",
      "Epoch [92/100], Step [420/1248], Loss: 0.0002\n",
      "Epoch [92/100], Step [440/1248], Loss: 0.0004\n",
      "Epoch [92/100], Step [460/1248], Loss: 0.0018\n",
      "Epoch [92/100], Step [480/1248], Loss: 0.0006\n",
      "Epoch [92/100], Step [500/1248], Loss: 0.0026\n",
      "Epoch [92/100], Step [520/1248], Loss: 0.0063\n",
      "Epoch [92/100], Step [540/1248], Loss: 0.0027\n",
      "Epoch [92/100], Step [560/1248], Loss: 0.0012\n",
      "Epoch [92/100], Step [580/1248], Loss: 0.0066\n",
      "Epoch [92/100], Step [600/1248], Loss: 0.0108\n",
      "Epoch [92/100], Step [620/1248], Loss: 0.0027\n",
      "Epoch [92/100], Step [640/1248], Loss: 0.0057\n",
      "Epoch [92/100], Step [660/1248], Loss: 0.0138\n",
      "Epoch [92/100], Step [680/1248], Loss: 0.0031\n",
      "Epoch [92/100], Step [700/1248], Loss: 0.0085\n",
      "Epoch [92/100], Step [720/1248], Loss: 0.0508\n",
      "Epoch [92/100], Step [740/1248], Loss: 0.0026\n",
      "Epoch [92/100], Step [760/1248], Loss: 0.0009\n",
      "Epoch [92/100], Step [780/1248], Loss: 0.0017\n",
      "Epoch [92/100], Step [800/1248], Loss: 0.0001\n",
      "Epoch [92/100], Step [820/1248], Loss: 0.0014\n",
      "Epoch [92/100], Step [840/1248], Loss: 0.0092\n",
      "Epoch [92/100], Step [860/1248], Loss: 0.0001\n",
      "Epoch [92/100], Step [880/1248], Loss: 0.0008\n",
      "Epoch [92/100], Step [900/1248], Loss: 0.0026\n",
      "Epoch [92/100], Step [920/1248], Loss: 0.0002\n",
      "Epoch [92/100], Step [940/1248], Loss: 0.0015\n",
      "Epoch [92/100], Step [960/1248], Loss: 0.0002\n",
      "Epoch [92/100], Step [980/1248], Loss: 0.0003\n",
      "Epoch [92/100], Step [1000/1248], Loss: 0.0109\n",
      "Epoch [92/100], Step [1020/1248], Loss: 0.0316\n",
      "Epoch [92/100], Step [1040/1248], Loss: 0.0084\n",
      "Epoch [92/100], Step [1060/1248], Loss: 0.0003\n",
      "Epoch [92/100], Step [1080/1248], Loss: 0.0084\n",
      "Epoch [92/100], Step [1100/1248], Loss: 0.0005\n",
      "Epoch [92/100], Step [1120/1248], Loss: 0.0455\n",
      "Epoch [92/100], Step [1140/1248], Loss: 0.0195\n",
      "Epoch [92/100], Step [1160/1248], Loss: 0.0013\n",
      "Epoch [92/100], Step [1180/1248], Loss: 0.0010\n",
      "Epoch [92/100], Step [1200/1248], Loss: 0.0034\n",
      "Epoch [92/100], Step [1220/1248], Loss: 0.0001\n",
      "Epoch [92/100], Step [1240/1248], Loss: 0.0004\n",
      "\n",
      "train-loss: 0.1035, train-acc: 99.2485\n",
      "validation loss: 1.7989, validation acc: 68.6267\n",
      "\n",
      "Epoch 93\n",
      "\n",
      "Epoch [93/100], Step [0/1248], Loss: 0.0021\n",
      "Epoch [93/100], Step [20/1248], Loss: 0.0018\n",
      "Epoch [93/100], Step [40/1248], Loss: 0.0284\n",
      "Epoch [93/100], Step [60/1248], Loss: 0.0034\n",
      "Epoch [93/100], Step [80/1248], Loss: 0.0022\n",
      "Epoch [93/100], Step [100/1248], Loss: 0.0002\n",
      "Epoch [93/100], Step [120/1248], Loss: 0.0146\n",
      "Epoch [93/100], Step [140/1248], Loss: 0.0497\n",
      "Epoch [93/100], Step [160/1248], Loss: 0.0005\n",
      "Epoch [93/100], Step [180/1248], Loss: 0.0507\n",
      "Epoch [93/100], Step [200/1248], Loss: 0.0017\n",
      "Epoch [93/100], Step [220/1248], Loss: 0.0201\n",
      "Epoch [93/100], Step [240/1248], Loss: 0.0008\n",
      "Epoch [93/100], Step [260/1248], Loss: 0.1701\n",
      "Epoch [93/100], Step [280/1248], Loss: 0.0223\n",
      "Epoch [93/100], Step [300/1248], Loss: 0.0014\n",
      "Epoch [93/100], Step [320/1248], Loss: 0.0354\n",
      "Epoch [93/100], Step [340/1248], Loss: 0.0011\n",
      "Epoch [93/100], Step [360/1248], Loss: 0.0045\n",
      "Epoch [93/100], Step [380/1248], Loss: 0.0037\n",
      "Epoch [93/100], Step [400/1248], Loss: 0.0938\n",
      "Epoch [93/100], Step [420/1248], Loss: 0.0169\n",
      "Epoch [93/100], Step [440/1248], Loss: 0.0014\n",
      "Epoch [93/100], Step [460/1248], Loss: 0.0013\n",
      "Epoch [93/100], Step [480/1248], Loss: 0.0023\n",
      "Epoch [93/100], Step [500/1248], Loss: 0.0020\n",
      "Epoch [93/100], Step [520/1248], Loss: 0.0010\n",
      "Epoch [93/100], Step [540/1248], Loss: 0.0193\n",
      "Epoch [93/100], Step [560/1248], Loss: 0.0030\n",
      "Epoch [93/100], Step [580/1248], Loss: 0.0037\n",
      "Epoch [93/100], Step [600/1248], Loss: 0.0001\n",
      "Epoch [93/100], Step [620/1248], Loss: 0.0013\n",
      "Epoch [93/100], Step [640/1248], Loss: 0.0450\n",
      "Epoch [93/100], Step [660/1248], Loss: 0.0007\n",
      "Epoch [93/100], Step [680/1248], Loss: 0.0050\n",
      "Epoch [93/100], Step [700/1248], Loss: 0.0669\n",
      "Epoch [93/100], Step [720/1248], Loss: 0.0092\n",
      "Epoch [93/100], Step [740/1248], Loss: 0.0018\n",
      "Epoch [93/100], Step [760/1248], Loss: 0.0001\n",
      "Epoch [93/100], Step [780/1248], Loss: 0.0002\n",
      "Epoch [93/100], Step [800/1248], Loss: 0.0032\n",
      "Epoch [93/100], Step [820/1248], Loss: 0.0003\n",
      "Epoch [93/100], Step [840/1248], Loss: 0.0043\n",
      "Epoch [93/100], Step [860/1248], Loss: 0.0004\n",
      "Epoch [93/100], Step [880/1248], Loss: 0.0010\n",
      "Epoch [93/100], Step [900/1248], Loss: 0.0987\n",
      "Epoch [93/100], Step [920/1248], Loss: 0.4571\n",
      "Epoch [93/100], Step [940/1248], Loss: 0.0012\n",
      "Epoch [93/100], Step [960/1248], Loss: 0.0009\n",
      "Epoch [93/100], Step [980/1248], Loss: 0.0047\n",
      "Epoch [93/100], Step [1000/1248], Loss: 0.0023\n",
      "Epoch [93/100], Step [1020/1248], Loss: 0.0006\n",
      "Epoch [93/100], Step [1040/1248], Loss: 0.0029\n",
      "Epoch [93/100], Step [1060/1248], Loss: 0.0004\n",
      "Epoch [93/100], Step [1080/1248], Loss: 0.0313\n",
      "Epoch [93/100], Step [1100/1248], Loss: 0.0020\n",
      "Epoch [93/100], Step [1120/1248], Loss: 0.0007\n",
      "Epoch [93/100], Step [1140/1248], Loss: 0.0177\n",
      "Epoch [93/100], Step [1160/1248], Loss: 0.0058\n",
      "Epoch [93/100], Step [1180/1248], Loss: 0.0081\n",
      "Epoch [93/100], Step [1200/1248], Loss: 0.1318\n",
      "Epoch [93/100], Step [1220/1248], Loss: 0.0006\n",
      "Epoch [93/100], Step [1240/1248], Loss: 0.0051\n",
      "\n",
      "train-loss: 0.1026, train-acc: 99.3086\n",
      "validation loss: 1.8029, validation acc: 68.4370\n",
      "\n",
      "Epoch 94\n",
      "\n",
      "Epoch [94/100], Step [0/1248], Loss: 0.0028\n",
      "Epoch [94/100], Step [20/1248], Loss: 0.0532\n",
      "Epoch [94/100], Step [40/1248], Loss: 0.0001\n",
      "Epoch [94/100], Step [60/1248], Loss: 0.0010\n",
      "Epoch [94/100], Step [80/1248], Loss: 0.0011\n",
      "Epoch [94/100], Step [100/1248], Loss: 0.0011\n",
      "Epoch [94/100], Step [120/1248], Loss: 0.0001\n",
      "Epoch [94/100], Step [140/1248], Loss: 0.0052\n",
      "Epoch [94/100], Step [160/1248], Loss: 0.0001\n",
      "Epoch [94/100], Step [180/1248], Loss: 0.0010\n",
      "Epoch [94/100], Step [200/1248], Loss: 0.0003\n",
      "Epoch [94/100], Step [220/1248], Loss: 0.0002\n",
      "Epoch [94/100], Step [240/1248], Loss: 0.0015\n",
      "Epoch [94/100], Step [260/1248], Loss: 0.0016\n",
      "Epoch [94/100], Step [280/1248], Loss: 0.0004\n",
      "Epoch [94/100], Step [300/1248], Loss: 0.0003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100], Step [320/1248], Loss: 0.0128\n",
      "Epoch [94/100], Step [340/1248], Loss: 0.0002\n",
      "Epoch [94/100], Step [360/1248], Loss: 0.0000\n",
      "Epoch [94/100], Step [380/1248], Loss: 0.0008\n",
      "Epoch [94/100], Step [400/1248], Loss: 0.0017\n",
      "Epoch [94/100], Step [420/1248], Loss: 0.0005\n",
      "Epoch [94/100], Step [440/1248], Loss: 0.0001\n",
      "Epoch [94/100], Step [460/1248], Loss: 0.0005\n",
      "Epoch [94/100], Step [480/1248], Loss: 0.0002\n",
      "Epoch [94/100], Step [500/1248], Loss: 0.0002\n",
      "Epoch [94/100], Step [520/1248], Loss: 0.0005\n",
      "Epoch [94/100], Step [540/1248], Loss: 0.0003\n",
      "Epoch [94/100], Step [560/1248], Loss: 0.0019\n",
      "Epoch [94/100], Step [580/1248], Loss: 0.0006\n",
      "Epoch [94/100], Step [600/1248], Loss: 0.0132\n",
      "Epoch [94/100], Step [620/1248], Loss: 0.0009\n",
      "Epoch [94/100], Step [640/1248], Loss: 0.0021\n",
      "Epoch [94/100], Step [660/1248], Loss: 0.0027\n",
      "Epoch [94/100], Step [680/1248], Loss: 0.0043\n",
      "Epoch [94/100], Step [700/1248], Loss: 0.0004\n",
      "Epoch [94/100], Step [720/1248], Loss: 0.0003\n",
      "Epoch [94/100], Step [740/1248], Loss: 0.0011\n",
      "Epoch [94/100], Step [760/1248], Loss: 0.0049\n",
      "Epoch [94/100], Step [780/1248], Loss: 0.0119\n",
      "Epoch [94/100], Step [800/1248], Loss: 0.0174\n",
      "Epoch [94/100], Step [820/1248], Loss: 0.0005\n",
      "Epoch [94/100], Step [840/1248], Loss: 0.0015\n",
      "Epoch [94/100], Step [860/1248], Loss: 0.0034\n",
      "Epoch [94/100], Step [880/1248], Loss: 0.0108\n",
      "Epoch [94/100], Step [900/1248], Loss: 0.0097\n",
      "Epoch [94/100], Step [920/1248], Loss: 0.0017\n",
      "Epoch [94/100], Step [940/1248], Loss: 0.0189\n",
      "Epoch [94/100], Step [960/1248], Loss: 0.0048\n",
      "Epoch [94/100], Step [980/1248], Loss: 0.0069\n",
      "Epoch [94/100], Step [1000/1248], Loss: 0.0563\n",
      "Epoch [94/100], Step [1020/1248], Loss: 0.0008\n",
      "Epoch [94/100], Step [1040/1248], Loss: 0.0002\n",
      "Epoch [94/100], Step [1060/1248], Loss: 0.0001\n",
      "Epoch [94/100], Step [1080/1248], Loss: 0.0006\n",
      "Epoch [94/100], Step [1100/1248], Loss: 0.0466\n",
      "Epoch [94/100], Step [1120/1248], Loss: 0.0333\n",
      "Epoch [94/100], Step [1140/1248], Loss: 0.0189\n",
      "Epoch [94/100], Step [1160/1248], Loss: 0.0066\n",
      "Epoch [94/100], Step [1180/1248], Loss: 0.0004\n",
      "Epoch [94/100], Step [1200/1248], Loss: 0.0764\n",
      "Epoch [94/100], Step [1220/1248], Loss: 0.0004\n",
      "Epoch [94/100], Step [1240/1248], Loss: 0.0023\n",
      "\n",
      "train-loss: 0.1017, train-acc: 99.3337\n",
      "validation loss: 1.8084, validation acc: 67.6657\n",
      "\n",
      "Epoch 95\n",
      "\n",
      "Epoch [95/100], Step [0/1248], Loss: 0.9016\n",
      "Epoch [95/100], Step [20/1248], Loss: 0.0048\n",
      "Epoch [95/100], Step [40/1248], Loss: 0.0005\n",
      "Epoch [95/100], Step [60/1248], Loss: 0.0306\n",
      "Epoch [95/100], Step [80/1248], Loss: 0.0004\n",
      "Epoch [95/100], Step [100/1248], Loss: 0.0217\n",
      "Epoch [95/100], Step [120/1248], Loss: 0.0001\n",
      "Epoch [95/100], Step [140/1248], Loss: 0.0004\n",
      "Epoch [95/100], Step [160/1248], Loss: 0.1283\n",
      "Epoch [95/100], Step [180/1248], Loss: 0.0151\n",
      "Epoch [95/100], Step [200/1248], Loss: 0.2020\n",
      "Epoch [95/100], Step [220/1248], Loss: 0.0043\n",
      "Epoch [95/100], Step [240/1248], Loss: 0.0102\n",
      "Epoch [95/100], Step [260/1248], Loss: 0.0127\n",
      "Epoch [95/100], Step [280/1248], Loss: 0.0013\n",
      "Epoch [95/100], Step [300/1248], Loss: 0.0369\n",
      "Epoch [95/100], Step [320/1248], Loss: 0.0579\n",
      "Epoch [95/100], Step [340/1248], Loss: 0.1662\n",
      "Epoch [95/100], Step [360/1248], Loss: 0.0227\n",
      "Epoch [95/100], Step [380/1248], Loss: 0.3719\n",
      "Epoch [95/100], Step [400/1248], Loss: 0.1226\n",
      "Epoch [95/100], Step [420/1248], Loss: 0.2031\n",
      "Epoch [95/100], Step [440/1248], Loss: 0.0279\n",
      "Epoch [95/100], Step [460/1248], Loss: 0.3013\n",
      "Epoch [95/100], Step [480/1248], Loss: 0.0006\n",
      "Epoch [95/100], Step [500/1248], Loss: 0.0066\n",
      "Epoch [95/100], Step [520/1248], Loss: 0.0000\n",
      "Epoch [95/100], Step [540/1248], Loss: 0.0011\n",
      "Epoch [95/100], Step [560/1248], Loss: 0.0004\n",
      "Epoch [95/100], Step [580/1248], Loss: 0.0002\n",
      "Epoch [95/100], Step [600/1248], Loss: 0.0193\n",
      "Epoch [95/100], Step [620/1248], Loss: 0.1113\n",
      "Epoch [95/100], Step [640/1248], Loss: 0.0014\n",
      "Epoch [95/100], Step [660/1248], Loss: 0.0043\n",
      "Epoch [95/100], Step [680/1248], Loss: 0.0004\n",
      "Epoch [95/100], Step [700/1248], Loss: 0.0009\n",
      "Epoch [95/100], Step [720/1248], Loss: 0.0036\n",
      "Epoch [95/100], Step [740/1248], Loss: 0.0243\n",
      "Epoch [95/100], Step [760/1248], Loss: 0.0001\n",
      "Epoch [95/100], Step [780/1248], Loss: 0.0005\n",
      "Epoch [95/100], Step [800/1248], Loss: 0.0015\n",
      "Epoch [95/100], Step [820/1248], Loss: 0.0003\n",
      "Epoch [95/100], Step [840/1248], Loss: 0.0007\n",
      "Epoch [95/100], Step [860/1248], Loss: 0.0132\n",
      "Epoch [95/100], Step [880/1248], Loss: 0.0082\n",
      "Epoch [95/100], Step [900/1248], Loss: 0.0017\n",
      "Epoch [95/100], Step [920/1248], Loss: 0.0011\n",
      "Epoch [95/100], Step [940/1248], Loss: 0.0002\n",
      "Epoch [95/100], Step [960/1248], Loss: 0.0087\n",
      "Epoch [95/100], Step [980/1248], Loss: 0.0064\n",
      "Epoch [95/100], Step [1000/1248], Loss: 0.0002\n",
      "Epoch [95/100], Step [1020/1248], Loss: 0.0009\n",
      "Epoch [95/100], Step [1040/1248], Loss: 0.0007\n",
      "Epoch [95/100], Step [1060/1248], Loss: 0.0002\n",
      "Epoch [95/100], Step [1080/1248], Loss: 0.0036\n",
      "Epoch [95/100], Step [1100/1248], Loss: 0.0010\n",
      "Epoch [95/100], Step [1120/1248], Loss: 0.0020\n",
      "Epoch [95/100], Step [1140/1248], Loss: 0.0010\n",
      "Epoch [95/100], Step [1160/1248], Loss: 0.0034\n",
      "Epoch [95/100], Step [1180/1248], Loss: 0.0004\n",
      "Epoch [95/100], Step [1200/1248], Loss: 0.0205\n",
      "Epoch [95/100], Step [1220/1248], Loss: 0.1274\n",
      "Epoch [95/100], Step [1240/1248], Loss: 0.0430\n",
      "\n",
      "train-loss: 0.1009, train-acc: 99.1283\n",
      "validation loss: 1.8112, validation acc: 68.7532\n",
      "\n",
      "Epoch 96\n",
      "\n",
      "Epoch [96/100], Step [0/1248], Loss: 0.0050\n",
      "Epoch [96/100], Step [20/1248], Loss: 0.0022\n",
      "Epoch [96/100], Step [40/1248], Loss: 0.0001\n",
      "Epoch [96/100], Step [60/1248], Loss: 0.0036\n",
      "Epoch [96/100], Step [80/1248], Loss: 0.0025\n",
      "Epoch [96/100], Step [100/1248], Loss: 0.0843\n",
      "Epoch [96/100], Step [120/1248], Loss: 0.0010\n",
      "Epoch [96/100], Step [140/1248], Loss: 0.0048\n",
      "Epoch [96/100], Step [160/1248], Loss: 0.0304\n",
      "Epoch [96/100], Step [180/1248], Loss: 0.0016\n",
      "Epoch [96/100], Step [200/1248], Loss: 0.0003\n",
      "Epoch [96/100], Step [220/1248], Loss: 0.0007\n",
      "Epoch [96/100], Step [240/1248], Loss: 0.0002\n",
      "Epoch [96/100], Step [260/1248], Loss: 0.0330\n",
      "Epoch [96/100], Step [280/1248], Loss: 0.0000\n",
      "Epoch [96/100], Step [300/1248], Loss: 0.0114\n",
      "Epoch [96/100], Step [320/1248], Loss: 0.0036\n",
      "Epoch [96/100], Step [340/1248], Loss: 0.0021\n",
      "Epoch [96/100], Step [360/1248], Loss: 0.0041\n",
      "Epoch [96/100], Step [380/1248], Loss: 0.0006\n",
      "Epoch [96/100], Step [400/1248], Loss: 0.0002\n",
      "Epoch [96/100], Step [420/1248], Loss: 0.0012\n",
      "Epoch [96/100], Step [440/1248], Loss: 0.0004\n",
      "Epoch [96/100], Step [460/1248], Loss: 0.0011\n",
      "Epoch [96/100], Step [480/1248], Loss: 0.0005\n",
      "Epoch [96/100], Step [500/1248], Loss: 0.0001\n",
      "Epoch [96/100], Step [520/1248], Loss: 0.0018\n",
      "Epoch [96/100], Step [540/1248], Loss: 0.0001\n",
      "Epoch [96/100], Step [560/1248], Loss: 0.0609\n",
      "Epoch [96/100], Step [580/1248], Loss: 0.2221\n",
      "Epoch [96/100], Step [600/1248], Loss: 0.0708\n",
      "Epoch [96/100], Step [620/1248], Loss: 0.0128\n",
      "Epoch [96/100], Step [640/1248], Loss: 0.0007\n",
      "Epoch [96/100], Step [660/1248], Loss: 0.0010\n",
      "Epoch [96/100], Step [680/1248], Loss: 0.0053\n",
      "Epoch [96/100], Step [700/1248], Loss: 0.0004\n",
      "Epoch [96/100], Step [720/1248], Loss: 0.0008\n",
      "Epoch [96/100], Step [740/1248], Loss: 0.0033\n",
      "Epoch [96/100], Step [760/1248], Loss: 0.0086\n",
      "Epoch [96/100], Step [780/1248], Loss: 0.0045\n",
      "Epoch [96/100], Step [800/1248], Loss: 0.0001\n",
      "Epoch [96/100], Step [820/1248], Loss: 0.0004\n",
      "Epoch [96/100], Step [840/1248], Loss: 0.0000\n",
      "Epoch [96/100], Step [860/1248], Loss: 0.0003\n",
      "Epoch [96/100], Step [880/1248], Loss: 0.0003\n",
      "Epoch [96/100], Step [900/1248], Loss: 0.0064\n",
      "Epoch [96/100], Step [920/1248], Loss: 0.0005\n",
      "Epoch [96/100], Step [940/1248], Loss: 0.0228\n",
      "Epoch [96/100], Step [960/1248], Loss: 0.0012\n",
      "Epoch [96/100], Step [980/1248], Loss: 0.0224\n",
      "Epoch [96/100], Step [1000/1248], Loss: 0.0363\n",
      "Epoch [96/100], Step [1020/1248], Loss: 0.0009\n",
      "Epoch [96/100], Step [1040/1248], Loss: 0.0002\n",
      "Epoch [96/100], Step [1060/1248], Loss: 0.0094\n",
      "Epoch [96/100], Step [1080/1248], Loss: 0.0021\n",
      "Epoch [96/100], Step [1100/1248], Loss: 0.0023\n",
      "Epoch [96/100], Step [1120/1248], Loss: 0.0001\n",
      "Epoch [96/100], Step [1140/1248], Loss: 0.0034\n",
      "Epoch [96/100], Step [1160/1248], Loss: 0.0038\n",
      "Epoch [96/100], Step [1180/1248], Loss: 0.0391\n",
      "Epoch [96/100], Step [1200/1248], Loss: 0.0040\n",
      "Epoch [96/100], Step [1220/1248], Loss: 0.0063\n",
      "Epoch [96/100], Step [1240/1248], Loss: 0.0775\n",
      "\n",
      "train-loss: 0.1000, train-acc: 99.4439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.8143, validation acc: 67.2610\n",
      "\n",
      "Epoch 97\n",
      "\n",
      "Epoch [97/100], Step [0/1248], Loss: 0.0020\n",
      "Epoch [97/100], Step [20/1248], Loss: 0.0003\n",
      "Epoch [97/100], Step [40/1248], Loss: 0.0001\n",
      "Epoch [97/100], Step [60/1248], Loss: 0.0004\n",
      "Epoch [97/100], Step [80/1248], Loss: 0.0263\n",
      "Epoch [97/100], Step [100/1248], Loss: 0.1175\n",
      "Epoch [97/100], Step [120/1248], Loss: 0.3025\n",
      "Epoch [97/100], Step [140/1248], Loss: 0.0047\n",
      "Epoch [97/100], Step [160/1248], Loss: 0.0946\n",
      "Epoch [97/100], Step [180/1248], Loss: 0.0092\n",
      "Epoch [97/100], Step [200/1248], Loss: 0.0047\n",
      "Epoch [97/100], Step [220/1248], Loss: 0.0005\n",
      "Epoch [97/100], Step [240/1248], Loss: 0.0029\n",
      "Epoch [97/100], Step [260/1248], Loss: 0.0472\n",
      "Epoch [97/100], Step [280/1248], Loss: 0.0272\n",
      "Epoch [97/100], Step [300/1248], Loss: 0.0013\n",
      "Epoch [97/100], Step [320/1248], Loss: 0.0645\n",
      "Epoch [97/100], Step [340/1248], Loss: 0.0931\n",
      "Epoch [97/100], Step [360/1248], Loss: 0.0165\n",
      "Epoch [97/100], Step [380/1248], Loss: 0.0005\n",
      "Epoch [97/100], Step [400/1248], Loss: 0.0017\n",
      "Epoch [97/100], Step [420/1248], Loss: 0.0004\n",
      "Epoch [97/100], Step [440/1248], Loss: 0.0028\n",
      "Epoch [97/100], Step [460/1248], Loss: 0.0035\n",
      "Epoch [97/100], Step [480/1248], Loss: 0.0021\n",
      "Epoch [97/100], Step [500/1248], Loss: 0.0010\n",
      "Epoch [97/100], Step [520/1248], Loss: 0.0114\n",
      "Epoch [97/100], Step [540/1248], Loss: 0.0007\n",
      "Epoch [97/100], Step [560/1248], Loss: 0.0030\n",
      "Epoch [97/100], Step [580/1248], Loss: 0.0903\n",
      "Epoch [97/100], Step [600/1248], Loss: 0.0024\n",
      "Epoch [97/100], Step [620/1248], Loss: 0.0356\n",
      "Epoch [97/100], Step [640/1248], Loss: 0.0053\n",
      "Epoch [97/100], Step [660/1248], Loss: 0.0009\n",
      "Epoch [97/100], Step [680/1248], Loss: 0.0023\n",
      "Epoch [97/100], Step [700/1248], Loss: 0.0313\n",
      "Epoch [97/100], Step [720/1248], Loss: 0.0013\n",
      "Epoch [97/100], Step [740/1248], Loss: 0.0013\n",
      "Epoch [97/100], Step [760/1248], Loss: 0.0023\n",
      "Epoch [97/100], Step [780/1248], Loss: 0.3156\n",
      "Epoch [97/100], Step [800/1248], Loss: 0.0011\n",
      "Epoch [97/100], Step [820/1248], Loss: 0.0043\n",
      "Epoch [97/100], Step [840/1248], Loss: 0.0020\n",
      "Epoch [97/100], Step [860/1248], Loss: 0.0006\n",
      "Epoch [97/100], Step [880/1248], Loss: 0.0020\n",
      "Epoch [97/100], Step [900/1248], Loss: 0.0010\n",
      "Epoch [97/100], Step [920/1248], Loss: 0.0011\n",
      "Epoch [97/100], Step [940/1248], Loss: 0.0489\n",
      "Epoch [97/100], Step [960/1248], Loss: 0.0012\n",
      "Epoch [97/100], Step [980/1248], Loss: 0.0023\n",
      "Epoch [97/100], Step [1000/1248], Loss: 0.0030\n",
      "Epoch [97/100], Step [1020/1248], Loss: 0.0085\n",
      "Epoch [97/100], Step [1040/1248], Loss: 0.1185\n",
      "Epoch [97/100], Step [1060/1248], Loss: 0.0025\n",
      "Epoch [97/100], Step [1080/1248], Loss: 0.0072\n",
      "Epoch [97/100], Step [1100/1248], Loss: 0.0005\n",
      "Epoch [97/100], Step [1120/1248], Loss: 0.0029\n",
      "Epoch [97/100], Step [1140/1248], Loss: 0.0012\n",
      "Epoch [97/100], Step [1160/1248], Loss: 0.0001\n",
      "Epoch [97/100], Step [1180/1248], Loss: 0.0037\n",
      "Epoch [97/100], Step [1200/1248], Loss: 0.0577\n",
      "Epoch [97/100], Step [1220/1248], Loss: 0.0040\n",
      "Epoch [97/100], Step [1240/1248], Loss: 0.0034\n",
      "\n",
      "train-loss: 0.0992, train-acc: 99.1483\n",
      "validation loss: 1.8192, validation acc: 68.7658\n",
      "\n",
      "Epoch 98\n",
      "\n",
      "Epoch [98/100], Step [0/1248], Loss: 0.0047\n",
      "Epoch [98/100], Step [20/1248], Loss: 0.0322\n",
      "Epoch [98/100], Step [40/1248], Loss: 0.0001\n",
      "Epoch [98/100], Step [60/1248], Loss: 0.0304\n",
      "Epoch [98/100], Step [80/1248], Loss: 0.0002\n",
      "Epoch [98/100], Step [100/1248], Loss: 0.0134\n",
      "Epoch [98/100], Step [120/1248], Loss: 0.0219\n",
      "Epoch [98/100], Step [140/1248], Loss: 0.0001\n",
      "Epoch [98/100], Step [160/1248], Loss: 0.0011\n",
      "Epoch [98/100], Step [180/1248], Loss: 0.0116\n",
      "Epoch [98/100], Step [200/1248], Loss: 0.0007\n",
      "Epoch [98/100], Step [220/1248], Loss: 0.0291\n",
      "Epoch [98/100], Step [240/1248], Loss: 0.0359\n",
      "Epoch [98/100], Step [260/1248], Loss: 0.0005\n",
      "Epoch [98/100], Step [280/1248], Loss: 0.0056\n",
      "Epoch [98/100], Step [300/1248], Loss: 0.0008\n",
      "Epoch [98/100], Step [320/1248], Loss: 0.0022\n",
      "Epoch [98/100], Step [340/1248], Loss: 0.0011\n",
      "Epoch [98/100], Step [360/1248], Loss: 0.0143\n",
      "Epoch [98/100], Step [380/1248], Loss: 0.0035\n",
      "Epoch [98/100], Step [400/1248], Loss: 0.0265\n",
      "Epoch [98/100], Step [420/1248], Loss: 0.0042\n",
      "Epoch [98/100], Step [440/1248], Loss: 0.0033\n",
      "Epoch [98/100], Step [460/1248], Loss: 0.0001\n",
      "Epoch [98/100], Step [480/1248], Loss: 0.0019\n",
      "Epoch [98/100], Step [500/1248], Loss: 0.0000\n",
      "Epoch [98/100], Step [520/1248], Loss: 0.0205\n",
      "Epoch [98/100], Step [540/1248], Loss: 0.0003\n",
      "Epoch [98/100], Step [560/1248], Loss: 0.0103\n",
      "Epoch [98/100], Step [580/1248], Loss: 0.0022\n",
      "Epoch [98/100], Step [600/1248], Loss: 0.0001\n",
      "Epoch [98/100], Step [620/1248], Loss: 0.0045\n",
      "Epoch [98/100], Step [640/1248], Loss: 0.0027\n",
      "Epoch [98/100], Step [660/1248], Loss: 0.0004\n",
      "Epoch [98/100], Step [680/1248], Loss: 0.0065\n",
      "Epoch [98/100], Step [700/1248], Loss: 0.0011\n",
      "Epoch [98/100], Step [720/1248], Loss: 0.0002\n",
      "Epoch [98/100], Step [740/1248], Loss: 0.0067\n",
      "Epoch [98/100], Step [760/1248], Loss: 0.0068\n",
      "Epoch [98/100], Step [780/1248], Loss: 0.0001\n",
      "Epoch [98/100], Step [800/1248], Loss: 0.0005\n",
      "Epoch [98/100], Step [820/1248], Loss: 0.0004\n",
      "Epoch [98/100], Step [840/1248], Loss: 0.0357\n",
      "Epoch [98/100], Step [860/1248], Loss: 0.0012\n",
      "Epoch [98/100], Step [880/1248], Loss: 0.0078\n",
      "Epoch [98/100], Step [900/1248], Loss: 0.0001\n",
      "Epoch [98/100], Step [920/1248], Loss: 0.0004\n",
      "Epoch [98/100], Step [940/1248], Loss: 0.0085\n",
      "Epoch [98/100], Step [960/1248], Loss: 0.0001\n",
      "Epoch [98/100], Step [980/1248], Loss: 0.0031\n",
      "Epoch [98/100], Step [1000/1248], Loss: 0.0002\n",
      "Epoch [98/100], Step [1020/1248], Loss: 0.0347\n",
      "Epoch [98/100], Step [1040/1248], Loss: 0.0152\n",
      "Epoch [98/100], Step [1060/1248], Loss: 0.1106\n",
      "Epoch [98/100], Step [1080/1248], Loss: 0.0223\n",
      "Epoch [98/100], Step [1100/1248], Loss: 0.0016\n",
      "Epoch [98/100], Step [1120/1248], Loss: 0.0031\n",
      "Epoch [98/100], Step [1140/1248], Loss: 0.0002\n",
      "Epoch [98/100], Step [1160/1248], Loss: 0.0306\n",
      "Epoch [98/100], Step [1180/1248], Loss: 0.0008\n",
      "Epoch [98/100], Step [1200/1248], Loss: 0.0826\n",
      "Epoch [98/100], Step [1220/1248], Loss: 0.0050\n",
      "Epoch [98/100], Step [1240/1248], Loss: 0.0090\n",
      "\n",
      "train-loss: 0.0984, train-acc: 99.2886\n",
      "validation loss: 1.8222, validation acc: 68.7152\n",
      "\n",
      "Epoch 99\n",
      "\n",
      "Epoch [99/100], Step [0/1248], Loss: 0.0004\n",
      "Epoch [99/100], Step [20/1248], Loss: 0.0008\n",
      "Epoch [99/100], Step [40/1248], Loss: 0.0005\n",
      "Epoch [99/100], Step [60/1248], Loss: 0.0130\n",
      "Epoch [99/100], Step [80/1248], Loss: 0.0027\n",
      "Epoch [99/100], Step [100/1248], Loss: 0.0001\n",
      "Epoch [99/100], Step [120/1248], Loss: 0.0064\n",
      "Epoch [99/100], Step [140/1248], Loss: 0.0004\n",
      "Epoch [99/100], Step [160/1248], Loss: 0.0020\n",
      "Epoch [99/100], Step [180/1248], Loss: 0.0006\n",
      "Epoch [99/100], Step [200/1248], Loss: 0.0091\n",
      "Epoch [99/100], Step [220/1248], Loss: 0.0001\n",
      "Epoch [99/100], Step [240/1248], Loss: 0.0001\n",
      "Epoch [99/100], Step [260/1248], Loss: 0.0001\n",
      "Epoch [99/100], Step [280/1248], Loss: 0.0147\n",
      "Epoch [99/100], Step [300/1248], Loss: 0.0005\n",
      "Epoch [99/100], Step [320/1248], Loss: 0.0004\n",
      "Epoch [99/100], Step [340/1248], Loss: 0.0015\n",
      "Epoch [99/100], Step [360/1248], Loss: 0.0020\n",
      "Epoch [99/100], Step [380/1248], Loss: 0.0013\n",
      "Epoch [99/100], Step [400/1248], Loss: 0.0065\n",
      "Epoch [99/100], Step [420/1248], Loss: 0.0559\n",
      "Epoch [99/100], Step [440/1248], Loss: 0.0419\n",
      "Epoch [99/100], Step [460/1248], Loss: 0.0014\n",
      "Epoch [99/100], Step [480/1248], Loss: 0.0006\n",
      "Epoch [99/100], Step [500/1248], Loss: 0.0003\n",
      "Epoch [99/100], Step [520/1248], Loss: 0.0095\n",
      "Epoch [99/100], Step [540/1248], Loss: 0.0021\n",
      "Epoch [99/100], Step [560/1248], Loss: 0.0004\n",
      "Epoch [99/100], Step [580/1248], Loss: 0.0010\n",
      "Epoch [99/100], Step [600/1248], Loss: 0.0009\n",
      "Epoch [99/100], Step [620/1248], Loss: 0.0115\n",
      "Epoch [99/100], Step [640/1248], Loss: 0.0006\n",
      "Epoch [99/100], Step [660/1248], Loss: 0.0000\n",
      "Epoch [99/100], Step [680/1248], Loss: 0.0007\n",
      "Epoch [99/100], Step [700/1248], Loss: 0.0016\n",
      "Epoch [99/100], Step [720/1248], Loss: 0.2878\n",
      "Epoch [99/100], Step [740/1248], Loss: 0.0001\n",
      "Epoch [99/100], Step [760/1248], Loss: 0.0014\n",
      "Epoch [99/100], Step [780/1248], Loss: 0.0004\n",
      "Epoch [99/100], Step [800/1248], Loss: 0.0012\n",
      "Epoch [99/100], Step [820/1248], Loss: 0.0019\n",
      "Epoch [99/100], Step [840/1248], Loss: 0.0033\n",
      "Epoch [99/100], Step [860/1248], Loss: 0.0022\n",
      "Epoch [99/100], Step [880/1248], Loss: 0.1001\n",
      "Epoch [99/100], Step [900/1248], Loss: 0.0113\n",
      "Epoch [99/100], Step [920/1248], Loss: 0.0099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100], Step [940/1248], Loss: 0.0002\n",
      "Epoch [99/100], Step [960/1248], Loss: 0.0010\n",
      "Epoch [99/100], Step [980/1248], Loss: 0.0002\n",
      "Epoch [99/100], Step [1000/1248], Loss: 0.0005\n",
      "Epoch [99/100], Step [1020/1248], Loss: 0.0067\n",
      "Epoch [99/100], Step [1040/1248], Loss: 0.0143\n",
      "Epoch [99/100], Step [1060/1248], Loss: 0.0150\n",
      "Epoch [99/100], Step [1080/1248], Loss: 0.0001\n",
      "Epoch [99/100], Step [1100/1248], Loss: 0.0011\n",
      "Epoch [99/100], Step [1120/1248], Loss: 0.0030\n",
      "Epoch [99/100], Step [1140/1248], Loss: 0.0014\n",
      "Epoch [99/100], Step [1160/1248], Loss: 0.2027\n",
      "Epoch [99/100], Step [1180/1248], Loss: 0.0000\n",
      "Epoch [99/100], Step [1200/1248], Loss: 0.0001\n",
      "Epoch [99/100], Step [1220/1248], Loss: 0.0225\n",
      "Epoch [99/100], Step [1240/1248], Loss: 0.0146\n",
      "\n",
      "train-loss: 0.0976, train-acc: 99.3136\n",
      "validation loss: 1.8253, validation acc: 67.4380\n",
      "\n",
      "Epoch 100\n",
      "\n",
      "Epoch [100/100], Step [0/1248], Loss: 0.2183\n",
      "Epoch [100/100], Step [20/1248], Loss: 0.0205\n",
      "Epoch [100/100], Step [40/1248], Loss: 0.0041\n",
      "Epoch [100/100], Step [60/1248], Loss: 0.0212\n",
      "Epoch [100/100], Step [80/1248], Loss: 0.0017\n",
      "Epoch [100/100], Step [100/1248], Loss: 0.0013\n",
      "Epoch [100/100], Step [120/1248], Loss: 0.0002\n",
      "Epoch [100/100], Step [140/1248], Loss: 0.2875\n",
      "Epoch [100/100], Step [160/1248], Loss: 0.0800\n",
      "Epoch [100/100], Step [180/1248], Loss: 0.0005\n",
      "Epoch [100/100], Step [200/1248], Loss: 0.0035\n",
      "Epoch [100/100], Step [220/1248], Loss: 0.0003\n",
      "Epoch [100/100], Step [240/1248], Loss: 0.0015\n",
      "Epoch [100/100], Step [260/1248], Loss: 0.0562\n",
      "Epoch [100/100], Step [280/1248], Loss: 0.0072\n",
      "Epoch [100/100], Step [300/1248], Loss: 0.0006\n",
      "Epoch [100/100], Step [320/1248], Loss: 0.0092\n",
      "Epoch [100/100], Step [340/1248], Loss: 0.0019\n",
      "Epoch [100/100], Step [360/1248], Loss: 0.0005\n",
      "Epoch [100/100], Step [380/1248], Loss: 0.0011\n",
      "Epoch [100/100], Step [400/1248], Loss: 0.0859\n",
      "Epoch [100/100], Step [420/1248], Loss: 0.0014\n",
      "Epoch [100/100], Step [440/1248], Loss: 0.0085\n",
      "Epoch [100/100], Step [460/1248], Loss: 0.0037\n",
      "Epoch [100/100], Step [480/1248], Loss: 0.0012\n",
      "Epoch [100/100], Step [500/1248], Loss: 0.0396\n",
      "Epoch [100/100], Step [520/1248], Loss: 0.0004\n",
      "Epoch [100/100], Step [540/1248], Loss: 0.0014\n",
      "Epoch [100/100], Step [560/1248], Loss: 0.0009\n",
      "Epoch [100/100], Step [580/1248], Loss: 0.0002\n",
      "Epoch [100/100], Step [600/1248], Loss: 0.0029\n",
      "Epoch [100/100], Step [620/1248], Loss: 0.0072\n",
      "Epoch [100/100], Step [640/1248], Loss: 0.0023\n",
      "Epoch [100/100], Step [660/1248], Loss: 0.0013\n",
      "Epoch [100/100], Step [680/1248], Loss: 0.0016\n",
      "Epoch [100/100], Step [700/1248], Loss: 0.0016\n",
      "Epoch [100/100], Step [720/1248], Loss: 0.0034\n",
      "Epoch [100/100], Step [740/1248], Loss: 0.0855\n",
      "Epoch [100/100], Step [760/1248], Loss: 0.0004\n",
      "Epoch [100/100], Step [780/1248], Loss: 0.0020\n",
      "Epoch [100/100], Step [800/1248], Loss: 0.0012\n",
      "Epoch [100/100], Step [820/1248], Loss: 0.0009\n",
      "Epoch [100/100], Step [840/1248], Loss: 0.0015\n",
      "Epoch [100/100], Step [860/1248], Loss: 0.0001\n",
      "Epoch [100/100], Step [880/1248], Loss: 0.0089\n",
      "Epoch [100/100], Step [900/1248], Loss: 0.0064\n",
      "Epoch [100/100], Step [920/1248], Loss: 0.0025\n",
      "Epoch [100/100], Step [940/1248], Loss: 0.0106\n",
      "Epoch [100/100], Step [960/1248], Loss: 0.0369\n",
      "Epoch [100/100], Step [980/1248], Loss: 0.0011\n",
      "Epoch [100/100], Step [1000/1248], Loss: 0.0005\n",
      "Epoch [100/100], Step [1020/1248], Loss: 0.0011\n",
      "Epoch [100/100], Step [1040/1248], Loss: 0.0070\n",
      "Epoch [100/100], Step [1060/1248], Loss: 0.0023\n",
      "Epoch [100/100], Step [1080/1248], Loss: 0.0019\n",
      "Epoch [100/100], Step [1100/1248], Loss: 0.0004\n",
      "Epoch [100/100], Step [1120/1248], Loss: 0.0018\n",
      "Epoch [100/100], Step [1140/1248], Loss: 0.0073\n",
      "Epoch [100/100], Step [1160/1248], Loss: 0.0126\n",
      "Epoch [100/100], Step [1180/1248], Loss: 0.0551\n",
      "Epoch [100/100], Step [1200/1248], Loss: 0.0001\n",
      "Epoch [100/100], Step [1220/1248], Loss: 0.0840\n",
      "Epoch [100/100], Step [1240/1248], Loss: 0.0008\n",
      "\n",
      "train-loss: 0.0969, train-acc: 99.0731\n",
      "validation loss: 1.8292, validation acc: 68.2473\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "print_every = 10\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_dataloader)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    for batch_idx, (data_, target_) in enumerate(train_dataloader):\n",
    "        data_, target_ = data_.to(device), target_.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(data_)\n",
    "        loss = criterion(outputs, target_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==target_).item()\n",
    "        total += target_.size(0)\n",
    "        if (batch_idx) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data_t, target_t in (test_dataloader):\n",
    "            data_t, target_t = data_t.to(device), target_t.to(device)\n",
    "            outputs_t = net(data_t)\n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            _,pred_t = torch.max(outputs_t, dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "        val_acc.append(100 * correct_t/total_t)\n",
    "        val_loss.append(batch_loss/len(test_dataloader))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n",
    "\n",
    "        \n",
    "        if network_learned:\n",
    "            valid_loss_min = batch_loss\n",
    "            torch.save(net.state_dict(), 'Densenet201_lr0.0001_adam_batch16_epoch20.pt')\n",
    "            print('Improvement-Detected, save-model')\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f1ee4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_acc)):\n",
    "    train_acc[i] = train_acc[i] / 100\n",
    "for i in range(len(val_acc)):\n",
    "    val_acc[i] = val_acc[i] / 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b844c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.title(\"model accuracy\")\n",
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.axis([1, 100, 0, 1])\n",
    "plt.ylabel(\"accuracy\", fontsize=15)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.xlabel(\"epoch\", fontsize=15)\n",
    "plt.legend([\"train\", \"validation\"], loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f877e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
