{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np #데이터 배열화\n",
    "import os #경로 설정용 \n",
    "import keras #딥러닝용 패키지\n",
    "import random #데이터 분산할 때 쓸 랜덤\n",
    "import cv2 #이미지 읽기용\n",
    "import math #연산용\n",
    "import seaborn as sns #matplotlib에 다양한 시각화 기능이 추가된 패키지\n",
    "\n",
    "from sklearn.metrics import confusion_matrix #분류의 정확성 평가\n",
    "from sklearn.preprocessing import LabelBinarizer #데이터 전처리용\n",
    "from sklearn.model_selection import train_test_split #데이터 분할용\n",
    "\n",
    "import matplotlib.pyplot as plt #데이터 시각화용\n",
    "\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Convolution2D,BatchNormalization #사용할 BN, ConV2 등의 계층\n",
    "from tensorflow.keras.layers import Flatten,MaxPooling2D,Dropout #사용할 레이어\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121 #Densenet121 모델 사용\n",
    "from tensorflow.keras.applications.densenet import preprocess_input #tensor나 numpy배열 전처리용\n",
    "\n",
    "from tensorflow.keras.preprocessing import image #이미지 데이터를 실시간으로 처리하기 위한 도구\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array #이미지 편집을 위한 제너레이터(Affine Transform)\n",
    "\n",
    "from tensorflow.keras.models import Model #교육 및 추론 기능이 있는 개체로 레이어를 그룹화\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam #Adam 옵티마이저 사용. loss는 categorical_crossentropy 사용\n",
    "\n",
    "#체크포인트를 두고 저장 + metric이 중지되면 학습률을 감소\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau \n",
    "\n",
    "import warnings #경고 무시\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d=DenseNet121(weights='imagenet',include_top=False, input_shape=(48, 48, 3)) #채널이 무조건 3개여야 하며 크기는 최소 32 이상\n",
    "\n",
    "x=model_d.output\n",
    "\n",
    "x= GlobalAveragePooling2D()(x) #전역 평균 풀링 레이어 추가\n",
    "x= BatchNormalization()(x) #배치 정규화 레이어\n",
    "x= Dropout(0.3)(x)\n",
    "#Fully Connected 레이어 추가\n",
    "x= Dense(1024,activation='relu')(x) \n",
    "x= Dense(512,activation='relu')(x) \n",
    "x= BatchNormalization()(x)\n",
    "x= Dropout(0.3)(x) #과적합 감소용 드롭아웃 레이어\n",
    "\n",
    "preds=Dense(6,activation='softmax')(x) #FC-layer. 클래스가 7개이므로 softmax 7개 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=model_d.input,outputs=preds) \n",
    "model.summary() #모델 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze\n",
    "for layer in model.layers[:-8]:\n",
    "    layer.trainable=False\n",
    "    \n",
    "for layer in model.layers[-8:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy']) #모델 생성\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[] #데이터\n",
    "labels=[] #라벨\n",
    "random.seed(42)\n",
    "imagePaths = sorted(list(os.listdir(\"emotionHochan/\"))) #데이터셋 경로\n",
    "random.shuffle(imagePaths) #셔플\n",
    "print(imagePaths)\n",
    "#이미지 읽기 및 라벨링\n",
    "for img in imagePaths:\n",
    "    path=sorted(list(os.listdir(\"emotionHochan/\"+img)))\n",
    "    for i in path:\n",
    "        image = cv2.imread(\"emotionHochan/\"+img+'/'+i) #이미지 읽기\n",
    "        image = cv2.resize(image, (48,48)) #이미지 사이즈 편집\n",
    "        image = img_to_array(image) #이미지 배열화\n",
    "        data.append(image) #data 배열에 데이터 추가\n",
    "        l = label = img\n",
    "        labels.append(l) #labels 배열에 데이터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data, dtype=\"float32\") / 255.0 #데이터 타입 변경 및 numpy 배열화\n",
    "labels = np.array(labels) #라벨 numpy 배열화\n",
    "mlb = LabelBinarizer() #데이터 전처리(원핫인코딩)\n",
    "labels = mlb.fit_transform(labels) #데이터에 대해서 fit 작업과 transform 작업을 적용해주는 것\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain,xtest,ytrain,ytest)=train_test_split(data,labels,test_size=0.3,random_state=42) #train용과 test용으로 데이터셋 분리. test 30%\n",
    "print(xtrain.shape, xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#검증 손실에 변화가 없는 경우 학습률을 감소시켜 도움을 줌\n",
    "anne = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, min_lr=1e-3) \n",
    "checkpoint = ModelCheckpoint('Densenet121_revised2.h5', verbose=1, save_best_only=True) #체크포인트를 두고 모델 저장\n",
    "\n",
    "datagen = ImageDataGenerator(zoom_range = 0.3, horizontal_flip=True, shear_range=0.3) #이미지 가공(학습률 향상을 위한 augmentation)\n",
    "\n",
    "\n",
    "datagen.fit(xtrain)\n",
    "# 모델 학습\n",
    "history = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=32),\n",
    "               steps_per_epoch=xtrain.shape[0] //128,\n",
    "               epochs=50,\n",
    "               verbose=2,#verbose 2는 '세대 당 한 라인'을 의미\n",
    "               callbacks=[anne, checkpoint], #학습과 검증 과정에서 적용할 콜백 리스트\n",
    "               validation_data=(xtrain, ytrain)) #steps_per_epoch일 때 사용하는 것으로 샘플 배치 데이터 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#완성도 확인\n",
    "ypred = model.predict(xtest)\n",
    "\n",
    "total = 0\n",
    "accurate = 0\n",
    "accurateindex = []\n",
    "wrongindex = []\n",
    "\n",
    "for i in range(len(ypred)):\n",
    "    if np.argmax(ypred[i]) == np.argmax(ytest[i]): #가장 큰 값을 찾아 인덱스 반환\n",
    "        accurate += 1\n",
    "        accurateindex.append(i)\n",
    "    else:\n",
    "        wrongindex.append(i)\n",
    "        \n",
    "    total += 1\n",
    "    \n",
    "print('Total-test-data;', total, '\\taccurately-predicted-data:', accurate, '\\t wrongly-predicted-data: ', total - accurate)\n",
    "print('Accuracy:', round(accurate/total*100, 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xtest, ytest, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
